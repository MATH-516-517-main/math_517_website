[
  {
    "objectID": "resources/tips/collaboration.html",
    "href": "resources/tips/collaboration.html",
    "title": "Best practices for collaborative work",
    "section": "",
    "text": "In the world of statistics and data analysis, collaboration plays a vital role in advancing scientific knowledge and solving complex problems. When working in a team or contributing to open-source projects, it is crucial to follow best practices for collaborative work. In this section, we will explore key strategies to enhance collaboration, maintain code quality, and ensure seamless cooperation with peers."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-style-guidelines",
    "href": "resources/tips/collaboration.html#code-style-guidelines",
    "title": "Best practices for collaborative work",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  },
  {
    "objectID": "resources/tips/collaboration.html#documentation",
    "href": "resources/tips/collaboration.html#documentation",
    "title": "Best practices for collaborative work",
    "section": "Documentation",
    "text": "Documentation\nWriting clear and comprehensive documentation for your code is crucial for effective collaboration. Documenting your functions, classes, and important code blocks with comments ensures that others can understand the purpose and functionality of each component. Additionally, provide explanations for any complex algorithms or statistical methods used in your analysis."
  },
  {
    "objectID": "resources/tips/collaboration.html#using-version-control-effectively",
    "href": "resources/tips/collaboration.html#using-version-control-effectively",
    "title": "Best practices for collaborative work",
    "section": "Using Version Control Effectively",
    "text": "Using Version Control Effectively\nWhen collaborating with others, version control becomes even more critical. Follow these best practices:\n\nCommit often\nMake small, logical commits with descriptive commit messages. Good commit messages are clear, concise, and informative. They provide context and explain the purpose of the commit in a way that anyone reading them, whether it’s your collaborators or your future self, can understand. A well-written commit message describes why the change was made, how it affects the codebase, and any relevant issues it addresses. By adhering to this practice, you make it easier to track changes, collaborate effectively, and maintain a clean and understandable version history for your projects.\nBad Commit Message:\nMade changes\nThis commit message is too generic and doesn’t provide any insight into what changes were made or why.\nCorrected Commit Message:\nAdd median calculation function for dataset variance\n\nIn response to user feedback and to enhance the statistical capabilities of our library, this commit introduces a new function, `calculate_median()`, which accurately calculates the median value for datasets. The function has been rigorously tested against various data sets to ensure its reliability and accuracy in statistical computations.\nThe corrected commit message provides specific information about the change made, mentions its purpose in improving the library’s statistical capabilities, and briefly explains the testing process to ensure the quality of the new feature. This level of detail is essential for collaboration and helps others understand the changes made to the codebase. If you are working on your own, you might not need to be so verbose in the body of the commit, but it is still a good practice to write good commit messages. See this article for some tips on how to write good commit messages, and why it could be useful.\n\n\nBranching\nUse branches for different features or tasks to keep the main development branch clean and stable.\n\n\nPull Requests (PRs)\nWhen contributing to shared repositories, submit pull requests for review before merging changes into the main branch."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-reviews",
    "href": "resources/tips/collaboration.html#code-reviews",
    "title": "Best practices for collaborative work",
    "section": "Code Reviews",
    "text": "Code Reviews\nCode reviews are an integral part of the collaborative process. Reviewing each other’s code helps identify potential issues, provides valuable feedback, and improves the overall quality of the project. During code reviews, be respectful, specific, and constructive in your comments."
  },
  {
    "objectID": "resources/tips/collaboration.html#issue-tracking",
    "href": "resources/tips/collaboration.html#issue-tracking",
    "title": "Best practices for collaborative work",
    "section": "Issue Tracking",
    "text": "Issue Tracking\nUtilize issue tracking systems like GitHub Issues to keep track of tasks, bugs, and enhancements. When collaborating on larger projects, this helps organize and prioritize work effectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#continuous-integration-ci",
    "href": "resources/tips/collaboration.html#continuous-integration-ci",
    "title": "Best practices for collaborative work",
    "section": "Continuous Integration (CI)",
    "text": "Continuous Integration (CI)\nConsider integrating continuous integration into your workflow. CI systems automatically build and test your code whenever changes are pushed to the repository. This ensures that the project remains in a working state and prevents introducing new bugs unintentionally."
  },
  {
    "objectID": "resources/tips/collaboration.html#collaborative-decision-making",
    "href": "resources/tips/collaboration.html#collaborative-decision-making",
    "title": "Best practices for collaborative work",
    "section": "Collaborative Decision-Making",
    "text": "Collaborative Decision-Making\nWhen making significant decisions about the project, involve all relevant team members. Encourage open discussions and consider different perspectives to arrive at the best solutions collectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#communication-and-feedback",
    "href": "resources/tips/collaboration.html#communication-and-feedback",
    "title": "Best practices for collaborative work",
    "section": "Communication and Feedback",
    "text": "Communication and Feedback\nMaintain effective communication channels with your team. Use tools like Slack or Discord to discuss ideas, share progress, and address any challenges. Offer and receive feedback graciously to create a positive and productive working environment."
  },
  {
    "objectID": "resources/tips/collaboration.html#licensing-and-copyright",
    "href": "resources/tips/collaboration.html#licensing-and-copyright",
    "title": "Best practices for collaborative work",
    "section": "Licensing and Copyright",
    "text": "Licensing and Copyright\nEnsure that all code and resources used in the project comply with appropriate licenses and copyright laws. Respect intellectual property rights and provide proper attribution when using external libraries or resources.\nIn conclusion, effective collaboration is crucial for success in statistical research and data analysis projects. By adhering to code style guidelines, providing thorough documentation, using version control effectively, conducting code reviews, and maintaining open communication, you and your team can work together seamlessly, produce high-quality code, and contribute to the advancement of statistical knowledge. Collaborative work is not only about sharing ideas but also about learning from others and growing as a team. By adopting these best practices, you will become an effective collaborator and contribute to the success of your statistical projects."
  },
  {
    "objectID": "resources/tips/reproducible_research.html",
    "href": "resources/tips/reproducible_research.html",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely. See the virtual environments for more information on managing dependencies.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "href": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely. See the virtual environments for more information on managing dependencies.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/tutorials/installing_software.html",
    "href": "resources/tutorials/installing_software.html",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable.\n\n\n\nGo to https://www.python.org/downloads/ and follow the instructions to install Python.\n\n\n\nGo to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/installing_software.html#r",
    "href": "resources/tutorials/installing_software.html#r",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/installing_software.html#python",
    "href": "resources/tutorials/installing_software.html#python",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://www.python.org/downloads/ and follow the instructions to install Python.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/installing_software.html#julia",
    "href": "resources/tutorials/installing_software.html#julia",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/installing_software.html#visual-studio-code",
    "href": "resources/tutorials/installing_software.html#visual-studio-code",
    "title": "Software installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nGo to https://code.visualstudio.com/ and follow the instruction. You can choose the nightly or stable build. I am personally using the nightly build, but would not recommend it if you are not comfortable with debugging random issues that might arise.\nIt is a good idea to install extension from the visual studio marketplace. You can do that by clicking on the extension icon on the left side of the screen. I would recommend the following extensions:\n\nfor Julia: https://www.julia-vscode.org/\nfor Python: follow this https://code.visualstudio.com/docs/python/editing\nfor R : https://code.visualstudio.com/docs/languages/r",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/installing_software.html#rstudio",
    "href": "resources/tutorials/installing_software.html#rstudio",
    "title": "Software installation",
    "section": "RStudio",
    "text": "RStudio\nGo to posit.co and download the RStudio IDE. Choose RStudio Desktop free version and make sure the correct operating system was selected. Install RStudio from the downloaded executable. Open RStudio.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "Installing software"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html",
    "href": "resources/tutorials/github_classroom.html",
    "title": "Github Classroom",
    "section": "",
    "text": "We will use Github Classroom for the assignments in this course. This document will give you a brief introduction to Github Classroom and how to use it.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#introduction",
    "href": "resources/tutorials/github_classroom.html#introduction",
    "title": "Github Classroom",
    "section": "Introduction",
    "text": "Introduction\nThis guide is here to help you get started. GitHub Classroom is a platform that we will use to manage and distribute assignments, making it easier for you to collaborate on coursework.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "href": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "title": "Github Classroom",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nGitHub Classroom is a tool that simplifies the process of creating, distributing, and submitting assignments on GitHub. It leverages the power of Git, a version control system, and GitHub, a web-based platform for code hosting and collaboration. With GitHub Classroom, you’ll be able to access your assignments, work on them, and submit your work – all in one place.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#prerequisites",
    "href": "resources/tutorials/github_classroom.html#prerequisites",
    "title": "Github Classroom",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we dive into using GitHub Classroom, make sure you have the following:\n\nGitHub Account: If you don’t already have a GitHub account, you can sign up for free at GitHub. This account will be essential for participating in assignments.\nGit Installed: Git is a tool that helps you manage your code changes. You can download and install Git from the official website: Git Downloads. See this tutorial if you need help installing Git.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#getting-started",
    "href": "resources/tutorials/github_classroom.html#getting-started",
    "title": "Github Classroom",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s begin by taking the first steps with GitHub Classroom:\n\n1. Accept the Classroom Invitation\nYour instructor will provide you with a link to the GitHub Classroom assignment. Click on the link, and it will guide you to accept the invitation. This action will create a private repository for your assignment within your GitHub account.\n\n\n2. Clone the Repository\nNext, you’ll want to bring the assignment to your computer so you can work on it. To do this, you need to clone the assignment repository. Don’t worry; cloning simply means making a copy of the assignment on your computer.\n\nOpen a terminal or Git Bash (if you installed Git).\nNavigate to the directory where you want to store your assignment (e.g., Documents).\nUse the following command, replacing &lt;repository_url&gt; with the URL of your assignment repository (you can find it on the GitHub page of your assignment):\n\ngit clone &lt;repository_url&gt;\n\n\n3. Work on the Assignment\nNow that you have the assignment on your computer, explore the repository. Inside, you’ll find instructions and possibly some starter code or files. These will guide you on what to do. You can use any code editor or development environment you’re comfortable with.\n\n\n4. Save Your Work\nWhile working on your assignment, be sure to save your progress regularly. In the Git world, this is called “committing.” It helps you keep track of your changes and lets you go back to previous versions if needed.\n\nTo commit your changes, open the terminal in the project folder and run these commands:\n\ngit add .\ngit commit -m \"Your commit message here\"\n\n\n\n\n\n\nNote\n\n\n\nYour commit message should be short and descriptive. It should explain what changes you made in this commit. See for example this article for some tips on how to write good commit messages.\n\n\n\n\n5. Share Your Progress\nTo collaborate with classmates or ask for help from your instructor, you need to share your work with them on GitHub. You do this by “pushing” your changes to GitHub. This action will update your assignment repository on GitHub with your latest changes. You don’t have to do it everytime you make a change, but keep in mind that you’ll need to push your changes before submitting your assignment, and that pushing helps you keep your work safe in case something happens to your computer. When working in group, don’t forget to pull the changes from your teammates before pushing your own changes.\n\nTo push your changes to GitHub, use this command:\n\ngit push\n\n\n6. Submit Your Assignment\nWhen you’re satisfied with your work and ready to turn it in, you’ll need to submit your assignment through GitHub. This is typically done through a “Pull Request” on GitHub. Your instructor will provide instructions on how to submit. (If nothing is mentioned, you can assume that you only need to push your work on the main branch.)\n\n\n7. Review Feedback\nAfter the submission deadline, your instructor will review your assignment and may provide feedback or grades through GitHub. You can see this feedback by checking the pull request in your assignment repository on GitHub.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#conclusion",
    "href": "resources/tutorials/github_classroom.html#conclusion",
    "title": "Github Classroom",
    "section": "Conclusion",
    "text": "Conclusion\nGitHub Classroom is a valuable tool that can make your Master’s program coursework more manageable and collaborative. While it might seem intimidating at first, remember that it’s a skill worth learning, and you can always reach out to your instructor or classmates for assistance. With this guide, you’re well on your way to successfully using GitHub Classroom for your assignments. Happy learning and coding!",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "href": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "title": "Github Classroom",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nThere exists an integration with VSCode that can make your life easier when working with Github Classroom if you are using VSCode.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub classroom"
    ]
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html",
    "title": "Good practices in Julia",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "title": "Good practices in Julia",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "title": "Good practices in Julia",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\nfunction calculate_mean(data)\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / length(data)\n    \n    return mean_value\nend\n\n# Avoid: Lack of comments and explanation\nfunction calc_mean(d)\n    t = sum(d)\n    m = t / length(d)\n    return m\nend"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Julia",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\nfunction calculate_variance(data)\n    n = length(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend\n\n# Avoid: Using global variables in functions\nmean_value = 0\nfunction calc_variance(data)\n    n = length(data)\n    global mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html",
    "href": "resources/computing/intro_to_r/2_working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "title": "Working with data",
    "section": "Importing spreadsheet-style data into R",
    "text": "Importing spreadsheet-style data into R\nOpen RStudio, and we’ll check to make sure you’re ready to start work again. You can check to see if you’re working in your project directory by looking at the top of the Console. You should see the path (location in your computer) for the project directory you created last time (e.g., ~/Desktop/intro_r).\nIf you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script (File -&gt; New File -&gt; R Script) and save it in your project directory with the name class2.R. Place the following comment on the top line as a title:\n# Introduction to R: Class 2\nIn the last session, we recommended organizing your work in directories (folders) according to projects. While a thorough discussion of project organization is beyond the scope of this class, we will continue to model best practices by creating a directory to hold our data:\n\n# make a directory\ndir.create(\"data\")\n\nYou should see the new directory appear in your project directory, in the lower right panel in RStudio. There is also a button in that panel you can use to create a new folder, but including the code to perform this task makes other people (and yourself) able to reproduce your work more easily.\nNow that we have a place to store our data, we can go ahead and download the dataset:\n\n# download data from url\ndownload.file(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\", \"data/clinical.csv\")\n\nThe code above has two arguments, both encompassed in quotation marks: first, you indicate where the data can be found online. Second, you indicate where R should store a copy of the file on your own computer.\nThe output from that command may look alarming, but it represents information confirming it worked. You can click on the data folder to ensure the file is now present.\nNotice that the URL above ends in clinical.csv, which is also the name we used to save the file on our computers. If you click on the URL and view it in a web browser, the format isn’t particularly easy for us to understand. You can also view the file by clicking on it in the lower right hand panel, then selecting “View File.”\n\nThe option to “Import Dataset” you see after clicking on the file references some additional tools present in RStudio that can assist with various kinds of data import. Because this requires installing additional software, complete exploration of these options is outside the scop of this class. For more information, check out this article.\n\nThe data we’ve downloaded are in csv format, which stands for “comma separated values.” This means the data are organized into rows and columns, with columns separated by commas.\nThese data are arranged in a tidy format, meaning each row represents an observation, and each column represents a variable (piece of data for each observation). Moreover, only one piece of data is entered in each cell.\nNow that the data are downloaded, we can import the data and assign to an object:\n\n# import data and assign to object\nclinical &lt;- read.csv(\"data/clinical.csv\")\n\nYou should see clinical appear in the Environment window on the upper right panel in RStudio. If you click on clinical there, a new tab will appear next to your R script in the Source window.\n\nClicking on the name of an object in the Environment window is a shortcut for running View(clinical); you’ll see this code appear in the Console after clicking.\n\nNow that we have the data imported and assigned to an object, we can take some time to explore the data we’ll be using for the rest of this course:\n\nThese data are clinical cancer data from the National Cancer Institute’s Genomic Data Commons, specifically from The Cancer Genome Atlas, or TCGA.\nEach row represents a patient, and each column represents information about demographics (race, age at diagnosis, etc) and disease (e.g., cancer type).\nThe data were downloaded and aggregated using an R script, which you can view in the GitHub repository for this course.\n\nThe function we used to import the data is one of a family of commands used to import the data. Check out the help documentation for read.csv for more options for importing data.\n\nYou can also import data directly into R using read.csv, using clinical &lt;- read.csv(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\"). For these lessons, we model downloading and importing in two steps, so you retain a copy of the data on your computer. This reflects how you’re likely to import your own data, as well as recommended practice for retaining data used in an analysis (since data online may be updated).\n\n\nChallenge-data\nDownload, inspect, and import the following data files. The URL for each sample dataset is included along with a name to assign to the object. (Hint: you can use the same function as above, but may need to update the sep = parameter) - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.tsv, object name: example1 - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.txt, object name: example2\n\nImporting data can be tricky and frustrating, However, if you can’t get your data into R, you can’t do anything to analyze or visualize it. It’s worth understanding how to do it effectively to save you time and energy later."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "title": "Working with data",
    "section": "Data frames",
    "text": "Data frames\nNow that we have data imported and available, we can start to inspect the data more closely.\nThese data have been interpreted by R to be a data frame, which is a data structure (way of organizing data) that is analogous to tabular or spreadsheet style data. By definition, a data frame is a table made of vectors (columns) of all the same length. As we learned in our last session, a vector needs to include all of the same type of data (e.g., character, numeric). A data frame, however, can include vectors (columns) of different data types.\nTo learn more about this data frame, we’ll first explore its dimensions:\n\n# assess size of data frame\ndim(clinical)\n\n[1] 6832   20\n\n\nThe output reflects the number of rows first (6832), then the number of columns (20).\nWe can also preview the content by showing the first few rows:\n\n# preview first few rows\nhead(clinical) \n\n  primary_diagnosis tumor_stage age_at_diagnosis vital_status morphology\n1             C34.1    stage ia            24477         dead     8070/3\n2             C34.1    stage ib            26615         dead     8070/3\n3             C34.3    stage ib            28171         dead     8070/3\n4             C34.1    stage ia            27154        alive     8083/3\n5             C34.3   stage iib            29827         dead     8070/3\n6             C34.1  stage iiia            23370        alive     8070/3\n  days_to_death state tissue_or_organ_of_origin days_to_birth\n1           371  live                     C34.1        -24477\n2           136  live                     C34.1        -26615\n3          2304  live                     C34.3        -28171\n4            NA  live                     C34.1        -27154\n5           146  live                     C34.3        -29827\n6            NA  live                     C34.1        -23370\n  site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n1                       C34.1                     NA          10.958904\n2                       C34.1                     NA           2.191781\n3                       C34.3                   2099           1.643836\n4                       C34.1                   3747           1.095890\n5                       C34.3                     NA                 NA\n6                       C34.1                   3576           2.739726\n  years_smoked gender year_of_birth         race              ethnicity\n1           NA   male          1936        white not hispanic or latino\n2           NA   male          1931        asian not hispanic or latino\n3           NA female          1927        white not hispanic or latino\n4           NA   male          1930        white not hispanic or latino\n5           NA   male          1923 not reported           not reported\n6           NA female          1942 not reported           not reported\n  year_of_death bcr_patient_barcode disease\n1          2004        TCGA-18-3406    LUSC\n2          2003        TCGA-18-3407    LUSC\n3            NA        TCGA-18-3408    LUSC\n4            NA        TCGA-18-3409    LUSC\n5          2004        TCGA-18-3410    LUSC\n6            NA        TCGA-18-3411    LUSC\n\n\nThe default number of rows shown is six. You can specify a different number using the n = parameter, demonstrated below using tail, which shows the last few rows\n\n# show last three rows\ntail(clinical, n = 3) \n\n     primary_diagnosis  tumor_stage age_at_diagnosis vital_status morphology\n6830             C54.1 not reported            27326         dead     8950/3\n6831             C54.1 not reported            24781        alive     8950/3\n6832             C54.1 not reported            20318        alive     8950/3\n     days_to_death state tissue_or_organ_of_origin days_to_birth\n6830           949  live                     C54.1        -27326\n6831            NA  live                     C54.1        -24781\n6832            NA  live                     C54.1        -20318\n     site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n6830                       C54.1                     NA                 NA\n6831                       C54.1                    587                 NA\n6832                       C54.1                      0                 NA\n     years_smoked gender year_of_birth  race              ethnicity\n6830           NA female          1932 white not hispanic or latino\n6831           NA female          1945 white not hispanic or latino\n6832           NA female          1957 asian not hispanic or latino\n     year_of_death bcr_patient_barcode disease\n6830          2008        TCGA-NG-A4VW     UCS\n6831            NA        TCGA-QM-A5NM     UCS\n6832            NA        TCGA-QN-A5NN     UCS\n\n\nWe often need to reference the names of columns, so it’s useful to print only those to the screen:\n\n# view column names\nnames(clinical) \n\n [1] \"primary_diagnosis\"           \"tumor_stage\"                \n [3] \"age_at_diagnosis\"            \"vital_status\"               \n [5] \"morphology\"                  \"days_to_death\"              \n [7] \"state\"                       \"tissue_or_organ_of_origin\"  \n [9] \"days_to_birth\"               \"site_of_resection_or_biopsy\"\n[11] \"days_to_last_follow_up\"      \"cigarettes_per_day\"         \n[13] \"years_smoked\"                \"gender\"                     \n[15] \"year_of_birth\"               \"race\"                       \n[17] \"ethnicity\"                   \"year_of_death\"              \n[19] \"bcr_patient_barcode\"         \"disease\"                    \n\n\nIt’s also possible to view row names usingrownames(clinical), but our data only possess numbers for row names so it’s not very informative.\nAs we learned last time, we can use str to provide a general overview of the object:\n\n# show overview of object\nstr(clinical) \n\n'data.frame':   6832 obs. of  20 variables:\n $ primary_diagnosis          : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr  \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : int  24477 26615 28171 27154 29827 23370 19025 26938 28430 30435 ...\n $ vital_status               : chr  \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr  \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : int  371 136 2304 NA 146 NA 345 716 2803 973 ...\n $ state                      : chr  \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : int  -24477 -26615 -28171 -27154 -29827 -23370 -19025 -26938 -28430 -30435 ...\n $ site_of_resection_or_biopsy: chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : int  NA NA 2099 3747 NA 3576 NA NA 1810 956 ...\n $ cigarettes_per_day         : num  10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : int  1936 1931 1927 1930 1923 1942 1953 1932 1929 1923 ...\n $ race                       : chr  \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr  \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : int  2004 2003 NA NA 2004 NA 2005 2006 NA NA ...\n $ bcr_patient_barcode        : chr  \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr  \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n\n\nThe output provided includes:\n\ndata structure: data frame\ndimensions: 6832 rows and 20 columns\ncolumn-by-column information: each prefaced with a $, and includes the column name, data type (num, int, Factor)\n\n\nFactors are how character data are interpreted by R in data frames. We’ll talk more about working with factors at the end of this lesson.\n\nFinally, we can also examine basic summary statistics for each column:\n\n# provide summary statistics for each column\nsummary(clinical) \n\n primary_diagnosis  tumor_stage        age_at_diagnosis vital_status      \n Length:6832        Length:6832        Min.   : 3982    Length:6832       \n Class :character   Class :character   1st Qu.:19191    Class :character  \n Mode  :character   Mode  :character   Median :22842    Mode  :character  \n                                       Mean   :22320                      \n                                       3rd Qu.:26002                      \n                                       Max.   :32872                      \n                                       NA's   :114                        \n  morphology        days_to_death        state          \n Length:6832        Min.   :    0.0   Length:6832       \n Class :character   1st Qu.:  274.0   Class :character  \n Mode  :character   Median :  524.0   Mode  :character  \n                    Mean   :  878.2                     \n                    3rd Qu.: 1044.5                     \n                    Max.   :10870.0                     \n                    NA's   :4645                        \n tissue_or_organ_of_origin days_to_birth    site_of_resection_or_biopsy\n Length:6832               Min.   :-32872   Length:6832                \n Class :character          1st Qu.:-26002   Class :character           \n Mode  :character          Median :-22842   Mode  :character           \n                           Mean   :-22320                              \n                           3rd Qu.:-19191                              \n                           Max.   : -3982                              \n                           NA's   :114                                 \n days_to_last_follow_up cigarettes_per_day  years_smoked      gender         \n Min.   :  -64.0        Min.   : 0.008     Min.   : 8.00   Length:6832       \n 1st Qu.:  345.0        1st Qu.: 1.370     1st Qu.:30.75   Class :character  \n Median :  650.0        Median : 2.192     Median :40.00   Mode  :character  \n Mean   :  976.8        Mean   : 2.599     Mean   :39.96                     \n 3rd Qu.: 1259.0        3rd Qu.: 3.288     3rd Qu.:50.00                     \n Max.   :11252.0        Max.   :40.000     Max.   :63.00                     \n NA's   :1118           NA's   :5661       NA's   :6384                      \n year_of_birth      race            ethnicity         year_of_death \n Min.   :1902   Length:6832        Length:6832        Min.   :1990  \n 1st Qu.:1937   Class :character   Class :character   1st Qu.:2004  \n Median :1947   Mode  :character   Mode  :character   Median :2007  \n Mean   :1948                                         Mean   :2006  \n 3rd Qu.:1957                                         3rd Qu.:2010  \n Max.   :1993                                         Max.   :2014  \n NA's   :170                                          NA's   :5266  \n bcr_patient_barcode   disease         \n Length:6832         Length:6832       \n Class :character    Class :character  \n Mode  :character    Mode  :character  \n                                       \n                                       \n                                       \n                                       \n\n\nFor numeric data (such as year_of_death), this output includes common statistics like median and mean, as well as the number of rows (patients) with missing data (as NA). For factors (character data, such as disease), you’re given a count of the number of times the top six most frequent factors (categories) occur in the data frame."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "title": "Working with data",
    "section": "Subsetting data frames",
    "text": "Subsetting data frames\nNow that our data are available for use, we can begin extracting relevant information from them.\n\n# extract first column and assign to a variable\nfirst_column &lt;- clinical[1]\n\nAs discussed last time with vectors, the square brackets ([ ]) are used to subset, or reference part of, a data frame. You can inspect the output object by clicking on it in the environment. It contains all of the rows for only the first column.\nWhen a single number is included in the square brackets, R assumes you are referencing a column. When you include two numbers in square brackets separated by a comma, R assumes the first number references the row and the second number references the column you desire.\nThis means you can also reference the first column as follows:\n\n# extract first column\nfirst_column_again &lt;- clinical[ , 1]\n\nLeaving one field blank means you want the entire set in the output (in this case, all rows).\n\nChallenge-extract\nWhat is the difference in results between the last two lines of code?\n\nSimilarly, we can also extract only the first row across all columns:\n\n# extract first row \nfirst_row &lt;- clinical[1, ]\n\nWe can also extract slices, or sections of rows and columns, such as a single cell:\n\n# extract cell from first row of first column\nsingle_cell &lt;- clinical[1,1]\n\nTo extract a range of cells, we use the same colon (:) syntax from last time:\n\n# extract a range of cells, rows 1 to 3, second column\nrange_cells &lt;- clinical[1:3, 2]\n\nThis works for ranges of columns as well.\nWe can also exclude particular parts of the dataset using a minus sign:\n\n# exclude first column\nexclude_col &lt;- clinical[ , -1] \n\nCombining what we know about R syntax, we can also exclude a range of cells using the c function:\n\n# exclude first 100 rows\nexclude_range &lt;- clinical[-c(1:100), ] \n\nSo far, we’ve been referencing parts of the dataset based on index position, or the number of row/column. Because we have included column names in our dataset, we can also reference columns using those names:\n\n# extract column by name\nname_col1 &lt;- clinical[\"tumor_stage\"]\nname_col2 &lt;- clinical[ , \"tumor_stage\"]\n\nNote the example above features quotation marks around the column name. Without the quotation marks, R will assume we’re attempting to reference an object.\nAs we discussed with subsetting based on index above, the two objects created above differ in the data structure. name_col1 is a data frame (with one column), while name_col2 is a vector. Although this difference in the type of object may not matter for your analysis, it’s useful to understand that there are multiple ways to accomplish a task, each of which may make particular code work more easily.\nThere are additional ways to extract columns, which use R specific for complex data objects, and may be useful to recognize as your R skills progress.\nThe first is to use double square brackets:\n\n# double square brackets syntax\nname_col3 &lt;- clinical[[\"tumor_stage\"]]\n\nYou can think of this approach as digging deeply into a complex object to retrieve data.\nThe final approach is equivalent to the last example, but can be considered a shortcut since it requires fewer keystrokes (no quotation marks, and only one symbol):\n\n# dollar sign syntax\nname_col4 &lt;- clinical$tumor_stage\n\nBoth of the last two approaches above return vectors. For more information about these different ways of accessing parts of a data frame, see this article.\nThe following challenges all use the clinical object:\n\nChallenge-days\nCode as many different ways possible to extract the column days_to_death.\n\n\nChallenge-rows\nExtract the first 6 rows for only age_at_diagnosis and days_to_death.\n\n\nChallenge-calculate\nCalculate the range and mean for cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "title": "Working with data",
    "section": "Factors",
    "text": "Factors\nNote: This section was written with a previous version of R that automatically interprets all character data as factors (this is not true of more recent versions of R). To execute the code in this section, please first import your data again, using the following modified command:\n\nclinical &lt;- read.csv(\"data/clinical.csv\", stringsAsFactors = TRUE)\n\nThis section explores one of the trickier types of data you’re likely to encounter: factors, which are how R interprets categorical data.\nWhen we imported our dataset into R, the read.csv function assumed that all the character data in our dataset are factors, or categories. Factors have predefined sets of values, called levels. We can explore what this means by first creating a factor vector:\n\n# create vector with factor data\ntest_data &lt;- factor(c(\"placebo\", \"test_drug\", \"placebo\", \"known_drug\"))\n# show factor\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug placebo test_drug\n\n\nThis vector includes four pieces of data (often referred to as items or elements), which are printed as output above. The second line of the output shows information about the levels, or categories, of our vector. We can also access this information separately, which is useful if the data (vector) has a large number of elements:\n\n# show levels of factor\nlevels(test_data) \n\n[1] \"known_drug\" \"placebo\"    \"test_drug\" \n\n\nThe levels in this test dataset are currently listed in alphabetical order, which is the default presentation in R. The order of factors dictates how they are presented in subsequent analyses, so there are definitely cases in which you may want the levels in a specific order. In the case of test_data, we may want to keep the two drug treatments together, with placebo at the end:\n\n# reorder factors to put placebo at end\ntest_data &lt;- factor(test_data, levels = c(\"known_drug\", \"test_drug\", \"placebo\"))\n# show reordered\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug test_drug placebo\n\n\nThis doesn’t change the data itself, but does make it easier to manage the data later.\nAnother useful aspect of factors is that they are stored as integers with labels. This means that you can easily convert them to numeric data:\n\n# converting factors to numeric\nas.numeric(test_data)\n\n[1] 3 2 3 1\n\n\nThis can be handy for some types of statistical analyses, and also illustrates the importance of ordering your levels appropriately.\nWe can apply this knowledge to our clinical dataset, by first observing how the data are presented when creating a basic plot:\n\n# quick and dirty plot\nplot(clinical$race)\n\n\n\n\n\n\n\n\nThe labels as presented by default are not particularly readable, and also lack appropriate capitalization and formatting. While it is possible to modify only the plot labels, we would have to do that for all of our subsequent analyses. It is more efficient to modify the levels once:\n\n# assign race data to new object \nrace &lt;- clinical$race \nlevels(race)\n\n[1] \"american indian or alaska native\"         \n[2] \"asian\"                                    \n[3] \"black or african american\"                \n[4] \"native hawaiian or other pacific islander\"\n[5] \"not reported\"                             \n[6] \"white\"                                    \n\n\nBy assigning the data to a new object, we can more easily perform manipulations without altering the original dataset.\nThe output above shows the current levels for race. We can access each level using their position in this order, combined with our knowledge of square brackets for subsetting:\n\nlevels(race)[1]\n\n[1] \"american indian or alaska native\"\n\n\nWe can modify them to improve their formatting by assigning a new level (name) of our choosing:\n\n# correct factor levels\nlevels(race)[1] &lt;- \"Am Indian\"\nlevels(race)[2] &lt;- \"Asian\" # capitalize asian\nlevels(race)[3] &lt;- \"black\"\nlevels(race)[4] &lt;- \"Pac Isl\"\nlevels(race)[5] &lt;- \"unknown\"\n# show revised levels\nlevels(race) \n\n[1] \"Am Indian\" \"Asian\"     \"black\"     \"Pac Isl\"   \"unknown\"   \"white\"    \n\n\nAlthough we’re not doing so here, we could also reorder the levels (as we did for test_data).\nOnce we are satisfied with the resulting levels, we assign the modified factor back to the original dataset:\n\n# replace race in data frame\nclinical$race &lt;- race\n# replot with corrected names\nplot(clinical$race)\n\n\n\n\n\n\n\n\nThis section was a very brief introduction to factors, and it’s likely you’ll need more information when working with categorical data of your own. A good place to start would be this article, and exploring some of the tools in the tidyverse (which we’ll discuss in the next lesson).\n\nChallenge-not-reported\nIn your clinical dataset, replace “not reported” in ethnicity with NA\n\n\nChallenge-remove\nWhat Google search helps you identify additional strategies for renaming missing data?"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "title": "Working with data",
    "section": "Optional: Creating a data frame by hand",
    "text": "Optional: Creating a data frame by hand\nThis last section shows two different approaches to creating a data frame by hand (in other words, without importing the data from a spreadsheet). It isn’t particularly useful for most of your day-to-day work, and also not a method you want to use often, as this type of data entry can introduce errors. However, it’s frequently used in online tutorials, which can be confusing, and also helps illustrate how data frames are composed.\nThe first approach is to create separate vectors (columns), and then join them together in a second step:\n\n# create individual vectors\ncancer &lt;- c(\"lung\", \"prostate\", \"breast\")\nmetastasis &lt;- c(\"yes\", \"no\", \"yes\")\ncases &lt;- c(30, 50, 100)\n# combine vectors\nexample_df1 &lt;- data.frame(cancer, metastasis, cases)\nstr(example_df1)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nThe resulting data frame has column headers, identified from the names of the vectors combined together.\nThe next way seems more complex, but represents the code above combined into one step:\n\n# create vectors and combine into data frame simultaneously\nexample_df2 &lt;- data.frame(cancer = c(\"lung\", \"prostate\", \"breast\"),\n                          metastasis = c(\"yes\", \"no\", \"yes\"),\n                          cases = c(30, 50, 100), stringsAsFactors = FALSE)\nstr(example_df2)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nAs we learned above, factors can be particularly difficult, so it’s useful to know that you can use stringsAsFactors = FALSE to import such data as character instead."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "title": "Working with data",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we learned to import data into R from a csv file, learned multiple ways to access parts of data frames, and manipulated factors.\nIn the next session, we’ll begin to explore a set of powerful, elegant data manipulation tools for data cleaning, transforming, and summarizing, and we’ll prepare some data to visualize in our final session."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "title": "Working with data",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-disease-race\nExtract the last 100 rows for only disease and race and save to an object called disease_race.\n\n\nChallenge-min-max\nCalculate the minimum and maximum for days_to_death.\n\n\nChallenge-factors\nChange all of the factors of race to shorter names for each category, and appropriately indicate missing data."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "title": "Data visualization in R",
    "section": "Getting set up",
    "text": "Getting set up\nSince we are continuing to work with data in tidyverse, we need to make sure all of our data and packages are available for use.\nOpen your project in RStudio. Create a new script called class4.R, add a title, and enter the following code with comments:\n\n# load library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# read in first filtered data from last class\nbirth_reduced &lt;- read_csv(\"data/birth_reduced.csv\")\n\nRows: 4169 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in second filtered data from last class\nsmoke_complete &lt;- read_csv(\"data/smoke_complete.csv\")\n\nRows: 1152 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you have trouble accessing your data and see an error indicating the file is not found, it is likely one of the following problems:\n\nCheck to make sure your project is open in RStudio. You should see the path to your project directory (e.g., ~/Desktop/introR) appear at the top of the console (above the window showing output). If this doesn’t appear, you should save your script in your project directory, then go to File -&gt; Open Project. Navigate to the location of your project directory and open the folder, then try to reexecute your code.\nMake sure you have the two datasets (birth_reduced.csv and smoke_complete.csv) in your data directory. Please reference the materials from class 3 to filter the original clinical dataset and export these data.\n\nOnce your data are imported appropriately, we can create a quick plot:\n\n# simple plot from base R from the smoke_complete dataset\nplot(x=smoke_complete$age_at_diagnosis, y=smoke_complete$cigarettes_per_day)\n\n\n\n\n\n\n\n\nThis plot is from base R. It gives you a general idea about the data, but isn’t very aesthetically pleasing. Our work today will focus on developing more refined plots using ggplot2, which is part of the tidyverse."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "title": "Data visualization in R",
    "section": "Intro to ggplot2 and scatterplots",
    "text": "Intro to ggplot2 and scatterplots\nThere are three steps to creating a ggplot. We’ll start with a scatterplot, which is used to compare quantitative (continuous) variables.\n\nbind data: create a new plot with a designated dataset\n\n\n# basic ggplot\nggplot(data = smoke_complete) # bind data to plot\n\n\n\n\n\n\n\n\nThe last line of code creates an empty plot, since we didn’t include any instructions for how to present the data.\n\nspecify the aesthetic: maps the data to axes on a plot\n\n\n# basic ggplot\nggplot(data = smoke_complete, aes(x = age_at_diagnosis, \n                           y = cigarettes_per_day)) # specify aesthetics (axes)\n\n\n\n\n\n\n\n\nThis adds labels to the axis, but no data appear because we haven’t specified how they should be represented\n\nadd layers: visual representation of plot, including ways through which data are represented (geometries or shapes) and themes (anything not the data, like fonts)\n\n\nggplot(data = smoke_complete,\n       mapping = aes(x = age_at_diagnosis, y = cigarettes_per_day)) + \n  geom_point() # add a layer of geometry\n\n\n\n\n\n\n\n\nThe plus sign (+) is used here to connect parts of ggplot code together. The line breaks and indentation used here represents the convention for ggplot, which makes the code more readible and easy to modify.\nIn the code above, note that we don’t need to include the labels for data = and mapping =. It’s also common to include the mapping (aes) in the geom, which allows for more flexibility in customizing (we’ll get to this later!).\n\nggplot(smoke_complete) + \n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day)) \n\n\n\n\n\n\n\n\nThis plot is identical to the previous plot, despite the differences in code."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "title": "Data visualization in R",
    "section": "Customizing plots",
    "text": "Customizing plots\nNow that we have the data generally displayed the way we’d like, we can start to customize a plot.\n\n# add transparency with alpha\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), alpha = 0.1)\n\n\n\n\n\n\n\n\nTransparency is useful to help see the distribution of data, especially when points are overlapping.\n\n# change color of points\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), \n             alpha = 0.1, color = \"green\")\n\n\n\n\n\n\n\n\nFor more information on colors available, look here.\nWe can also color points based on another (usually categorical) variable:\n\n# plot disease by color\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, \n                 color = disease), \n             alpha = 0.1)\n\n\n\n\n\n\n\n\nNote the location of color= with the other aesthetics, as well as the lack of quotation marks around disease.\nColoring by a variable automatically adds a legend as well.\nWe can also change the general appearance of the plot (background colors and fonts):\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  theme_bw() # change background theme\n\n\n\n\n\n\n\n\nThis adds another layer to our plot representing a black and white theme. A complete list of pre-set themes is available here, and we’ll cover ways to customize our own themes later in this lesson.\nWhile the axes are currently sufficient, they aren’t particularly attractive. We can add a title and replace the axis labels using labs:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  labs(title = \"Age at diagnosis vs cigarettes per day\", # title\n       x=\"age (days)\", # x axis label\n       y=\"cigarettes per day\") +# y axis label\n  theme_bw()\n\n\n\n\n\n\n\n\nAnother common feature to customize involves the orientation and appearance of fonts. While this can be controlled by default themes like theme_bw), you can also control different parts independently. For example, we can make a dramatic modification to all text in the plot:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(text = element_text(size = 16)) # increase all font size\n\n\n\n\n\n\n\n\nAlternatively, you can alter only one specific type of text:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) # rotate and adjust x axis text\n\n\n\n\n\n\n\n\nThis rotates and adjusts the horizontal and vertical arrangement of the labels on only the x axis. Of course, you can also modify other text (y axis, axis labels, legend).\nAfter you’re satisfied with a plot, it’s likely you’d want to share it with other people or include in a manuscript or report.\n\n# create directory for output\ndir.create(\"figures\")\n\n\n# save plot to file\nggsave(\"figures/awesomePlot.jpg\", width = 10, height = 10, dpi = 300)\n\nThis automatically saves the last plot for which code was executed. You can view your figures/ directory to see the exported jpeg file. This command interprets the file format for export using the file suffix you specify. The other arguments dictate the size (width and height) and resolution (dpi).\n\nChallenge-scatterplot\nCreate a scatterplot showing age at diagnosis vs years smoked with points colored by gender and appropriate axis labels."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "title": "Data visualization in R",
    "section": "Box and whisker plots",
    "text": "Box and whisker plots\nBox and whisker plots compare the distribution of a quantitative variable among categories.\n\n# creating a box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day))\n\n\n\n\n\n\n\n\nThe main differences from the scatterplots we created earlier are the geom type and the variables plotted.\nWe can change the color similarly to scatterplots:\n\n# adding color\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), color = \"tomato\")\n\n\n\n\n\n\n\n\nIt seems weird to change the color of the entire box, though. A better option would be to add colored points to a black box and whisker plot:\n\n# adding colored points to black box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day)) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\n\n\n\n\nJitter references a method of randomly offsetting points slightly to allow them to be seen and interpreted more easily.\nThis method, however, effectively duplicates some data points, since all points are shown with jitter and the boxplot shows outliers. You can use an option in geom_boxplot to suppress plotting of outliers:\n\n# boxplot with both boxes and points\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), outlier.shape = NA) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\n\n\n\n\n\nChallenge-comments\nWrite code comments for each of the following lines of code. What is the advantage of writing code like this?\n\n\nmy_plot &lt;- ggplot(smoke_complete, aes(x = vital_status, y = cigarettes_per_day)) \nmy_plot +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(alpha = 0.2, color = \"purple\")\n\n\n\n\n\n\n\n\n\nChallenge-order\nDoes the order of layers in the last plot matter? What happens if jitter is coded before boxplot?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "title": "Data visualization in R",
    "section": "Time series data as line plots",
    "text": "Time series data as line plots\nSo far we’ve been able to work with the data as it appears in our filtered dataset. Now that we’re moving on to time series plots (changes in variables over time), we need to manipulate the data. We’ll also be working with the birth_reduced dataset, which we created last class (primarily by removing all missing data for year of birth). We’d like to plot the number of individuals in the dataset born by year, so we need to first count our observations based on both disease and year of birth:\n\n# count number of observations for each disease by year of birth\nyearly_counts &lt;- birth_reduced %&gt;%\n  count(year_of_birth, disease) \n\nWe can plot these data as a single line:\n\n# plot all counts by year\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n))\n\n\n\n\n\n\n\n\nHere, n represents the number of patients born in each year, from the count table created above. The result isn’t very satisfying, because we also grouped by disease. We can improve this by plotting each disease on a separate line, which is more appropriate when there are multiple data points per year:\n\n# plot one line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, \n                group = disease))\n\n\n\n\n\n\n\n\nMoreover, we can color each line individually:\n\n# color each line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, color = disease))\n\n\n\n\n\n\n\n\nNote that you don’t have to include a separate argument for group = disease because grouping is assumed by color = disease.\n\nChallenge-line\nCreate a line plot for year of birth and number of patients with lines representing each gender. Hint: you’ll need to manipulate the birth_reduced dataset first.\n\n\nChallenge-dash\nHow do you show differences in lines using dashes/dots instead of color?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "title": "Data visualization in R",
    "section": "Faceting",
    "text": "Faceting\nSo far we’ve been working on building single plots, which can show us two main variables (for the x and y axes) and additional variables using color (and potentially size/shape/etc). Scientific visualizations often need to compare among categories (e.g., control vs various treatments), which is generally clearer if those categories are presented in separate panels. ggplot provides this capacity through faceting.\nLet’s revisit the scatterplot we initially created, plotting age at diagnosis by cigarettes per day, with points colored by disease. We add an additional layer to create facets, or separate panels, for a given variable (in this case, the same variable being used to color points):\n\n# use previous scatterplot, but separate panels by disease\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(disease)) # wraps panels to make a square/rectangular plot\n\n\n\n\n\n\n\n\nvars is used for faceting in the same way that aes() is used for mapping: it is used to specify the variable to form facet groups.\nfacet_wrap determines how many rows and columns of panels are needed to create the most square-shaped final plot possible. This becomes useful when there are many more categories:\n\n# add a variable by leaving color but changing panels to other categorical data\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(tumor_stage))\n\n\n\n\n\n\n\n\nIn this case, we’re now visualizing an additional variable (tumor stage), in addition to the original three (age at diagnosis, cigarettes per day, and disease).\nIf you want to control the specific layout of panels, you can use facet_grid instead of facet_wrap:\n\n# scatterplots with panels for vital status in one row\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status)) \n\n\n\n\n\n\n\n\nThis method can also plot panels in columns.\nWe may want to show interactions between two categorical variables, by arranging panels into rows according to one variable and columns according to another:\n\n# add another variable using faceting\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status), cols = vars(disease)) # arrange plots via variables in rows, columns\n\n\n\n\n\n\n\n\nDon’t forget to look at the help documentation (e.g., ?facet_grid) to learn more about additional ways to customize your plots!\n\nChallenge-panels\nAlter your last challenge plot of (birth year by number of patients) to show each gender in separate panels.\n\n\nChallenge-axis\nHow do you change axis formatting, like tick marks and lines? Hint: You may want to use Google!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "title": "Data visualization in R",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis material introduced you to ggplot as a tool for data visualization, allowing you to now create publication-quality images using R code. Combined with our previous explorations of the basic principles of R syntax, importing and extracting data with base R, and manipulating data using tidyverse, you should be equipped to continue learning about R on your own and developing code to meet your research needs.\nIf you are interested in learning more about ggplot: - Documentation for all ggplot features is available here. - RStudio also publishes a ggplot cheat sheet that is really handy!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "title": "Data visualization in R",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-improve\nImprove one of the plots previously created today, by changing thickness of lines, name of legend, or color palette http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html",
    "href": "resources/computing/intro_to_r/3_tidyverse.html",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "title": "Data manipulation with tidyverse",
    "section": "Installing and loading packages",
    "text": "Installing and loading packages\nPlease ensure RStudio is open with your project directory path (e.g., ~/Desktop/intro_r) listed at the top of your Console. If you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script called class3.R, and add # Introduction to R: Class 3 as a title.\nFor this lesson, we’ll be working with a group of R packages called tidyverse. A package is a group of related functions that help you accomplish particular tasks. tidyverse packages have been designed specifically to support tasks related to data science, such as data manipulation, filtering, and visualization.\nThe first thing we need to do is install the software:\nA few notes about installing packages:\n\nYou only need to perform this installation once per computer, or when updating R or the package.\nIf you see red text output in the Console during this installation, don’t be alarmed: this doens’t necessarily indicate a problem. You are seeing a report of the various pieces of software being downloaded and installed.\nIf prompted, you should install all packages (say yes or all), as well as yes to compiling any packages\nWhen the installation is complete (this may take several minutes), you’ll see the command prompt (&gt;) in your Console.\n\nOnce you have the software installed, you’ll need to load it:\n\n# load library/package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading packages is similar to opening a software application on your computer; it makes a previously installed set of software available for use. A few notes about loading packages:\n\nYou’ll need to load packages every time you open RStudio (or R restarts)\nLoading tidyverse loads a collection of packages; these are listed under “Attaching packages”\nThere are many other packages included as dependencies. If some of them did not install successfully, you will receive an error at this step. For this lesson, you can try library(dplyr), and ask your instructor for help later.\nThe section in the output above referencing “Conflicts” shows you which functions you just loaded have names identical to packages you already have loaded (in base R). This shouldn’t affect the code we write in this lesson, though it’s useful to know the double colon syntax (::) allows you to reference functions in a different package with same name.\n\nYou can check to make sure the new package we’ll be using is available by executing ?select in the Console, or by searching for that function in the help panel. You can also look in the “Packages” tab in the same panel. If the package (in this case, either tidyverse or dplyr) is present in the list, it’s installed. If the box next to the package name is checked, it’s loaded. In this lesson, if you receive an error saying a function isn’t available or recognized, check to make sure the package is loaded."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "title": "Data manipulation with tidyverse",
    "section": "Selecting columns and rows",
    "text": "Selecting columns and rows\nThe first task we’ll undertake with our newly installed tidyverse tools is importing our data:\n\n# import data\nclinical &lt;- read_csv(\"data/clinical.csv\")\n\nRows: 6832 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this function looks similar to what we used in the last lesson (read.csv), but the underscore replacing the dot means it’s actually a different function. In fact, this is the data import function from tidyverse. The output provided by this function indicates a few key differences with our import yesterday.\nThe data import execution includes a description of how each variable (column) is interpreted. In our data’s case, the numeric data are col_double and the character data are col_character (not factors!).\nWe can explore these differences further:\n\n# inspect object\nstr(clinical)\n\nspc_tbl_ [6,832 × 20] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ primary_diagnosis          : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr [1:6832] \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : num [1:6832] 24477 26615 28171 27154 29827 ...\n $ vital_status               : chr [1:6832] \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr [1:6832] \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : num [1:6832] 371 136 2304 NA 146 ...\n $ state                      : chr [1:6832] \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : num [1:6832] -24477 -26615 -28171 -27154 -29827 ...\n $ site_of_resection_or_biopsy: chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : num [1:6832] NA NA 2099 3747 NA ...\n $ cigarettes_per_day         : num [1:6832] 10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : num [1:6832] NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr [1:6832] \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : num [1:6832] 1936 1931 1927 1930 1923 ...\n $ race                       : chr [1:6832] \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr [1:6832] \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : num [1:6832] 2004 2003 NA NA 2004 ...\n $ bcr_patient_barcode        : chr [1:6832] \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr [1:6832] \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   primary_diagnosis = col_character(),\n  ..   tumor_stage = col_character(),\n  ..   age_at_diagnosis = col_double(),\n  ..   vital_status = col_character(),\n  ..   morphology = col_character(),\n  ..   days_to_death = col_double(),\n  ..   state = col_character(),\n  ..   tissue_or_organ_of_origin = col_character(),\n  ..   days_to_birth = col_double(),\n  ..   site_of_resection_or_biopsy = col_character(),\n  ..   days_to_last_follow_up = col_double(),\n  ..   cigarettes_per_day = col_double(),\n  ..   years_smoked = col_double(),\n  ..   gender = col_character(),\n  ..   year_of_birth = col_double(),\n  ..   race = col_character(),\n  ..   ethnicity = col_character(),\n  ..   year_of_death = col_double(),\n  ..   bcr_patient_barcode = col_character(),\n  ..   disease = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou may notice the presence of tbl and related labels in the classes for this object. tbl stands for tibble, which is a type of data frame with specific constraints to ensure better data handling.\nIf you preview the dataset, it will look the same, and we can interact with the data in the same way. These assumptions about the data mesh nicely with the other tools in the tidyverse.\nNow that our data are imported, we can explore the tidyverse functions for extracting parts of the dataset.\nFirst, we can explore selecting certain columns by name:\n\n# selecting columns with tidyverse (dplyr)\nsel_columns &lt;- select(clinical, tumor_stage, ethnicity, disease)\n\nThe syntax for the select function is to specify the dataset first, then the names of each of the columns you would like to retain in the output object. If we look at the object, we’ll see it has only three columns but all rows.\nYou’ll note that the column headers don’t require quotation marks; this is a shortcut programmed into tidyverse functions.\nAs with base R functions, we can also select a range of columns:\n\n# select range of columns\nsel_columns2 &lt;- select(clinical, tumor_stage:vital_status)\n\nIn addition to these approaches, we can also use other helper functions for selecting columns: starts_with(), ends_with(), and contains() are examples that assist in extracting columns with headers that meet certain conditions. For example, using starts_with(tumor) in place of the column names will give you all columns that start with the word tumor.\nWe can use a separate function to extract rows that meet particular conditions:\n\n# select rows conditionally: keep only lung cancer cases\nfiltered_rows &lt;- filter(clinical, disease == \"LUSC\") \n\nThe syntax here is similar to select, and the conditional filters can be applied in similarly to base R functions.\n\nChallenge-columns\nCreate a new object from clinical called race_disease that includes only the race, &gt; ethnicity, and disease columns.\n\n\nChallenge-rows\nCreate a new object from race_disease called race_BRCA that includes only BRCA (from disease)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "title": "Data manipulation with tidyverse",
    "section": "Combining commands",
    "text": "Combining commands\nThe last challenges used an intermediate object to obtain an object with two subsetting methods applied. It’s common in data science to apply more than one requirement for extracting data. If you want to avoid creating an intermediate object, you could nest one command inside the other:\n\n# same task as challenges, but nested commands \nrace_BRCA2 &lt;- select(filter(clinical, disease == \"BRCA\"), race, ethnicity, disease)\n\nIn this case, filter(clinical, disease == \"BRCA\") becomes the input for select.\nWhile this is a common approach, especially in base R, it can be difficult for us as coders to read and interpret the code.\nOne of the most useful features of tidyverse is its inclusion of a programming method called pipes. This approach can be found in many programming languages, in part because of its utility: a pipe sends the output from the lefthand side of the symbol as the input for the righthand side. In R, pipes are represented as %&gt;%.\nWe can use pipes to connect the same two data extraction tasks:\n\n# same task as above, but with pipes\npiped &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(disease == \"BRCA\")\n\nThe command above starts by naming the object that will result from this assignment. The dataset is named as the first input. Because executing the name of an object sends the object contents as output, this means the second line receives the object as input. The output from the select line is sent as input to the filter line. This effectively demonstrates how pipes can be used to connect multiple commands together.\n\nNow that we are running code in chunks that span multiple lines, you can see one of the other nice features of RStudio: your cursor can be placed on any line of the multi-line chunk when you execute, and the entire set of code will run together.\n\nThese examples also help highlight the importance of style and convention in code formatting. After the first line, the code is indented. While this isn’t necessary for the code to work, it does make it a lot easier to read and understand the code.\nLet’s take a look at another example of piped commands:\n\n# extract race, ethinicity, and disease from cases born prior to 1930\npiped2 &lt;- clinical %&gt;%\n  filter(year_of_birth &lt; 1930) %&gt;%\n  select(race, ethnicity, disease)\n\nIn the code above, we’re applying a mathematical condition to find specific rows, and the selecting certain columns. Does the order of commands differ? We can switch the order of the filter and select lines to see:\n\npiped3 &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(year_of_birth &lt; 1930)\n\nThe code above should give you an error, because in this case, the order does matter! The output from the second line does not include the year_of_birth column, so R is unable to apply the filter in the third line.\n\nChallenge-pipes\nUse pipes to extract the columns gender, years_smoked, and year_of_birth from the object clinical for only living patients (vital_status) who have smoked fewer than 1 cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "title": "Data manipulation with tidyverse",
    "section": "Mutate",
    "text": "Mutate\nThis lesson so far has mostly shown new ways of accomplishing the same tasks we learned in the last lesson. tidyverse includes much more functionality, however, including the ability to mutate columns. Common tasks for which mutate is useful include unit conversions, transformation, and creating ratios from among existing columns.\nWe can use this function to convert the days_to_death column to years:\n\n# convert days to years\nclinical_years &lt;- clinical %&gt;%\n  mutate(years_to_death = days_to_death / 365)\n\nThe actual conversion works by providing a formula (days_to_death / 365) and the name of the new column (years_to_death). If you inspect the resulting object, you’ll see years_to_death added as a new column at the end of the table.\n\nmutate works by retaining all previous columns and creating new columns as per the formula specified. tidyverse also includes transmute, which drops the existing columns used to calculcate the new columns.\n\nWe can use mutate to perform multiple conversions at once:\n\n# convert days to year and months at same time, and we don't always need to assign to object\nclinical %&gt;%\n  mutate(years_to_death = days_to_death / 365,\n         months_to_death = days_to_death / 30) %&gt;%\n  glimpse() # preview data output\n\nRows: 6,832\nColumns: 22\n$ primary_diagnosis           &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ tumor_stage                 &lt;chr&gt; \"stage ia\", \"stage ib\", \"stage ib\", \"stage…\n$ age_at_diagnosis            &lt;dbl&gt; 24477, 26615, 28171, 27154, 29827, 23370, …\n$ vital_status                &lt;chr&gt; \"dead\", \"dead\", \"dead\", \"alive\", \"dead\", \"…\n$ morphology                  &lt;chr&gt; \"8070/3\", \"8070/3\", \"8070/3\", \"8083/3\", \"8…\n$ days_to_death               &lt;dbl&gt; 371, 136, 2304, NA, 146, NA, 345, 716, 280…\n$ state                       &lt;chr&gt; \"live\", \"live\", \"live\", \"live\", \"live\", \"l…\n$ tissue_or_organ_of_origin   &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_birth               &lt;dbl&gt; -24477, -26615, -28171, -27154, -29827, -2…\n$ site_of_resection_or_biopsy &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_last_follow_up      &lt;dbl&gt; NA, NA, 2099, 3747, NA, 3576, NA, NA, 1810…\n$ cigarettes_per_day          &lt;dbl&gt; 10.9589041, 2.1917808, 1.6438356, 1.095890…\n$ years_smoked                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 26…\n$ gender                      &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"male\", …\n$ year_of_birth               &lt;dbl&gt; 1936, 1931, 1927, 1930, 1923, 1942, 1953, …\n$ race                        &lt;chr&gt; \"white\", \"asian\", \"white\", \"white\", \"not r…\n$ ethnicity                   &lt;chr&gt; \"not hispanic or latino\", \"not hispanic or…\n$ year_of_death               &lt;dbl&gt; 2004, 2003, NA, NA, 2004, NA, 2005, 2006, …\n$ bcr_patient_barcode         &lt;chr&gt; \"TCGA-18-3406\", \"TCGA-18-3407\", \"TCGA-18-3…\n$ disease                     &lt;chr&gt; \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"L…\n$ years_to_death              &lt;dbl&gt; 1.0164384, 0.3726027, 6.3123288, NA, 0.400…\n$ months_to_death             &lt;dbl&gt; 12.366667, 4.533333, 76.800000, NA, 4.8666…\n\n\nThe code above also features a new function, glimpse, that can be useful when developing new piped code. Note that we did not assign the output above to a new object; we allowed it to be printed to the Console. Because this is a large dataset, that type of output can be unweildy. glimpse allows us to see a preview of the data, including the two new columns created.\n\nChallenge-lung\nExtract only lung cancer patients (LUSC, from disease) and create a new column called total_cig representing an estimate of the total number of cigarettes smoked (use columns years_smoked and cigarettes_per_day)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "title": "Data manipulation with tidyverse",
    "section": "Split-apply-combine",
    "text": "Split-apply-combine\nOur clinical dataset includes categorical (character) data. One example is the gender column. We can assess the different categories available using a base R function:\n\n# show categories in gender\nunique(clinical$gender)\n\n[1] \"male\"   \"female\" NA      \n\n\ntidyverse includes an approach called split-apply-combine that allows us to:\n\nsplit data into groups,\napply a task for each group,\ncombine the results back together into a single table.\n\nWe can try out this approach by counting the number of each gender in our dataset:\n\n# count number of individuals of each gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  tally() \n\n# A tibble: 3 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female  3535\n2 male    3258\n3 &lt;NA&gt;      39\n\n\ngroup_by is not particularly useful by itself, but powerful together with a second function like tally. The two columns in the resulting tibble represent the categories from group_by and the number of cases for each gender (n).\nAn additional function for use with group_by is summarize:\n\n# summarize average days to death by gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female               947.\n2 male                 826.\n3 &lt;NA&gt;                 NaN \n\n\nSimilar to mutate, we provide summarize with a formula indicating how we would like the groups to be handled.\nIn the command above, we use na.rm = TRUE to exclude missing data from the calculation of mean from days_to_death. We still have NA reported in the output table, though, because of the NA category in gender.\nWe can apply an additional filter to remove this missing data, prior to grouping:\n\n# remove NA\nclinical %&gt;%\n  filter(!is.na(gender)) %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death))\n\n# A tibble: 2 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female                 NA\n2 male                   NA\n\n\n\nChallenge-smoke-complete\nCreate object called smoke_complete from clinical that contains no missing data for cigarettes per day or age at diagnosis.\n\n\nChallenge-save\nHow do you save resulting table to file? How would you find this answer?\n\nThe solution to the challenges above represent the first of two datasets we’ll be using for data visualization in our next class. Make sure you’ve executed this code to save the filtered data file for use next time:\n\nsmoke_complete &lt;- clinical %&gt;%\n  filter(!is.na(age_at_diagnosis)) %&gt;%\n  filter(!is.na(cigarettes_per_day))\nwrite_csv(smoke_complete, \"data/smoke_complete.csv\")\n\nThe command above uses write_csv, which is the tidyverse method of saving a csv file. Base R possesses a function, write.csv, that performs a similar task, but by default includes quotation marks around cells with character data as well as row names (sequential numbers, unless otherwise specified).\n\nChallenge-birth-complete\nCreate a new object called birth_complete that contains no missing data for year of birth or vital status.\n\nThis challenge begins filtering the second of our two datasets for next time. Make sure you include the filter to remove missing data that’s been encoded as “not reported”!\n\n# make sure ALL missing data is removed!\nbirth_complete &lt;- clinical %&gt;%\n  filter(!is.na(year_of_birth)) %&gt;%\n  filter(!is.na(vital_status)) %&gt;%\n  filter(vital_status != \"not reported\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "title": "Data manipulation with tidyverse",
    "section": "Filtering data based on number of cases of each type",
    "text": "Filtering data based on number of cases of each type\nWe’re going to perform one last manipulation on this second dataset for next time, which will allow us to reduce the total number of cancer types present in this dataset.\nFirst, we’ll need to count how many cases for each cancer type exist in the dataset:\n\n# counting number of records in each cancer\ncancer_counts &lt;- clinical %&gt;%\n  count(disease) %&gt;%\n  arrange(n) \n\nThe count function is similar to tally, but doesn’t need to have group_by applied first. The arrange function added at the end sorts the table using the column specified. Although this isn’t necessary for the analysis to proceed, it makes it easier for us to interpret the results.\nNext, we’ll identify which cancer types are represented by at least 500 cases in this dataset:\n\n# get names of frequently occurring cancers\nfrequent_cancers &lt;- cancer_counts %&gt;%\n  filter(n &gt;= 500) \n\nWe can then use this object to filter based on the number of cases:\n\n# extract data from cancers to keep\nbirth_reduced &lt;- birth_complete %&gt;%\n  filter(disease %in% frequent_cancers$disease)\n\nThe new syntax here is %in%, which allows you to compare each entry in disease from birth_complete to the disease column in frequent_cancers (remember that frequent_cancers$disease means the disease column from frequent_cancers). This keeps only cases from the birth_complete dataset that are from cancers that are frequently occurring.\nFinally, we’ll write the final output to a file:\n\n# save results to file in data/ named birth_reduced\nwrite_csv(birth_reduced, \"data/birth_reduced.csv\")\n\n\nChallenge-tumor\nExtract all tumor stages with more than 200 cases (Hint: also check to see if there are any other missing/ambiguous data!)"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "title": "Data manipulation with tidyverse",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we acquainted ourselves with tidyverse, and learned some tools for data filtering and manipulation. We covered examples from many of the main categories of data manipulation tasks; if you’d like more information on these functions and others available (including methods of joining multiple tables together), please check out the dplyr cheatsheet.\nIn the next session, we’ll wrap up the course by creating publication-quality images using ggplot2, a data visualization package in tidyverse, and the two datasets we filtered in the sections above."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "title": "Data manipulation with tidyverse",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-ethnicity\nHow many hispanic or latino individuals in clinical are not also white? What are their races?\n\n\nChallenge-years\nCreate a new column for clinical called age_at_death that calculates this statistic (in years) from year_of_birth and year_of_death.\n\n\nChallenge-helpers\ndplyr includes several “helpers” that allows selection of columns meeting particular criteria (described on the first page of the dplyr cheatsheet near the top of the right hand column: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf). Using one of these tools, extract all columns that include “diagnosis”.\n\n\nChallenge-combine\nHow many patients are hispanic or latino patients (column ethnicity), died after the year 2000 (year_of_death), and possess no missing data for cigarettes per day?"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html",
    "href": "resources/computing/intro_to_python/good_practice_python.html",
    "title": "Good practices in Python",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "title": "Good practices in Python",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "title": "Good practices in Python",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ndef calculate_mean(data):\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / len(data)\n    \n    return mean_value\n\n# Avoid: Lack of comments and explanation\ndef calc_mean(d):\n    t = sum(d)\n    m = t / len(d)\n    return m"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Python",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ndef calculate_variance(data):\n    n = len(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance\n\n# Avoid: Using global variables in functions\nmean_value = 0\ndef calc_variance(data):\n    n = len(data)\n    global mean_value\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance"
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "All the additional resources",
    "section": "",
    "text": "Supplementary material for the course\n\n\n\n\n\nTips and tricks\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nBest practices for collaborative work\n\n\nUsing code style guidelines, sharing code and collaborating.\n\n\n\n\n\n\nGood practices for coding\n\n\nA collection of tips and tricks to write better code\n\n\n\n\n\n\nReproducible scientific computing\n\n\nWhy and how to make your scientific computing reproducible.\n\n\n\n\n\n\nVirtual environments\n\n\nHow to make it possible for others to run your code with the same dependencies as you\n\n\n\n\n\n\nNo matching items\n\n\n\nTutorials\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nGit and GitHub\n\n\nA guide to set-up Git and GitHub\n\n\n\n\n\n\nGithub Classroom\n\n\nHow to use Github Classroom to submit assignments\n\n\n\n\n\n\nSoftware installation\n\n\nHow to have everything ready for the course\n\n\n\n\n\n\nNo matching items\n\n\n\nIntroduction to Programming\n\n\n\n\n\n\nNote\n\n\n\nYou might want to choose from other tutorials as they might suit your learning style better. See the additional ressources for each language.\n\n\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nCheatsheets\n\n\nA collection of cheatsheets forR and Python\n\n\n\n\n\n\nIntroduction to R\n\n\nA collection of resources for learning R\n\n\n\n\n\n\nIntroduction to Julia\n\n\nA collection of resources for learning Julia\n\n\n\n\n\n\nIntroduction to Python\n\n\nA collection of resources for learning Python\n\n\n\n\n\n\nNo matching items\n\n\nAdditional resources\n\nSetting up your Data Science Project\nR for Data Science (2e)\nPython for Data Analysis (3e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data",
    "crumbs": [
      "Resources",
      "All resources"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#content",
    "href": "course-syllabus.html#content",
    "title": "Syllabus",
    "section": "Content",
    "text": "Content\n\nModern statistical computing environments (e.g., R, Julia, and Python)\nAids to efficiency and reproducibility (e.g., GitHub, Markdown)\nData management, wrangling, and ethics\nStatistical graphics (grammar, good practices, applications, and examples)\nKernel density estimation and smoothing\nCross-validation\nEM algorithm and applications\nResampling methods for uncertainty assessment (bootstrap, jackknife, cross-validation), with applications to regression, time series and dependent data\nMonte Carlo methods for sampling and numerical integration\nIntroduction to Bayesian inference\nMarkov chain Monte Carlo techniques (Gibbs sampler, Metropolis-Hastings algorithm, Hamiltonian Monte Carlo, convergence diagnostics)\nDecision trees for classification or Conformal prediction",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nRequired courses: Probability and statistics, Linear models",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the course, the student must be able to:\n\nPlan complex visualisation and computational tasks\nPerform complex visualisation and computational tasks\nImplement reproducible computational solutions to statistical problems in modern environments and platforms\nExpound the main approaches used for problem solving",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#transversal-skills",
    "href": "course-syllabus.html#transversal-skills",
    "title": "Syllabus",
    "section": "Transversal skills",
    "text": "Transversal skills\n\nTake feedback (critique) and respond in an appropriate manner\nDemonstrate the capacity for critical thinking\nIdentify the different roles that are involved in well-functioning teams and assume different roles, including leadership roles\nWrite a scientific or technical report",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "notes/week_08.html",
    "href": "notes/week_08.html",
    "title": "Monte Carlo Methods",
    "section": "",
    "text": "Monte Carlo is an omnibus name for computational algorithms in statistics, which rely on repeated random sampling in order to produce numerical results. Major areas of use include\nBut more generally, the basic idea is as follows: if we are able to generate samples from a complicated process, we can learn many things about the process with precision dictated by the amount of samples we draw. This way, we can sometimes use Monte Carlo to test hypotheses, construct confidence intervals, etc. As such, the name Monte Carlo is often used a synonym for simulation-based statistical techniques.\nExample: Let us return to the first one from the previous lecture, where we observe a random sample \\((X_1,Y_1)^\\top,\\ldots,(X_N,Y_N)^\\top\\) from a bivariate Gaussian distribution \\[\n\\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 & \\rho \\\\ \\rho & \\sigma_2^2 \\end{pmatrix} \\right)\n\\] and wish to test \\(H_0: \\rho = \\rho_0\\) for some fixed value of \\(\\rho_0\\). If we are willing to assume that \\(\\rho\\) is the only unknown parameter (which happens, e.g., when the data come from a standardized distribution with \\(\\mu_1 = \\mu_2 = 0\\) and \\(\\sigma_1 = \\sigma_2 = 1\\)), we can simulate from the distribution of the empirical estimator \\(\\widehat{\\rho} = N^{-1} \\sum_{n=1}^N X_n Y_n\\) under the null (under the null, we know everything, and hence we can simply draw new data sets) and hence we can use this simulation to calculate the \\(p\\)-value for our test. However, in practice, we would likely not have known the nuisance parameters \\(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2\\) (the distribution would not have been standardized) and we would have to somehow take care of them (maybe we would standardize the observed data set empirically before continuing). But how can we know whether our \\(p\\)-value would still be valid?\nThe name Monte Carlo refers to a casino in Monaco, where an uncle of one of the method’s inventors (Stanislaw Ulam) used to loose a significant amount of money by playing roulette. With a bit of exaggeration, we could say that with every single roll (and his corresponding bet), the uncle learnt a bit more about the fact that the casino always wins in a long run. In the simple case of roulette, everything can of course be calculated directly. But with more complicated processes it can be easier to draw samples and statistically evaluate those, compared to analyzing the processes themselves.\nThe prerequisite for doing Monte Carlo is being able to draw samples from well-known distributions.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#the-uniform-distribution-on-01",
    "href": "notes/week_08.html#the-uniform-distribution-on-01",
    "title": "Monte Carlo Methods",
    "section": "The Uniform Distribution on \\([0,1]\\)",
    "text": "The Uniform Distribution on \\([0,1]\\)\nIt may be surprising that even the linear congruential method forming a sequence \\(\\{X_n\\}_{n \\geq 1}\\) by setting \\[ X_{n} = (a X_{n-1} + c) \\,\\mathrm{mod}\\, m, \\quad n=1,2,\\ldots, \\] where \\(a,c,m\\) and \\(X_0\\) are cleverly chosen to fulfill the requirements above. The linear congruential method will naturally produce only a finite sequence of numbers before starting to repeat itself. A length of such sequence, known as the period, can however be very large (e.g. \\(10^{12}\\)) depending on the choice of the constants, so this is hardly a practical problem. Random numbers between \\(0\\) and \\(1\\) can then be derived as \\(X_n/m\\).\nActually, the fact that a sequence of numbers appears to be random even though every number in the sequence is a deterministic function of the previous number(s) has one huge advantage: reproducibility. Even though the linear congruential method in its simplest form above is not used, one can think of setting the seed (by set.seed() in R) as choosing the staring point (\\(X_0\\)) in the finite sequence produced by the linear congruential method, from which a sub-sequence (the random sample of length \\(N\\) is sampled). At any later point in time, the same sub-sequence can be obtained by using the same generator starting from the same starting point.\nWhile it is beyond the scope of this course to dive any deeper into the base-line pseudo-RNG algorithms, we should keep several points in mind:\n\nRNG used in statistics is only pseudo-random, i.e., it appears to be random, even though it is fully deterministic.\nRNG algorithms are constantly improving. Now, we believe to have a “perfect” algorithm for our purposes provided with our software.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#other-distributions",
    "href": "notes/week_08.html#other-distributions",
    "title": "Monte Carlo Methods",
    "section": "Other Distributions",
    "text": "Other Distributions\nNow that we know how to generate data from the \\(\\mathcal{U}(0,1)\\) distribution (at least for most practical purposes), we can formally use the following result to generate data from any other distribution.\nLemma. (Inverse transform.) Let \\(U \\sim \\mathcal{U}(0,1)\\). Then, for a distribution function \\(F\\), the random variable \\(X = F^{-1}(U)\\) has distribution \\(F\\).\nProof. Simply \\(P(X \\leq x) = P\\{F(X) \\leq F(x)\\} = P\\{U \\leq F(x)\\} = F(x)\\).\nThe lemma above silently assumes existence of the inverse \\(F^{-1}\\), but the inverse can be replaced by the generalized inverse \\(F^-(y) = \\inf\\{x \\,|\\, F(x) \\geq y\\}\\), and the proof is just slightly more complicated.\nWhile the theorem above seems almighty, it is not. For instance, to generate from \\(\\mathcal{N}(0,1)\\), we would need to work with the distribution function of standard normal, which does not have an analytic expression, hence numerical approximation is needed again. Software usually has built-in functions for standard distributions, and they should be used, e.g., rnorm() to generate from \\(N(0,1)\\), and those are often based on more simple and efficient transformation relationships between distributions, e.g., \\(a X + b \\sim \\mathcal{N}(b,a^2)\\) for \\(X \\sim \\mathcal{N}(0,1)\\). Many other examples are shown on this diagram, e.g., the standard uniform distribution can be transformed to the exponential distribution from which Laplace, Weibull, Gamma, or Chi-squared distributions can be easily obtained. Looking carefully at the diagram, one can notice that an important path is missing: the generation of \\(\\mathcal{N}(0,1)\\) from \\(\\mathcal{U}(0,1)\\). This can be achieved, e.g., by using the Box-Muller transform.\nLemma. (Box-Muller transform.) Let \\(U_1,U_2 \\sim \\mathcal{U}(0,1)\\) be independent. Then \\[\nZ_1 = \\sqrt{-2 \\log(U_1)} \\cos(2\\pi U_2) \\quad \\& \\quad Z_2 = \\sqrt{-2 \\log(U_1)} \\sin(2\\pi U_2)\n\\] are two independent standard Gaussian random variables.\nProof. Calculation based on the density transformation theorem and on deriving the distribution of the polar coordinates of \\((Z_1,Z_2)\\).\nThe Box-Muller transform is easy to be implemented, but it may not be the best way to generate from the standard Gaussian since evaluating trigonometric functions is rather expensive.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#rejection-sampling",
    "href": "notes/week_08.html#rejection-sampling",
    "title": "Monte Carlo Methods",
    "section": "Rejection Sampling",
    "text": "Rejection Sampling\nAssume we wish to simulate from a target density \\(f\\) and we know how to simulate from a proposal density \\(g\\). Assume the support of \\(f\\) is included in that of \\(g\\), i.e., \\(\\forall x: f(x) &gt; 0 \\Rightarrow g(x) &gt; 0\\), and there exists \\(c &gt; 1\\) such that \\(\\forall x : f(x) \\leq c g(x)\\), i.e., \\(\\underset{x}{\\sup} \\frac{f(x)}{g(x)}=c &lt; \\infty\\). Here, the assumption on the support can/should be relaxed by assuming that the support of the target density \\(f\\) is contained in that of \\(g\\). The following algorithm can then be used to draw a sample \\(X\\) from \\(f\\):\n\nDraw a proposal \\(Y\\) from \\(g\\).\nDraw \\(U \\sim \\mathcal{U}(0,1)\\).\nIf \\(U  \\leq \\frac{1}{c} \\frac{f(Y)}{g(Y)}\\), accept \\(X = Y\\) and stop, otherwise go back to 1.\n\nIn the example of the standard uniform proposal and the beta \\(\\mathcal{B}(2,5)\\) target below, it is easy to see that the algorithm works (really generates from the target \\(f\\)) and that the probability of stopping in step 3 (the acceptance probability) of the algorithm above is equal to \\(1/c\\).\n\n\n\n\n\n\n\n\n\nThe acceptance probability is clear even more generally, since we sample the uniform pair \\((Y,U)\\) from a mass \\(M\\) (the area under \\(M g(x)\\)) and accept iff the sample hits the area under \\(f(x)\\), which is one. For validity of the algorithm, note that the outcome \\(X\\) of the algorithm is really distributed according to \\(f\\): \\[\n\\begin{split}\nP(X \\leq x) &= P\\Bigg\\lbrace Y \\leq x \\,\\Bigg|\\, U \\leq \\underbrace{\\frac{1}{c} \\frac{f(Y)}{g(Y)}}_{=:t(Y)} \\Bigg\\rbrace\n= \\frac{P\\lbrace Y \\leq x \\,\\wedge\\, U \\leq t(Y) \\rbrace}{P\\lbrace U \\leq t(Y) \\rbrace}\\\\\n&= \\frac{\\int_{-\\infty}^x \\int_{0}^{t(y)}d u\\, g(y) d y}{\\int_{-\\infty}^{+\\infty} \\int_{0}^{t(y)}d u\\, g(y) d y}\n= \\frac{\\int_{-\\infty}^x t(y) g(y) d y}{\\int_{-\\infty}^{+\\infty} t(y) g(y) d y} = \\frac{\\int_{-\\infty}^x \\frac{1}{c} f(y) d y}{\\int_{-\\infty}^{+\\infty} \\frac{1}{c} f(y) d y} =\\frac{\\frac{1}{c} F(x)}{\\frac{1}{c}} = F(x),\n\\end{split}\n\\] where \\(F\\) is the distribution function corresponding to the density \\(f\\).\nSimilarly to the inverse transform method, rejection sampling is quite general and can be used to sample from virtually all distributions (for discrete distributions, one can work with probability mass functions instead of densities; multivariate distributions can be treated the same), as long as a proposal density can be found. However, it’s efficiency is dictated by how efficiently the target and proposal densities can be evaluated and how high is the acceptance probability \\(P\\lbrace U \\leq t(Y) \\rbrace=1/c\\) (basically we want the target and proposal densities to have a similar shape in order to have a low constant \\(c \\approx 1\\).\nExample: We wish to simulate data from the standard Gaussian target using the doubly exponential proposal, i.e., \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\quad \\& \\quad g(x) = \\frac{\\alpha}{2} e^{-\\alpha |x|}, \\text{ where } \\alpha &gt; 0, \\quad x \\in \\mathbb{R}\n\\]\nIn the code below, I use M in place of \\(c\\), since c is sacred in R. The rate of the doubly exponential proposal is set at 1 as it minimizes the value of \\(c=\\sup f(x)/g(x)\\) and hence maximizes the acceptance probability.\n\nM &lt;- optimize(f=function(x){dnorm(x)/dexp(abs(x))*2}, interval=c(0,5),maximum=T)$objective\n\n# returns a single accepted sample from standard doubly exp. proposal and N(0,1) target\nacc_rej_dexp_norm &lt;- function(M,a,b){\n  U &lt;- runif(1)*M\n  signs &lt;- c(-1,1)\n  Y &lt;- rexp(1)*sample(signs,1) # doubly exponential\n  while (U&gt;dnorm(Y)/dexp(abs(Y))/2/M){\n    U &lt;- runif(1)*M\n    Y &lt;- rexp(1)*sample(signs,1)\n  }\n  return(Y)\n}\n\nx &lt;- 1:5000/1001\nplot(c(-rev(x),x),M*c(rev(dexp(x))/2,dexp(x)/2),type=\"l\",col=\"blue\",xlab=\"\",ylab=\"\",\n     main=\"Target and scaled proposal densities\")\npoints(c(-rev(x),x),c(rev(dnorm(x)),dnorm(x)),type=\"l\",col=\"red\")\n\nset.seed(517)\nN &lt;- 1000\nX &lt;- rep(0,N)\nfor(n in 1:N){\n  X[n] &lt;- acc_rej_dexp_norm(M,a,b)\n}\nplot(density(X),col=\"red\",main=\"KDE plots from simulated data\")\npoints(density(rnorm(N)),type=\"l\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe example above provides another way of generating from the standard Gaussian: \\[\n\\mathcal{U}[0,1] \\longrightarrow Exp(1) \\longrightarrow DoublyExp(1) \\longrightarrow N(0,1)\n\\] where the arrows correspond to transformations:\n\nfirst arrow is a log-transform,\nsecond arrow is simple, see the code above,\nthird arrow is the rejection algorithm above.\n\nR actually uses neither Box-Muller nor the rejection approach from the doubly exponential proposal. Instead it uses the inverse transform, searching through the values of \\(F^{-1}\\), which are tabulated to a very high precision in the case of standard Gaussian.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#deterministic-approaches",
    "href": "notes/week_08.html#deterministic-approaches",
    "title": "Monte Carlo Methods",
    "section": "Deterministic Approaches",
    "text": "Deterministic Approaches\nNumerical integration (of definite integrals) is an important problem and constitutes a large area of research. The most well-known and simplest deterministic method, called the quadrature method, amounts to calculating Riemann sums, i.e., evaluating the function on a grid \\(t_1,\\ldots,t_K\\) and approximating \\[\nI = \\int_a^b f(x) d x\n\\] by \\(S_K = \\{(b-a)/K\\} \\sum_{k=1}^K f(t_k)\\). As \\(K\\to \\infty\\), \\(S_K \\to I\\) (for a reasonable \\(f\\)). Actually, the formula for \\(S_K\\) above corresponds to integrating locally constant interpolation of \\(f\\) based on \\(f(t_1),\\ldots,f(t_K)\\). Of course, we can do smarter than local constant:\n\nlocal linear interpolation leads to the trapezoidal rule,\nlocal quadratic interpolation leads to the Simpson’s rule.\n\n\n\n\n\n\nFigure: Riemann sum, trapezoidal rule, and Simpson’s rule. Simpson’s rule is missing the grid, sorry about that – imagine that there is the same grid as on the first two plots, and the function is approximated quadratically between every two neighboring grid points. Source: wiki.\n\n\n\n\n\n\n\nFigure: Riemann sum, trapezoidal rule, and Simpson’s rule. Simpson’s rule is missing the grid, sorry about that – imagine that there is the same grid as on the first two plots, and the function is approximated quadratically between every two neighboring grid points. Source: wiki.\n\n\n\n\n\n\n\nFigure: Riemann sum, trapezoidal rule, and Simpson’s rule. Simpson’s rule is missing the grid, sorry about that – imagine that there is the same grid as on the first two plots, and the function is approximated quadratically between every two neighboring grid points. Source: wiki.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#naive-monte-carlo",
    "href": "notes/week_08.html#naive-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Naive Monte Carlo",
    "text": "Naive Monte Carlo\nThe naive Monte Carlo basically replaces the deterministic grid with a random one: \\(N\\) samples \\(X_1,\\ldots,X_N\\) are drawn from \\(\\mathcal{U}[a,b]\\) distribution and \\[\nI = (b-a) \\mathbb{E}\\big\\lbrace f(U) \\big\\rbrace\n\\] is approximated stochastically by \\(\\widehat{I}_N = (b-a) N^{-1} \\sum_{n=1}^N f(X_n)\\). Due to the law of large numbers, \\(\\widehat{I}_N \\to I\\) almost surely. Compared to the deterministic approaches, Monte Carlo has the advantage of easy convergence monitoring: \\(\\widehat{I}_N\\) is a random variable arising as a mean of i.i.d. random variables. It is unbiased and its variance can be naturally estimated as \\(v_n\\) below. Hence by the central limit theorem we have \\[\n\\sqrt{N}\\frac{\\widehat{I}_N - I}{v_N} \\stackrel{\\cdot}{\\sim} \\mathcal{N}(0,1) \\,, \\quad \\text{where} \\quad v^2_N = \\frac{1}{N} \\sum_{n=1}^N \\big[ f(X_n) - \\bar{f}_N \\big]^2\n\\] from which confidence intervals can be constructed and convergence checked, provided the variance exists and converges fast enough (which is not always the case). Although constructions of CIs this way is often very useful, one has to be careful about silent failure. When, e.g., \\(f\\) explodes on a very small sub-interval of \\([a,b]\\) and we have no samples on this sub-interval, the CIs will be overly optimistic. Basically if the sampling regime misses an important pattern, the variance estimate and hence CI will miss it too, so we may be overly confident in a wrong result.\nThere are other problems with the simple approaches above. They struggle with unbounded domains (often failing to find the important areas) and they are prone to the curse of dimensionality in the case of multivariate integrals. Both of these are quite common in statistics.",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_08.html#importance-sampling",
    "href": "notes/week_08.html#importance-sampling",
    "title": "Monte Carlo Methods",
    "section": "Importance sampling",
    "text": "Importance sampling\nThe typical situation in which Monte Carlo overcomes the aforementioned problems and outshines deterministic approaches is when sampling can be custom-tailored to the integrand. We can write \\[\nI := \\int_\\mathcal{X} f(x) d x = \\int_\\mathcal{X} \\frac{f(x)}{g(x)} g(x) d x =\n\\mathbb{E}_g \\left\\lbrace \\frac{f(X)}{g(X)} \\right\\rbrace.\n\\] Now sampling \\(X_1,\\ldots,X_N \\sim g\\), we can approximate \\(I\\) by \\[\n\\widehat{I} := \\frac{1}{N}\\sum_{n=1}^N \\frac{f(X)}{g(X)}.\n\\] We still have from the law of large numbers that \\(\\widehat{I} \\stackrel{a.s.}{\\to} I\\) and can construct CIs from the limiting distribution provided by CLT. Notice that the asymptotic variance of \\(\\widehat{I}\\) is \\[\n\\mathrm{AVar}\\big(\\widehat{I}\\big) = \\frac{1}{N} \\int_\\mathcal{X} \\left[ \\frac{f(x)}{g(x)} -  \\mathbb{E}_g \\left\\lbrace \\frac{f(X)}{g(X)} \\right\\rbrace \\right]^2 g(x) d x.\n\\] In particular, the variance is zero if \\(f(x)/g(x) = 1\\) on the domain \\(\\mathcal{X}\\), and is small if \\(f/g\\) is approximately constant on \\(\\mathcal{X}\\).\nIntuitively, if we integrate a very flat function, i.e., if all regions of the domain are of equal importance, very few samples suffice to have a good approximation. This is of course true also for the deterministic approaches. But the great advantage of Monte Carlo is that we can incorporate different importance of different regions of the domain into the sampling scheme (and hence the name importance sampling).\nIn statistical applications, we are often interested directly in the integrals of the form \\[\n\\mathbb{E}_g \\big\\lbrace f(X) \\big\\rbrace = \\int_\\mathcal{X} f(x) g(x) d x\n\\] and if simulation from \\(g\\) is readily available, we can proceed as above. If simulation from \\(g\\) is difficult, we can change the reference (probability) measure from \\(g\\) to some \\(h\\) \\[\n\\mathbb{E}_g \\big\\lbrace f(X) \\big\\rbrace = \\int_\\mathcal{X} f(x) \\frac{g(x)}{h(x)} h(x) d x = \\mathbb{E}_h \\left\\lbrace f(X) \\frac{g(X)}{h(X)} \\right\\rbrace\n\\] and simulate from \\(h\\) instead.\nHence we have a lot of freedom in how to sample (i.e., how to choose the reference measure), but there is a certain trade-off: we want to\n\nhave the integrand as flat as possible\nbe able to simulate from the reference measure efficiently\n\nWhen \\(\\mathcal{X} = \\mathbb{R}\\), it is important to match the decay of the tails between the target and reference measures. This is the case for both rejection and importance sampling schemes, and it makes them practically not that useful in more complicated examples (where the tail behavior might not even be known).\nNote: We also have to be careful when changing measure to always cover the whole domain \\(\\mathcal{X}\\); we cannot use, e.g., \\(\\mathcal{U}[a,b]\\) as the new reference measure when \\(\\mathcal{X} = \\mathbb{R}\\). Typically (as shown in the examples below), we are in the opposite situation: we typically have means to sample from a larger domain than \\(\\mathcal{X}\\), and samples outside of \\(\\mathcal{X}\\) are rejected. Be careful about standardization of your results in that case.\nExample: Approximately calculate \\(P(2 &lt; X &lt; 6)\\) for the target distribution \\(X \\sim f\\) using a reference \\(g\\) for\n\nGaussian target, Exponential Reference\nCauchy target, Exponential Reference\nCauchy target, Gaussian Reference\n\n\nCode\nop &lt;- par(ps=30)\nx &lt;- 6*(1:1000)/1001\nplot(x,dnorm(x),type=\"l\",ylab=\"\")\npoints(x,dexp(x),type=\"l\",col=\"blue\")\nabline(v=c(2,6),lty=2)\n\nplot(x,dnorm(x)/dexp(x),type=\"l\",ylab=\"\")\nabline(v=c(2,6),lty=2)\n\nset.seed(696)\nx=rexp(10^5)\nwein=dnorm(x)/dexp(x)\n# boxplot(wein/sum(wein))\nplot(log10(1:10^5),cumsum(wein*(x&gt;2)*(x&lt;6))/cumsum(rep(1,10^5)),type=\"l\",ylim=c(0,0.05),xlab=\"log(N)\",ylab=\"\")\nabline(a=pnorm(6)-pnorm(2),b=0,col=\"sienna\")\n\n\n\n\n\n\n\nDensities\n\n\n\n\n\n\n\nRatio of densities\n\n\n\n\n\n\n\nImportance sampling error\n\n\n\n\n\n\nGaussian target, Exponential reference\n\n\n\n\nCode\nop &lt;- par(ps=30)\nx &lt;- 6*(1:1000)/1001\nplot(x,dcauchy(x),type=\"l\",ylab=\"\")\npoints(x,dexp(x),type=\"l\",col=\"blue\")\nabline(v=c(2,6),lty=2)\n\nplot(x,dcauchy(x)/dexp(x),type=\"l\",ylab=\"\")\nabline(v=c(2,6),lty=2)\n\nset.seed(696)\nx=rexp(10^5)\nwein=dcauchy(x)/dexp(x)\n# boxplot(wein/sum(wein))\nplot(log10(1:10^5),cumsum(wein*(x&gt;2)*(x&lt;6))/cumsum(rep(1,10^5)),type=\"l\",ylim=c(0,0.3),xlab=\"log(N)\",ylab=\"\")\nabline(a=pcauchy(6)-pcauchy(2),b=0,col=\"sienna\")\n\n\n\n\n\n\n\nDensities\n\n\n\n\n\n\n\nRatio of densities\n\n\n\n\n\n\n\nImportance sampling error\n\n\n\n\n\n\nCauchy target, Exponential reference\n\n\n\n\nCode\nop &lt;- par(ps=30)\nx &lt;- 6*(1:1000)/1001\nplot(x,dcauchy(x),type=\"l\",ylab=\"\")\npoints(x,dnorm(x),type=\"l\",col=\"blue\")\nabline(v=c(2,6),lty=2)\n\nplot(x,dcauchy(x)/dnorm(x),type=\"l\",ylim=c(0,10),ylab=\"\")\nabline(v=c(2,6),lty=2)\n\nset.seed(491)\nx=rnorm(10^5)\nwein=dcauchy(x)/dnorm(x)\n# boxplot(wein/sum(wein))\nplot(log10(1:10^5),cumsum(wein*(x&gt;2)*(x&lt;6))/cumsum(rep(1,10^5)),type=\"l\",ylim=c(0,0.3),xlab=\"log(N)\",ylab=\"\")\nabline(a=pcauchy(6)-pcauchy(2),b=0,col=\"sienna\")\n\n\n\n\n\n\n\nDensities\n\n\n\n\n\n\n\nRatio of densities\n\n\n\n\n\n\n\nImportance sampling error\n\n\n\n\n\n\nCauchy target, Gaussian reference",
    "crumbs": [
      "Supplementary notes",
      "Monte Carlo Methods"
    ]
  },
  {
    "objectID": "notes/week_04.html",
    "href": "notes/week_04.html",
    "title": "Non-parametric Regression",
    "section": "",
    "text": "Suppose we observe i.i.d. copies of a bivariate random vector \\((X,Y)^\\top\\), that is a random sample \\((X_1,Y_1)^\\top, \\ldots, (X_n, Y_n)^\\top\\), and we are interested in modeling the conditional expectation of the response variable \\(Y\\) given the single predictor variable \\(X\\), i.e. \\[ m(x) := \\mathbb{E}\\big[ Y \\big| X = x \\big] \\] In linear regression, we assume a parametric model \\(m(x) = \\beta_0 + \\beta_1 x\\). Here, we would like to make no such structural assumption, instead only assuming that \\(m\\) is sufficiently smooth. How to estimate \\(m = m(x)\\) from the observed data non-parametrically?",
    "crumbs": [
      "Supplementary notes",
      "Non-parametric Regression"
    ]
  },
  {
    "objectID": "notes/week_04.html#bandwidth-selection",
    "href": "notes/week_04.html#bandwidth-selection",
    "title": "Non-parametric Regression",
    "section": "3.1 Bandwidth Selection",
    "text": "3.1 Bandwidth Selection\nSimilarly to what we did last week with KDEs, we consider \\[MSE\\{\\widehat{m}(x)\\} = \\mathrm{var}\\{\\widehat{m}(x)\\} + \\big[\\mathrm{bias}\\{\\widehat{m}(x)\\}\\big]^2\\] and, dropping the little-o terms, we obtain \\[AMSE\\{\\widehat{m}(x)\\} = \\frac{\\sigma^2(x) \\int [K(z)]^2 dz}{f_X(x) n h_n} + \\frac{1}{4} \\big\\{m''(x)\\big\\}^2 h_n^4 \\bigg\\lbrace \\int z^2 K(z) dz\\bigg\\rbrace^2.\\]\nNow, a local bandwidth choice can be obtained by optimizing AMSE. Taking derivatives and setting them to zero, we obtain \\[h_{opt}(x) = n^{-1/5} \\left[ \\frac{\\sigma^2(x) \\int \\lbrace K(z)\\rbrace^2 dz}{\\big\\lbrace m''(x) \\int z^2 K(z) dz\\big\\rbrace^2 f_X(x)} \\right]^{1/5}.\\]\nThis is somewhat more complicated compared to the KDE case, because we have to estimate\n\nthe marginal density \\(f_X(x)\\),\n\nlet’s say that we already know how to do this, e.g. by KDE even though that requires choice of yet another bandwidth\n\nthe local variance function \\(\\sigma^2(x)\\), and\nthe second derivative of the regression functions \\(m''(x)\\).\n\nRule of thumb algorithms exist to actually obtain some \\(h_{opt}(x)\\) in practice starting from some initial bandwidth:\n\nuse a higher order \\(p\\) (e.g., \\(p=3\\)) to estimate \\(m(x)\\) with an inflated value of the bandwidth, and obtain an estimate of \\(m''(x)\\)\nassume that \\(\\sigma^2(x) = \\sigma^2\\) is constant and estimate it from the residuals as \\(\\frac{1}{n-d} \\sum_{i=1}^n \\{Y_i - \\widetilde{m}(X_i)\\}^2\\), where \\(\\widetilde{m}\\) is the estimator from the previous step and \\(d\\) is the degrees of freedom of \\(\\widetilde{m}\\)\nupdate the current bandwidth by the \\(h_{opt}(x)\\) formula above\n\nand iterate these steps a couple of times.\nAgain, like in the case of KDEs, the global bandwidth choice can be obtained by integration:\n\ncalculate \\(AMISE(\\widehat{m}) = \\int AMSE(\\widehat{m}(x)) f_X(x) dx\\), and\nset \\(h_{opt} = \\underset{h&gt;0}{\\mathrm{arg\\,min}} AMISE(\\widehat{m})\\),\n\nbut the situation is not simplified very much.\nNote: Next week we are going to study cross-validation, which is a much simpler, data-driven (i.e., without asymptotic arguments), and popular bandwidth (or more generally any tuning parameter) selection strategy.",
    "crumbs": [
      "Supplementary notes",
      "Non-parametric Regression"
    ]
  },
  {
    "objectID": "notes/week_04.html#why-local-linear-is-the-order-of-choice",
    "href": "notes/week_04.html#why-local-linear-is-the-order-of-choice",
    "title": "Non-parametric Regression",
    "section": "3.2 Why Local Linear is the Order of Choice?",
    "text": "3.2 Why Local Linear is the Order of Choice?\nBias and variance can be calculated similarly also for higher order local polynomial regression estimators. In general:\n\nbias decreases with an increasing order\nvariance increases with increasing order, but only for \\(p=2k+1 \\to p+1\\), i.e., when increasing an odd order to an even one\n\nFor this reason, odd orders are preferred to even ones, and \\(p=1\\) is easy to grasp as it corresponds to locally fitted simple regression line. Contrarily, it is hard to argue why \\(p=3\\) or higher should be used. In terms of degrees of freedom (and in fact also the convergence rates, which we are not showing) increasing the order has a similar effect as increasing the bandwidth. So it is reasonable to assume with a higher \\(p\\), lower \\(h\\) will be optimal, and vice versa. In practice, one simply fixes \\(p=1\\) and only tunes the bandwidth \\(h\\).",
    "crumbs": [
      "Supplementary notes",
      "Non-parametric Regression"
    ]
  },
  {
    "objectID": "notes/week_09.html",
    "href": "notes/week_09.html",
    "title": "Bootstrap",
    "section": "",
    "text": "The primary goal of statistics is to extract information about a population (described by a distribution function \\(F\\)) based on a random sample \\(\\mathcal{X} = \\{ X_1,\\ldots,X_N \\}\\) from this population. More often than not, one is interested in a particular characteristic of the population denoted generally by \\(\\theta = \\theta(F)\\).\nLeading Ex.: The mean \\(\\theta = \\mathbb{E}X_1 = \\int x d F(x)\\). \\(\\Delta\\)\nLet us focus generally on \\(F\\) rather than specifically on \\(\\theta\\). We can estimate \\(F\\)\n\nparametrically, if we are willing to assume that \\(F \\in \\{ F_\\lambda \\mid \\lambda \\in \\Lambda \\subset \\mathbb{R}^p \\}\\) for some integer \\(p\\), we can take \\(\\widehat{F} = F_{\\widehat{\\lambda}}\\) for an \\(\\widehat{\\lambda}\\) estimator of the parameter vector \\(\\lambda\\), or\nnon-parametrically by the empirical distribution function [ N(x) = {n=1}^N _{[X_n x]} ]\n\n\nedf_plot &lt;- function(N){\n  X &lt;- rnorm(N)\n  EDF &lt;- ecdf(X)\n  plot(EDF)\n  x &lt;- seq(-4,4,by=0.01)\n  points(x,pnorm(x),type=\"l\",col=\"red\")\n}\nset.seed(517)\nedf_plot(12)\nedf_plot(50)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeading Ex.: The empirical estimator of the mean is exactly the mean of the empirical distribution \\(\\widehat{\\theta} := \\frac{1}{N} \\sum_{n=1}^N X_n = \\int x d \\widehat{F}_N(x)\\). \\(\\Delta\\)\nIn both cases, the population characteristic of interest \\(\\theta = \\theta(F)\\) can often be estimated by the corresponding sample characteristic of \\(\\widehat{\\theta} = \\theta(\\widehat{F})\\). The sample characteristic is called a statistic, and it has a sampling distribution. Sometimes the statistic of interest can be a scaled version of the sample characteristic.\nLeading Ex.: For \\(F\\) a distribution function of the Gaussian distribution with variance \\(\\sigma^2\\), \\(\\widehat{\\theta} \\sim \\mathcal{N}(\\theta,\\frac{\\sigma^2}{N})\\), which is the sampling distribution. \\(\\Delta\\)\nWe are rarely ever interested only in a point estimator of \\(\\theta\\): most statistical procedures require some knowledge of the sampling distribution. For example, confidence intervals for \\(\\theta\\) or hypothesis testing about \\(\\theta\\) require knowledge of quantiles of the sampling distribution. And even in the case of point estimation, we may be interested in the estimator’s bias, variance, or jointly its mean squared error. Those are all characteristics of the sampling distribution.\nWhile the sampling distribution is generally unknown (it can be found and worked with analytically in simple cases only, like in our leading example, in other cases it has to be approximated by the CLT), it is identified by the underlying \\(F\\) and the sampling process. The bootstrap embraces the idea that a (re)sampling process from \\(\\widehat{F}\\) can mimic the sampling process from \\(F\\) itself that has produced the data. But since \\(\\widehat{F}\\) is known, the characteristics of the resampling distribution (which will in turn serve as proxies for the characteristics of the sampling distribution) are more readily available. Consider the following diagram \\[\n\\begin{eqnarray}\n\\text{Sampling (real world):} \\quad & F &\\Longrightarrow X_1,\\ldots,X_N &\\Longrightarrow \\widehat{\\theta} = \\theta(\\widehat{F}_N) \\\\\n\\text{Resampling (bootstrap world):} \\quad & \\widehat{F}_N &\\Longrightarrow X_1^\\star,\\ldots,X_N^\\star &\\Longrightarrow \\widehat{\\theta}^\\star = \\theta(\\widehat{F}_N^\\star)\n\\end{eqnarray}\n\\] The bootstrap idea is that the bootstrap world can serve as a proxy for the real world. The advantage of the bootstrap world is that it is ours to control. Moreover, approximating the sampling process by resampling (i.e. performing bootstrap) can sometimes have further benefits.\n Leading Ex.: Assume we are looking for a one-sided CI \\([\\theta_{\\alpha},\\infty)\\) with coverage \\(1-\\alpha\\), that is we search for a random variable \\(\\theta_{\\alpha}\\) such that \\(P( \\theta \\geq \\theta_{\\alpha}) = 1-\\alpha\\), for \\(\\alpha \\in (0,1)\\). We know that \\[\nT = \\sqrt{N}\\frac{\\bar{X}_N - \\theta}{\\widehat{\\sigma}} \\sim t_{n-1} \\quad \\Rightarrow \\quad P\\{T \\leq t_{n-1}(1-\\alpha)\\} = 1-\\alpha\n\\] and so we get a CI with exact coverage by expressing \\(\\theta\\) from the inequality \\(T \\leq t_{n-1}(1-\\alpha)\\): \\[\n\\theta \\geq \\bar{X}_N - \\frac{\\widehat{\\sigma}}{\\sqrt{N}} t_{n-1}(1-\\alpha) := \\widehat{\\theta}_{\\alpha}.\n\\] When the Gaussianity assumption is taken away, i.e., when we only assume \\(X_1,\\ldots,X_N\\) are i.i.d. with \\(\\mathbb{E}X_1^2 &lt; \\infty\\), we have from the CLT that \\(T \\stackrel{d}{\\to} \\mathcal{N}(0,1)\\) and thus \\[\nP( \\theta \\geq \\widehat{\\theta}_{\\alpha} ) \\approx 1-\\alpha \\quad \\text{for}\\quad \\widehat{\\theta}_{\\alpha} = \\bar{X}_N - \\frac{\\widehat{\\sigma}}{\\sqrt{N}} z(1-\\alpha),\n\\] where \\(z(1-\\alpha)\\) is the \\((1-\\alpha)\\)-quantile of the standard Gaussian. By the Berry-Esseen theorem: \\[\nP_F( T \\leq x) - \\Phi(x) = \\mathcal{O}\\left({\\frac{1}{\\sqrt{N}}}\\right) \\quad \\text{for all } x \\quad \\Rightarrow \\quad\nP( \\theta \\geq \\widehat{\\theta}_{\\alpha}) = 1-\\alpha + \\mathcal{O}\\left({\\frac{1}{\\sqrt{N}}}\\right).\n\\] That is, the coverage of the asymptotic CI is exact up to the order \\(\\mathcal{O}(N^{-1/2})\\).\nNow consider the bootstrap version of the CI. Let \\(X_1^\\star,\\ldots,X_N^\\star\\) be i.i.d. from the empirical distribution \\(\\widehat{F}_N\\) obtained from \\(X_1,\\ldots,X_N\\) i.i.d. with \\(\\mathbb{E}X_1^2 &lt; \\infty\\). Then, it can be shown using Edgeworth expansions that \\[\nP_F(T \\leq x) = \\Phi(x) + \\frac{1}{\\sqrt{N}} a(x) \\phi(x) + \\mathcal{O}\\left( \\frac{1}{N} \\right)\n\\] for all \\(x \\in \\mathbb{R}\\) and \\(\\Phi\\) and \\(\\phi\\) being respectively the distribution function and the density of the standard Gaussian, while the bootstrap version satisfies \\[\nP_{\\widehat{F}_N}(T^\\star \\leq x) = \\Phi(x) + \\frac{1}{\\sqrt{N}} \\widehat{a}(x) \\phi(x) + \\mathcal{O}\\left( \\frac{1}{N} \\right),\n\\] where \\(\\widehat{a}(x) - a(x) = \\mathcal{O}(N^{-1/2})\\) and \\(T^{\\star} = \\sqrt{N}\\frac{\\overline{X}_N^{\\star} - \\overline{X}_N}{\\widehat{\\sigma}^{\\star}}\\). Hence\n\\[\\begin{equation}\\label{eq:boot_rate}\\tag{1}\nP_{\\widehat{F}_N}(T^\\star \\leq x) - P_F(T \\leq x) = \\mathcal{O}\\left( \\frac{1}{N} \\right) \\quad \\Rightarrow \\quad P\\left(\\theta \\geq \\bar{X}_N - \\frac{\\widehat{\\sigma}}{\\sqrt{N}} q^\\star(1-\\alpha) =: \\widehat{\\theta}^\\star_{\\alpha} \\right) = 1-\\alpha + \\mathcal{O}\\left( \\frac{1}{N} \\right),\n\\end{equation}\\]\ni.e., the (one-sided) bootstrap interval has exact coverage up to the order \\(\\mathcal{O}(N^{-1})\\)!\nThe previous implication probably requires some more justification though. \\(T\\) has a sampling distribution inherited from \\(F\\) and let \\(q(\\alpha)\\) be its \\(\\alpha\\)-quantile. Then naturally \\[\nP\\{T \\leq q(1-\\alpha)\\} = 1-\\alpha \\quad \\Rightarrow \\quad P\\left(\\theta \\geq \\bar{X}_N - \\frac{\\widehat{\\sigma}}{\\sqrt{N}} q(1-\\alpha)\\right) = 1-\\alpha,\n\\] but the problem is we do not know the sampling distribution, and hence neither its quantiles. If we use asymptotic normality, we approximate the quantile as the corresponding Gaussian quantile, but we commit a certain error. The bootstrap alternative instead tries to approximate the unknown sampling distribution more directly by the distribution of \\(T^\\star = \\sqrt{N}\\frac{\\overline{X}_N^\\star - \\overline{X}_N}{\\widehat{\\sigma}^\\star}\\), where \\(\\overline{X}_N^\\star\\) and \\(\\widehat{\\sigma}^\\star\\) are calculated from the bootstrap sample \\(X_1^\\star,\\ldots,X_N^\\star\\). \\(T^\\star\\) also has a sampling distribution (inherited from \\(\\widehat{F}_N\\)) and let \\(q^\\star(\\alpha)\\) be its \\(\\alpha\\)-quantile. Then \\(P(T^\\star_1 \\leq q^\\star(1-\\alpha)) = 1-\\alpha\\), and according to the LHS of \\(\\eqref{eq:boot_rate}\\) we only commit error of order \\(\\mathcal{O}(N^{-1})\\) when we replace \\(T^\\star\\) by \\(T\\), which then yields the RHS of \\(\\eqref{eq:boot_rate}\\). \\(\\Delta\\)\n\nRemark: As discussed by Davison and Hinkley (see reference to their book below), the benefits of the bootstrap distribution of \\(T\\), and hence being an order of magnitude closer to the true distribution, disappears if \\(T\\) is not pivotal, i.e., if its (limiting) distribution does not depend on unknowns. This is justified by the fact that the unknown parameters on which the (limiting) distribution of \\(T\\) would depend, needs to be estimated to get the bootstrap distribution which would have a different leading term (in the Edgeworth expansion) than the true (limiting) distribution.\nExample: Assume \\(X_1,\\ldots,X_N\\) are i.i.d. with \\(\\mathbb{E}|X_1|^3 &lt; \\infty\\) and \\(\\mathbb{E}X_1 = \\mu\\), and let \\(\\theta = \\mu^3\\). In the same spirit to the leading example above, the empirical estimator of \\(\\theta\\) is \\(\\widehat{\\theta} = (\\bar{X}_N)^3\\), which is biased: \\(b := \\text{bias}(\\widehat{\\theta}) = \\mathbb{E}\\widehat{\\theta} - \\theta\\). We can use the bootstrap to estimate the bias \\(b\\) as \\(\\widehat{b}^\\star\\) and define a bias-corrected estimator as \\(\\widehat{\\theta}_b^\\star = \\widehat{\\theta} - \\widehat{b}^\\star\\). Let us do just that.\nIn this case, the bias can actually be calculated explicitly: \\[\n\\mathbb{E}_F \\widehat{\\theta} = \\mathbb{E}_F\\bar{X}_N^3 = \\mathbb{E}_F\\big[ \\mu + N^{-1} \\sum_{n=1}^N(X_n - \\mu) \\big]^3 = \\mu^3 + \\underbrace{N^{-1} 3 \\mu \\sigma^2 + N^{-2} \\gamma}_{=b},\n\\] where \\(\\sigma^2 = \\mathbb{E}(X_1-\\mu)^2\\) and \\(\\gamma = \\mathbb{E}(X_1-\\mu)^3\\). For the bootstrap version, we have \\[\n\\mathbb{E}_{\\widehat{F}_N} \\widehat{\\theta}^\\star = \\mathbb{E}_{\\widehat{F}_N} (\\bar{X}_N^\\star)^3 = \\mathbb{E}_{\\widehat{F}_N} \\big[ \\bar{X}_N + N^{-1} \\sum_{n=1}^N(X_n^\\star - \\bar{X}_N) \\big]^3 = \\bar{X}_N^3 + \\underbrace{N^{-1} 3 \\bar{X}_N \\widehat{\\sigma}^2 + N^{-2} \\widehat{\\gamma}}_{=\\widehat{b}^\\star},\n\\] where \\(\\widehat \\sigma^2 = N^{-1} \\sum_n (X_n - \\bar{X}_N)^2\\) and \\(\\widehat \\gamma = N^{-1} \\sum_n (X_n - \\bar{X}_N)^3\\).\nNow, for \\(\\widehat{\\theta}_b^\\star = \\widehat{\\theta} - \\widehat{b}^\\star = \\bar{X}_N^3 - N^{-1} 3 \\bar{X}_N \\widehat{\\sigma}^2 - N^{-2} \\widehat{\\gamma}\\) we have \\[\n\\mathbb{E}_F \\widehat{\\theta}_b^\\star = \\mu^3 + N^{-1} 3 \\big[ \\mu \\sigma^2 - \\mathbb{E}_F \\bar{X}_N \\widehat{\\sigma}^2 \\big] + N^{-2} \\big[ \\gamma - \\mathbb{E}_F \\widehat{\\gamma} \\big].\n\\]\nAt this point, there are several ways to argue that the bias has been decreased (but all of them are a bit painful). For example, it can be calculated explicitly that \\(\\mathbb{E}\\bar{X}_N \\widehat{\\sigma}^2 = \\mu \\sigma^2 + \\mathcal{O}(N^{-1})\\) and \\(\\mathbb{E}\\widehat{\\gamma} = \\gamma + \\mathcal{O}(N^{-1})\\), and hence \\(\\text{bias}(\\widehat{\\theta}_b^\\star) = \\mathcal{O}(N^{-2})\\) as opposed to \\(\\text{bias}(\\widehat{\\theta}) = \\mathcal{O}(N^{-1})\\). Thus, using bootstrap, we have successfully reduced the bias by the full order of the sample size. \\(\\Delta\\)\nIn the examples above, we saw how resampling can be used to construct confidence intervals with better coverage compared to those obtained using CLT, and how the resampling distribution can be used to partially correct the bias of an estimator. Of course, the second example is special in that it allows for explicit calculations, while explicit calculation of \\(q^\\star(\\alpha)\\) is not possible. Nonetheless, the bootstrap can be naturally paired up with Monte Carlo in such situations.\nWe have already seen how basic Monte Carlo can help us avoid analytic calculations and perform numerous statistical tasks. However, the cases in which the distribution under the null is fully known (like in this example) are quite rare. In many more cases, the distribution under the null is unknown, but can be approximated by the bootstrap idea above, and Monte Carlo can then be used to approximate desirable characteristics of such a distribution. From this point of view, by “bootstrap” it is commonly meant to combine\n\nthe plug-in principle (i.e. estimating the unknowns), and\nMonte Carlo principle (i.e. simulation instead of analytic calculations).\n\nThe former amounts to estimating either\n\nsome parameters in the parametric problems \\(\\Longrightarrow\\) parametric bootstrap, or\nthe whole \\(F\\) via the empirical estimator \\(\\widehat{F}_N\\) in non-parametric problems \\(\\Longrightarrow\\) non-parametric bootstrap.",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_09.html#some-theory",
    "href": "notes/week_09.html#some-theory",
    "title": "Bootstrap",
    "section": "Some Theory",
    "text": "Some Theory\nAny statistical methodology should be at least consistent. If we have consistency, we may ask for rates of convergence, like sketched in the example above. But with bootstrap, theoretical research is hard, results have to derived again for different kinds of statistics. Here we will only provide a consistency result for the case of bootstrapping a smooth transformation of the empirical mean.\nRecall that we are using the empirical bootstrap distribution \\(\\widehat{F}_{T,B}^\\star\\) as a proxy for the unknown distribution of the scaled estimator \\(T\\), denoted by \\(F_{T,N}(x) = P(T \\leq x)\\). As we increase the number of bootstrap samples, \\(\\widehat{F}_{T,B}^\\star\\) naturally approaches \\(F_{T,N}^\\star(x) = P(T_1^\\star \\leq x)\\). In fact, by the Glivenko-Cantelli theorem: \\[\n\\sup_{x} \\Big| \\widehat{F}_{T,B}^\\star(x) - F_{T,N}^\\star(x) \\Big| \\overset{a.s.}{\\to} 0 \\quad \\text{as} \\quad B \\to \\infty.\n\\] Hence the question is how does \\[\n\\sup_{x} \\Big| F_{T,N}^\\star(x) - F_{T,N}(x) \\Big|\n\\] behave as \\(N \\to \\infty\\).\nTheorem. Let \\(\\mathbb{E}X_1^2 &lt; \\infty\\) and \\(T = h(\\bar{X}_N)\\), where \\(h\\) is continuously differentiable at \\(\\mu := \\mathbb{E}(X_1)\\) and such that \\(h(\\mu) \\neq 0\\). Then \\[\n\\sup_{x} \\Big| F_{T,N}^\\star(x) - F_{T,N}(x) \\Big| \\overset{a.s.}{\\to} 0 \\quad \\text{as} \\quad N \\to \\infty.\n\\]\nThe previous theorem shows consistency of the bootstrap or smooth transformations of the empirical average. It also holds for random vectors, where \\(h : \\mathbb{R}^p \\to \\mathbb{R}^q\\) where has continuous partial derivatives on some neighborhood of \\(\\mu\\).",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_09.html#applications",
    "href": "notes/week_09.html#applications",
    "title": "Bootstrap",
    "section": "Applications",
    "text": "Applications\nHere, we will demonstrate how to device bootstrap procedures for standard statistical tasks. We will not prove validity of those procedures (i.e., we will not show that the bootstrap works).\n\nConfidence Intervals\nLet \\(T = \\sqrt{N}(\\widehat{\\theta} - \\theta)\\) where \\(\\theta \\in \\mathbb{R}\\), thus \\(T^\\star_b = \\sqrt{N}(\\widehat{\\theta}_b^\\star - \\widehat{\\theta})\\) for \\(b=1,\\ldots,B\\). We can take \\(q_B^\\star(\\alpha)\\) for \\(\\alpha \\in (0,1)\\) to be the \\(\\alpha\\)-quantile of the empirical distribution based on \\(T^\\star_b\\) for \\(b=1,\\ldots,B\\), i.e., the \\(\\alpha\\)-quantile of the bootstrap distribution of \\(T\\). Provided that the bootstrap works (is consistent), we have \\[\nP\\Big(q_B^\\star(\\alpha/2) \\leq \\sqrt{N}(\\widehat{\\theta} - \\theta) \\leq q_B^\\star(1-\\alpha/2) \\Big) \\to 1-\\alpha\n\\] and hence \\[\n\\left( \\widehat{\\theta} - \\frac{q_B^\\star(1-\\alpha/2)}{\\sqrt{N}}, \\widehat{\\theta} - \\frac{q_B^\\star(\\alpha/2)}{\\sqrt{N}} \\right)\n\\] is a CI for \\(\\theta\\) with asymptotically correct coverage (again, provided that the bootstrap works and hence that the quantiles of the bootstrap distribution converge to those of the true distribution of \\(T\\)).\nNote that the interval above is different from the one we worked with in our leading example above, where we have standardized for the scale. This is called studentization and it is highly recommended. It can often be shown that bootstrap procedures based on studentized statistics have better properties. For example, in the leading example above we have seen that the studentized CI had a faster rate of convergence (to the correct coverage) than the CI based on asymptotic normality. This would not happen without studentization. So let’s change the basic CI above into a studentized CI.\nAssume that \\(\\sqrt{N}(\\widehat{\\theta} - \\theta) \\to \\mathcal{N}(0,v^2)\\), i.e., \\(v^2\\) is the asymptotic variance of \\(T\\). Let \\(\\widehat{v}\\) be a consistent estimator for \\(v\\). Now we consider the studentized statistics: \\[\nT = \\sqrt{N}\\frac{\\widehat{\\theta} - \\theta}{\\widehat{v}} \\qquad \\& \\qquad\nT^\\star_b = \\sqrt{N}\\frac{\\widehat{\\theta}_b^\\star - \\widehat{\\theta}}{\\widehat{v}^\\star_b}\n\\] where \\(\\widehat{v}^\\star_b\\) is the estimator of the same form as \\(\\widehat{v}\\), but calculated from the \\(b\\)-th resample \\(\\mathcal{X}^\\star_b\\). Again, let \\(q_B^\\star(\\alpha)\\) for \\(\\alpha \\in (0,1)\\) be the \\(\\alpha\\)-quantile of the empirical distribution based on \\(T^\\star_b\\) for \\(b=1,\\ldots,B\\). If bootstrap works, we have now \\[\nP\\Big(q_B^\\star(\\alpha/2) \\leq \\sqrt{N}\\frac{\\widehat{\\theta} - \\theta}{\\widehat{v}} \\leq q_B^\\star(1-\\alpha/2) \\Big) \\to 1-\\alpha\n\\] from which we get the CI with asymptotically correct coverage: \\[\n\\left( \\widehat{\\theta} - \\frac{q_B^\\star(1-\\alpha/2)}{\\sqrt{N}}\\widehat{v}, \\widehat{\\theta} - \\frac{q_B^\\star(\\alpha/2)}{\\sqrt{N}} \\widehat{v}\\right)\n\\]\n\n\nVariance Estimation\nWe often know that \\[\n\\sqrt{N}(\\widehat{\\theta} - \\theta) \\stackrel{d}{\\to} \\mathcal{N_p}(0, \\Sigma),\n\\] but in these cases \\(V=N^{-1}\\Sigma\\) often has a complicated form (e.g., it has a complicated dependency on unknown parameters).\nBut the bootstrap can be deployed quite easily: \\[\n\\widehat{V}^\\star = \\frac{1}{B-1} \\sum_{b=1}^B \\left( \\widehat{\\theta}^\\star_b - \\bar{\\theta}^\\star \\right) \\left( \\widehat{\\theta}^\\star_b - \\bar{\\theta}^\\star \\right)^\\top, \\qquad \\text{where} \\qquad\n\\bar{\\theta}^\\star = \\frac{1}{B}\\sum_{b=1}^B \\widehat{\\theta}^\\star_b,\n\\] where \\(\\widehat{\\theta}^\\star_b\\) is the same estimator as \\(\\widehat{\\theta}\\), only calculated from the \\(b\\)-th bootstrap resample.\nNote that \\(\\widehat{V}^\\star\\) is an estimator of \\(N^{-1} \\Sigma\\), not just of \\(\\Sigma\\). Critically, the bootstrap will fail if \\(\\mathrm{var}(\\widehat{\\theta}) = \\infty\\), which can actually happen even when the asymptotic normality holds.\nExample: Let \\(X_1,\\ldots,X_N\\) be a random sample from the density \\(f(x) = 3/x^4 \\mathbb{I}_{[x \\geq 1]}\\). From the CLT we have \\(\\sqrt{N}(\\bar{X}_N - 3/2) \\to \\mathcal{N}(0,3/4)\\). Taking \\(g(x) = \\exp(x^4)\\), the delta method gives us \\[\n\\sqrt{N}\\Big( g\\big( \\bar{X}_n\\big) - g(3/2) \\Big) \\to \\mathcal{N}\\big(0,\\{g'(3/2)\\}^2 3/4 \\big)\n\\] But the bootstrap will not work in this case, since even \\(\\mathbb{E}g\\big( \\bar{X}_n \\big) = \\infty\\), see the code below.\n\nx &lt;- 1+log(1+(0:1000)/10)\ntarget_d &lt;- function(x){\n  return(3/x^4)\n}\nplot(x,target_d(x),type=\"l\")\nx &lt;- (1:6000)/1001\nproposal_d &lt;- dcauchy(x)\npoints(1+x,2*proposal_d,type=\"l\",col=\"red\")\n\n\n\n\n\n\n\nconst &lt;- 3/1^4 / (2*dcauchy(0)) # 4.7\n\n# rejection sampling from the target\nset.seed(317)\nN &lt;- 1000\nX &lt;- rep(0,N)\nj &lt;- 0\nwhile(j &lt; N){\n  Y &lt;- 1+abs(rcauchy(1))\n  U &lt;- runif(1)\n  if(U &lt; target_d(Y)/dcauchy(Y)/const){\n    X[j+1] &lt;- Y\n    j &lt;- j+1\n  }\n}\nhist(X,breaks=16,freq=F)\npoints(x,target_d(x),type=\"l\")\n\n\n\n\n\n\n\n# bootstrap estimator of the variance\nB &lt;- 1000\nboot_stat &lt;- rep(0,B)\nfor(b in 1:B){\n  Xb &lt;- sample(X,N,replace=T)\n  boot_stat[b] &lt;- exp(mean(Xb)^4)\n}\n(sigma_star &lt;- 1/(B-1)*sum((boot_stat-mean(boot_stat))^2)) # overshot\n\n[1] 5482212\n\n(4*exp(1.5^4)*1.5^3)^2*3/4/N # true asymptotic variance we wish to estimate\n\n[1] 3411.618\n\n\n\n\nBias Reduction\nUnbiased estimators are in reality quite rare. But, we can estimate the bias by bootstrapping as \\(\\widehat{b}^\\star = \\bar{\\theta}^\\star - \\widehat{\\theta}\\) and define the bias-corrected estimator \\(\\widehat{\\theta}_b = \\widehat{\\theta} - \\widehat{b}^\\star\\).\nWe have seen in an example above that the bias-corrected estimator can have better properties. In that example, we tried to estimate \\(g(\\mathbb{E}X_1)\\) for \\(g(x) = x^3\\) by \\(g(\\bar{X}_N)\\), and we got a faster rate for the bias of the bias-corrected estimator. Something similar happens when working with any smooth enough \\(g\\) instead.\n\n\nHypothesis Testing\nAssume we wish to test a hypothesis \\(H_0\\) using a statistic \\(T\\). Depending on the form of the alternative hypothesis \\(H_1\\), either\n\nlarge values of \\(T\\),\nsmall values of \\(T\\), or\nboth large and small values of \\(T\\)\n\ntestify against the null hypothesis and in favor of the alternative. Bootstrapping the statistic, we obtain \\(T_1^\\star, \\ldots, T_B^\\star\\) and depending on what values of the statistics testify against the hypothesis, we can estimate the p-value as either\n\n\\(\\widehat{\\text{p-val}} = \\frac{1}{B} \\left( \\sum_{b=1}^B \\mathbb{I}_{[T_b^\\star \\geq t_{\\text{obs}}]} \\right)\\),\n\\(\\widehat{\\text{p-val}} = \\frac{1}{B} \\left( \\sum_{b=1}^B \\mathbb{I}_{[T_b^\\star \\leq t_{\\text{obs}}]} \\right)\\), or\n\\(\\widehat{\\text{p-val}} = \\frac{1}{B} \\left( \\sum_{b=1}^B \\mathbb{I}_{[|T_b^\\star| \\geq |t_{\\text{obs}}|]} \\right)\\),\n\nIf the observed value \\(t_{\\text{obs}}\\) of the statistic \\(T\\) is not too extreme (the form of \\(H_1\\) decides what “extreme” is), we will observe more extreme values coming out of our bootstrap, which in turn increases our estimate of the p-value, which in turn prevents us from rejecting \\(H_0\\).\nExample: Let \\(X_1,\\ldots,X_N\\) be a random sample with \\(\\mathbb{E}(X_1^2) &lt; \\infty\\) and consider \\(H_0: \\mu = \\mu_0\\) against\n\n\\(H_1: \\mu &gt; \\mu_0\\). Taking the studentized statistic \\(T = \\sqrt{N}(\\bar{X}_N - \\mu_0)/s\\) for \\(s = (N-1)^{-1} \\sum_n (X_n - \\bar{X}_N)^2\\) and the corresponding bootstrap statistics \\(T_b^\\star = \\sqrt{N}(\\bar{X}_b^\\star - \\bar{X}_N)/s^\\star\\), the bootstrap p-value is \\[\n\\widehat{\\text{p-val}} = \\frac{1}{B} \\left(\\sum_{b=1}^B \\mathbb{I}_{[T_b^\\star \\geq t_{\\text{obs}}]} \\right).\n\\]\n\n\nset.seed(517)\nN &lt;- 100\nX &lt;- rexp(N,1/2) # thus true mean = 2\nmu_0 &lt;- 1.78 # hypothesized value\nT_stat &lt;- (mean(X)-mu_0)/sd(X)*sqrt(N) #asympt. normal under H0\nB &lt;- 10000\nboot_stat &lt;- boot_stat_param &lt;- rep(0,B)\nfor(b in 1:B){\n  Xb           &lt;- sample(X,N,replace=T)\n  boot_stat[b] &lt;- (mean(Xb)-mean(X))/sd(Xb)*sqrt(N)\n  Xb_param           &lt;- rexp(N, rate=1/mu_0)\n  boot_stat_param[b] &lt;- (mean(Xb_param)-mu_0)/sd(Xb_param)*sqrt(N)\n}\np_val       &lt;- mean(boot_stat &gt;= T_stat)\np_val_param &lt;- mean(boot_stat_param &gt;= T_stat)\nc(p_val,p_val_param)\n\n[1] 0.0379 0.0394\n\nI(abs(T_stat) &gt; qnorm(0.95)) # asymptotic test, easy to calculate asymptotic p-val\n\n[1] FALSE\n\n\nNotice that while (parametric and non-parametric) bootstrap rejects the hypothesis on 5 % level, the test based on asymptotic normality does not. This could be expected, since we saw in our leading example above that bootstrap produces better one-sided CIs, which are naturally dual to one-sided hypothesis tests. \n\n\\(H_1: \\mu \\neq \\mu_0\\). Taking the same studentized statistics, the bootstrap p-value is this time \\[\n\\widehat{\\text{p-val}} = \\frac{1}{B} \\left(\\sum_{b=1}^B \\mathbb{I}_{[|T_b^\\star| \\geq |t_{\\text{obs}}|]} \\right).\n\\]\n\n\nset.seed(517)\nN &lt;- 100\nX &lt;- rexp(N,1/2) # thus true mean = 2\nmu_0 &lt;- 1.7 # reduced, since harder to reject here\nT_stat &lt;- (mean(X)-mu_0)/sd(X)*sqrt(N)\nB &lt;- 10000\nboot_stat &lt;- rep(0,B)\nfor(b in 1:B){\n  Xb &lt;- sample(X,N,replace=T)\n  boot_stat[b] &lt;- (mean(Xb)-mean(X))/sd(Xb)*sqrt(N)\n}\np_val       &lt;- mean(boot_stat &gt;= T_stat)\np_val\n\n[1] 0.0139\n\n2*(1-pnorm(T_stat)) # asymptotic test, easy to calculate asymptotic p-val\n\n[1] 0.05592678",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_09.html#parametric-bootstrap-and-goodness-of-fit-testing",
    "href": "notes/week_09.html#parametric-bootstrap-and-goodness-of-fit-testing",
    "title": "Bootstrap",
    "section": "Parametric Bootstrap and Goodness-of-Fit Testing",
    "text": "Parametric Bootstrap and Goodness-of-Fit Testing\nAssume that \\(X_1,\\ldots,X_N\\) is a random sample from \\(F = F_\\gamma\\), i.e., the distribution function is known up to a parameter \\(\\lambda \\in \\mathbb{R}^p\\). The parametric bootstrap is similar to the non-parametric one, only instead of sampling with replacement from \\(X_1,\\ldots,X_N\\) we sample from the fitted distribution \\(F_\\widehat{\\lambda}\\), where \\(\\widehat{\\lambda}\\) is a consistent estimator of \\(\\lambda\\).\nParametric bootstrap is mainly useful for Goodness-of-Fit (GoF) testing, that is the following problem: assume \\(X_1,\\ldots,X_N\\) is a random sample from a general \\(F\\), and for a parametric model \\(\\mathcal{F} = \\{ F_\\lambda \\mid \\lambda \\in \\Lambda \\}\\), we would like to test whether the data supports the hypothesis that \\(F\\) belongs to this parametric family: \\[\nH_0: F \\in \\mathcal{F}, \\qquad H_1: F \\notin \\mathcal{F}.\n\\]\nThe standard approach is to use the Kolmogorov-Smirnov statistic \\[\nT = \\sup_x \\Big| \\widehat{F}_N(x) - F_\\widehat{\\lambda}(x) \\Big|,\n\\] where \\(\\widehat{\\lambda}\\) is a specific estimator consistent under the parametric model \\(H_0\\) (e.g., the MLE).\nBut here we are not comparing the empirical distribution to a fixed reference like in the usual Kolmogorov-Smirnov test, instead we compare the non-parametric fit (\\(\\widehat{F}_N\\)) to the parametric fit (\\(F_\\widehat{\\lambda}\\)). Hence the distribution of \\(T\\) is complicated, but doing parametric bootstrap is easy:\n\nfor \\(b=1,\\ldots,B\\)\n\ngenerate resample \\(\\mathcal{X}_b^\\star = \\{ X_{b,1}^\\star,\\ldots,X_{b,N}^\\star \\}\\)\nestimate \\(\\widehat{\\lambda}_b^\\star\\) from the resample \\(\\mathcal{X}_b^\\star\\)\ncalculate the EDF \\(\\widehat{F}_{N,b}^\\star\\) from the resample \\(\\mathcal{X}_b^\\star\\)\nset \\(t_b^\\star = \\sup_x \\Big| \\widehat{F}_{N,b}^\\star(x) - F_{\\widehat{\\lambda}_b^\\star}(x) \\Big|\\)\n\nestimate the p-value of the test by \\[\n\\widehat{\\text{p-val}} = \\frac{1}{B}\\left( \\sum_{b=1}^B \\mathbb{I}_{[T_b^\\star \\geq t_{\\text{obs}}]} \\right)\n\\]\n\nNote: When using bootstrap for hypothesis testing, it has been suggested to use the following bootstrap estimate of the \\(p\\)-value\n\\[\\widehat{\\text{p-val}} = \\frac{1}{B+1}\\left( 1+\\sum_{b=1}^B \\mathbb{I}_{[T_b^\\star \\geq t_{\\text{obs}}]} \\right)\\]\nto improve finite sample performance.",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_09.html#iterated-bootstrap",
    "href": "notes/week_09.html#iterated-bootstrap",
    "title": "Bootstrap",
    "section": "Iterated Bootstrap",
    "text": "Iterated Bootstrap\nAs we have seen in an example above, the bootstrap can be used to estimate the bias \\(b\\) of an estimator by \\(\\widehat{b}^\\star\\), and the estimator of the bias can be in turn used to correct the estimator. But in that example, the bias-corrected estimator \\(\\widehat{\\theta}_b^\\star\\) is still biased (just the asymptotic order of the bias has been lowered), which means that the estimator of the bias \\(\\widehat{b}^\\star\\) is itself biased. How about we iterate the bootstrap idea, and use another bootstrap to estimate the bias of \\(\\widehat{b}^\\star\\)? Then we can adjust the estimator \\(\\widehat{\\theta}_b^\\star\\) even further and hopefully reduce its bias even further (i.e., lower the asymptotic order of the bias even more).\nIn the following scheme, the double star denotes the double bootstrap estimate \\[\n\\begin{eqnarray}\n\\text{Estimator:} \\quad&\\quad \\widehat{\\theta} \\quad&\\quad \\widehat{b}^\\star \\quad&\\quad \\widehat{\\theta}^\\star_b \\quad&\\quad \\widehat{b}^{\\star \\star} \\quad&\\quad \\widehat{\\theta}_b^{\\star \\star} \\\\\n\\text{Bias:} \\quad&\\quad \\mathcal{O}(N^{-1}) \\quad&\\quad \\mathcal{O}(N^{-1}) \\quad&\\quad \\mathcal{O}(N^{-2}) \\quad&\\quad \\mathcal{O}(N^{-2}) \\quad&\\quad \\mathcal{O}(N^{-3})\n\\end{eqnarray}\n\\]\nExample: In this example, consider the estimator \\(\\widehat{\\tau} = N^{-1} \\sum_{n} (X_n - \\bar{X}_N)^2\\) of the variance \\(\\tau = \\sigma^2 = \\mathrm{var}{(X_1)}\\). We know this estimator is biased, and we know that replacing \\(N^{-1}\\) by \\((N-1)^{-1}\\) would make it unbiased, but let us show how the bootstrap can be used without the analytic knowledge (and use the analytic knowledge only to evaluate what the bootstrap does). The bias is \\(b=N^{-1} \\tau\\), and the bootstrap estimate of the bias is simply \\(\\widehat{b}^\\star = N^{-1} \\widehat{\\tau}\\). Next, since \\(\\mathrm{bias}(\\widehat{\\tau}) = \\tau (N-1)/N\\), we have \\[\n\\mathbb{E}(\\widehat{b}^\\star) = \\frac{(N-1) \\tau}{N^2} = b[1 + N^{-1}] \\quad \\Rightarrow \\quad \\text{bias}(\\widehat{b}^\\star) = N^{-1} b = N^{-2} \\tau\n\\] so \\(\\widehat{b}^\\star\\) is unbiased up to order \\(N^{-1}\\) for \\(b\\). Iterating bootstrap for the second time, we have \\(N^{-2} \\widehat{\\tau}\\) as the bootstrap estimator for the bias of \\(\\widehat{b}^{\\star}\\), and the second-iteration estimator for \\(b\\), i.e., for the bias of \\(\\widehat{\\tau}\\) itself, is thus \\(\\widehat{b}^{\\star\\star} = \\widehat{b}^{\\star} + N^{-2} \\widehat{\\tau}\\). This is unbiased up to the order \\(N^{-2}\\) for \\(b\\): \\[\n\\mathbb{E}(\\widehat{b}^{\\star\\star}) = \\frac{(N-1) \\tau}{N^2} + \\frac{(N-1) \\tau}{N^3} = \\frac{\\tau}{N}\\left[ 1 - \\frac{1}{N} + \\frac{1}{N} - \\frac{1}{N^2} \\right] = b [1 + N^{-2}]\n\\] \\(\\Delta\\)\nThe example above is again a toy example where bootstrap calculations can be performed analytically. But similar improvements can be observed even when analytic calculations are not possible. In that case, we need to apply a second-iteration bootstrap to every first-iteration bootstrap sample, i.e., generating \\(\\mathcal{X}_{b,m}^{\\star\\star}\\) for \\(m=1,\\ldots,M\\), for every \\(\\mathcal{X}_{b}^{\\star}\\) where \\(b=1,\\ldots,B\\). This is of course computationally demanding. Thus while we could in principle, iterate this idea to a third-iteration bootstrap an beyond, this would not be computationally feasible, and also the finite sample properties of the iterated bootstrap estimates may not show the improvements unless \\(N\\) is very large. On the other hand, the iterated bootstrap can be useful in many cases from the methodological viewpoint.\n Example: Let \\(X_1,\\ldots,X_p \\in \\mathbb{R}^p\\) be i.i.d. from a distribution depending on \\(\\theta \\in \\mathbb{R}^p\\). We are interested in testing \\(H_0: \\theta = \\theta_0\\) against \\(H_1: \\theta \\neq \\theta_0\\). Let \\(\\widehat{\\theta}\\) be an estimator such that \\(\\sqrt{N}(\\widehat{\\theta} - \\theta) \\stackrel{d}{\\to} \\mathcal{N}(0, \\Sigma)\\). Let us perform a bootstrap test based on the studentized statistic \\[\nT = \\sqrt{N} \\widehat{\\Sigma}^{-1/2}(\\widehat{\\theta} - \\theta_0) \\stackrel{d}{\\to} \\mathcal{N}(0, I_{p\\times p}) \\qquad (\\text{under } H_0),\n\\] where \\(\\widehat{\\Sigma}\\) is a consistent estimator of \\(\\Sigma\\). The asymptotic test can be based on the fact that \\(\\| T \\|^2 \\stackrel{d}{\\to} \\chi^2_p\\) under \\(H_0\\). We have several options how bootstrap can come into play:\n\nIf we do not want to use the asymptotic distribution directly, we can use the standard bootstrap to perform the hypothesis test.\nIf the estimator for \\(\\Sigma\\) is not available, which is often the case, one can use the standard bootstrap to estimate \\(\\Sigma\\) and carry on with the asymptotic test (based on the \\(\\chi^2_p\\) distribution).\nIf we do not want to use the asymptotic distribution directly and the estimator for \\(\\Sigma\\) is not available, we can use double bootstrap to perform the test, as shown schematically below.\n\n\\[\n\\begin{eqnarray}\n\\mathcal{X} = \\{ X_1,\\ldots,X_N \\} &\n\\left\\{\\begin{array}\nx\\mathcal{X}_1^\\star = \\{ X_{1,1}^\\star,\\ldots,X_{1,N}^\\star \\}\n  &\\left\\{\\begin{array}\n  x\\mathcal{X}_{1,1}^{\\star\\star} = \\{ X_{1,1,1}^{\\star\\star},\\ldots,X_{1,1,N}^{\\star\\star} \\} \\\\\n  \\quad\\qquad\\vdots \\\\\n  \\mathcal{X}_{1,M}^{\\star\\star} = \\{ X_{1,M,1}^{\\star\\star},\\ldots,X_{1,M,N}^{\\star\\star} \\}\n  \\end{array}\\right\\}\n  &\\widehat{\\Sigma}_1^{\\star\\star} \\quad \\Longrightarrow \\quad T_1^\\star \\\\\n&\\qquad\\qquad\\vdots \\\\\n\\quad\\qquad\\vdots&\\qquad\\qquad\\vdots \\\\\n&\\qquad\\qquad\\vdots \\\\\n\\mathcal{X}_B^\\star = \\{ X_{B,1}^\\star,\\ldots,X_{B,N}^\\star \\}\n  &\\left\\{\\begin{array}\n  x\\mathcal{X}_{B,1}^{\\star\\star} = \\{ X_{B,1,1}^{\\star\\star},\\ldots,X_{B,1,N}^{\\star\\star} \\} \\\\\n  \\quad\\qquad\\vdots \\\\\n  \\mathcal{X}_{B,M}^{\\star\\star} = \\{ X_{B,M,1}^{\\star\\star},\\ldots,X_{B,M,N}^{\\star\\star} \\}\n  \\end{array}\\right\\}\n  &\\widehat{\\Sigma}_B^{\\star\\star} \\quad \\Longrightarrow \\quad T_B^\\star \\\\\n\\end{array}\\right\\} \\widehat{p}\n\\end{eqnarray}\n\\] where \\[\n\\begin{align}\n\\widehat{\\Sigma}_b^{\\star\\star} &= \\frac{1}{M-1} \\sum_{m=1}^M \\left( \\widehat{\\theta}^{\\star\\star}_{b,m} - \\bar{\\theta}^{\\star\\star}_b \\right) \\left( \\widehat{\\theta}^{\\star\\star}_{b,m} - \\bar{\\theta}^{\\star\\star}_b \\right)^\\top, \\quad \\text{where} \\quad \\widehat{\\theta}^{\\star\\star}_m = \\theta\\big[\\mathcal{X}_{b,m}^{\\star\\star}\\big] \\quad \\&\\quad\n\\bar{\\theta}^{\\star\\star}_b = \\frac{1}{B}\\sum_{b=1}^B \\widehat{\\theta}^{\\star\\star}_{b,m} \\,,\\\\\nT_b^\\star &= \\sqrt{N} \\left( \\widehat{\\Sigma}_b^{\\star\\star} \\right)^{-1/2} \\left( \\widehat{\\theta}_b^\\star - \\widehat{\\theta}\\right), \\\\\n\\widehat{p} &= \\frac{1}{B} \\left( \\sum_{b=1}^B I\\big(\\| T^\\star_b \\|^2 \\geq \\| t_{\\text{obs}} \\|^2\\big) \\right), \\\\\n\\end{align}\n\\] \\(\\Delta\\)",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_09.html#jackknife",
    "href": "notes/week_09.html#jackknife",
    "title": "Bootstrap",
    "section": "Jackknife",
    "text": "Jackknife\nThe jackknife is a predecessor of the bootstrap that also shares some similarities with the bootstrap as well with cross-validation. It was originally proposed for bias estimation, and later (but still before bootstrap) for variance estimation. The idea is to recalculate an estimator many times, always leaving out one observation at a time from the data set.\nLet \\(X_1,\\ldots,X_N\\) be a random sample from a distribution depending on \\(\\theta \\in \\mathbb{R}^p\\). Let \\(\\widehat{\\theta} = \\theta[X_1,\\ldots,X_N]\\) be an estimator of \\(\\theta\\) and let \\(\\widehat{\\theta}_{-n} = \\theta[X_1,\\ldots,X_{n-1},X_{n+1},\\ldots,X_N]\\) be the estimator calculated without the \\(j\\)-th observation. Consider \\(\\bar{\\theta} = N^{-1} \\sum_n \\widehat{\\theta}_{-n}\\).\nThe jackknife estimator of the bias is \\[\n\\widehat{b} = (N-1)(\\bar{\\theta} - \\widehat{\\theta}),\n\\] which has a similar form to the bootstrap estimator of the bias, but the \\((N-1)\\) factor is surprising at first glance. A heuristic justification for the factor is as follows. Assuming for simplicity that \\(p=1\\) and that \\(b = \\text{bias}(\\widehat{\\theta}) = a N^{-1} + b N^{-2} + \\mathcal{O}(N^{-3})\\) for some constants \\(a\\) and \\(b\\), we have \\[\n\\text{bias}(\\widehat{\\theta}_{-n}) = a (N-1)^{-1} + b (N-1)^{-2} + \\mathcal{O}(N^{-3}) = \\text{bias}(\\bar{\\theta}).\n\\] Hence \\[\n\\begin{split}\n\\mathbb{E}(\\widehat{b}) &= (N-1)\\big\\lbrace\\text{bias}(\\bar{\\theta}) - \\text{bias}(\\widehat{\\theta}_{})\\big\\rbrace = (n-1)\\left\\lbrace a \\left( \\frac{1}{N-1} - \\frac{1}{N} \\right) + \\left( \\frac{1}{(N-1)^2} - \\frac{1}{N^2} \\right) + \\mathcal{O}\\left(\\frac{1}{N^3}\\right)\\right\\rbrace \\\\&= a N^{-1} + b N^{-2} \\frac{2N-1}{N-1} + \\mathcal{O}(N^{-3})\n\\end{split},\n\\] so \\(\\widehat{b}\\) approximates \\(b\\) correctly up to the order \\(N^{-2}\\), which corresponds to the bootstrap. The bias-corrected estimator is then \\[\n\\widehat{\\theta}_b^\\star = \\widehat{\\theta} - \\widehat{b} = N \\widehat{\\theta} - (N-1) \\bar{\\theta}.\n\\]\nTukey defined the pseudo-values \\[\n\\theta_{n}^\\star = N \\widehat{\\theta} - (N-1) \\widehat{\\theta}_{-n}\n\\] and conjectured that in some situations these can be treated as i.i.d. with approximately the same variance as \\(N \\mathrm{var}(\\widehat{\\theta})\\), and hence we can take \\[\n\\widehat{\\mathrm{var}}(\\widehat{\\theta}) = \\frac{1}{N}\\frac{1}{N-1} \\sum_{n=1}^N\\left( \\theta_{n}^\\star - \\bar{\\theta}^\\star \\right) \\left( \\theta_{n}^\\star - \\bar{\\theta}^\\star \\right)^\\top.\n\\]\nSimilarly to the bootstrap, the jackknife has its theoretical version which allows us to study when it works. Today, the jackknife is considered as outdated, replaced by bootstrap, but it can still has its place. In the example above, where double bootstrap is used to perform a studentized hypothesis test when variance estimator is not readily available, the second iteration of the bootstrap can be naturally replaced by jackknife instead. Depending on the sample size \\(N\\), this might lead to a better trade-off between accuracy and computation costs (note that the jackknife is generally cheaper than the bootstrap).",
    "crumbs": [
      "Supplementary notes",
      "Bootstrap"
    ]
  },
  {
    "objectID": "notes/week_02.html",
    "href": "notes/week_02.html",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "We will use two data sets for illustration.\n\n\nInsurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell auto insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals being refused homeowners insurance was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their past losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n\n\n\n\n\n\nThe following data set contains about 1/4 million flights that departed from New York City in 2014.\n\nlibrary(data.table)\nflights &lt;- fread(\"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\")\n# 1/4 million rows, so subsample\n\nThe variables are self-explanatory in this case\nSince the data has so many rows, let’s sub-sample it.\n\n# flights &lt;- flights[sample(1:dim(flights)[1], 5000),] # base R version of sub-sampling\nlibrary(tidyverse)\nflights &lt;- flights %&gt;%\n  slice_sample(n=5000)                               # tidyverse version of sub-sampling\nhead(flights)\n\n   year month day dep_delay arr_delay carrier origin dest air_time distance\n1: 2014     7  26        -6       -11      MQ    LGA  CMH       71      479\n2: 2014     8   3        24        17      UA    EWR  ORD      100      719\n3: 2014     1   7        66        54      DL    JFK  LAS      318     2248\n4: 2014    10  23        -2       -26      AA    LGA  ORD      106      733\n5: 2014     1  19         9       -10      B6    EWR  FLL      141     1065\n6: 2014    10  20        49        41      MQ    LGA  ORF       55      296\n   hour\n1:    8\n2:   10\n3:   21\n4:   17\n5:   13\n6:   16",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#chicago-redlining",
    "href": "notes/week_02.html#chicago-redlining",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "Insurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell auto insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals being refused homeowners insurance was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their past losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#flights",
    "href": "notes/week_02.html#flights",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "The following data set contains about 1/4 million flights that departed from New York City in 2014.\n\nlibrary(data.table)\nflights &lt;- fread(\"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\")\n# 1/4 million rows, so subsample\n\nThe variables are self-explanatory in this case\nSince the data has so many rows, let’s sub-sample it.\n\n# flights &lt;- flights[sample(1:dim(flights)[1], 5000),] # base R version of sub-sampling\nlibrary(tidyverse)\nflights &lt;- flights %&gt;%\n  slice_sample(n=5000)                               # tidyverse version of sub-sampling\nhead(flights)\n\n   year month day dep_delay arr_delay carrier origin dest air_time distance\n1: 2014     7  26        -6       -11      MQ    LGA  CMH       71      479\n2: 2014     8   3        24        17      UA    EWR  ORD      100      719\n3: 2014     1   7        66        54      DL    JFK  LAS      318     2248\n4: 2014    10  23        -2       -26      AA    LGA  ORD      106      733\n5: 2014     1  19         9       -10      B6    EWR  FLL      141     1065\n6: 2014    10  20        49        41      MQ    LGA  ORF       55      296\n   hour\n1:    8\n2:   10\n3:   21\n4:   17\n5:   13\n6:   16",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#histograms",
    "href": "notes/week_02.html#histograms",
    "title": "Exploring Data with tidyverse",
    "section": "3.1 Histograms",
    "text": "3.1 Histograms\nBasic bar plot showing number of flights for different carriers:\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier))\n\n\n\n\n\n\n\n\nWe can split every bar by departure airport (i.e., variable origin) by adding the fill argument.\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier, fill = origin))\n\n\n\n\n\n\n\n\nIf we wanted one histogram for each NY airport instead, we could use facet_wrap:\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier)) +\n  facet_wrap(~ origin)\n\n\n\n\n\n\n\n\nWhat if we wanted to add numbers denoting the bin sizes?\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier)) +\n  stat_count(mapping = aes(x = carrier, label=..count.., y=..count.. + 40), geom=\"text\", size=4, color=\"blue\")\n\n\n\n\n\n\n\n\nTo show labels for split bars above, one just adds group=origin in the stat_count’s aes, but it is impossible to vertically align the text properly. We bypass it by creating a new variable veralign, which controls the vertical alignment.\n\nn_flights &lt;- flights %&gt;%\n  group_by(origin, carrier) %&gt;%\n  summarise(count = n()) %&gt;% # now we have a table with the hist cell counts\n  ungroup() %&gt;%\n  group_by(carrier) %&gt;%\n  arrange(desc(origin)) %&gt;% # needed because histogram ordered alphabetically from the top\n  mutate(veralign = cumsum(count) - count/2) %&gt;% # here we calculate the vertical alignment and append it to the data set\n  ungroup() # drop the grouping\n\nggplot() + \n  geom_bar(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_text(data=n_flights, mapping = aes(x = carrier, label=count, y=veralign), size=4)\n\n\n\n\n\n\n\n\nAs long as we are plotting frequencies of occurrence, there is little difference between bar plots and histograms. If the variable on the x-axis is categorical, we speak of bar plots, while if the variable is numerical and some binning has to be done, we speak of histograms. For example:\n\nlibrary(viridis)\nflights %&gt;%\n  ggplot( aes(x=arr_delay_log, fill=origin)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6) +\n    scale_fill_viridis(discrete = TRUE) +\n    labs(fill=\"\")\n\n\n\n\n\n\n\n\nBut we may also scale the y-axis to show probabilities (total area equal to one), instead of frequencies (total area equal to the number of observations). In such a case, histogram can be considered as an estimator of density function (more on that later in the semester). Hence histograms are often overlaid with other density estimators. We will see that later.",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#scatterplots",
    "href": "notes/week_02.html#scatterplots",
    "title": "Exploring Data with tidyverse",
    "section": "3.2 Scatterplots",
    "text": "3.2 Scatterplots\nHere we take a look again at the insurance redlining data, where we are interested in the effect of race on the response variable. Let’s start with the marginal relationship of the response on race, which is what we are mostly interested in. We can distinguish\n\nggplot(data = chredlin,\n       mapping = aes(x = race, y = involact, color = side, shape = side)) +\n  geom_point() + scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\n\n\n\n\n\nggplot(data = chredlin, mapping = aes(x = race, y = involact,\n                                      shape = side, color = side)) +\n  geom_point() + # adds a layer to the empty plot above\n  stat_smooth(method=lm) +# adds a regression line too \n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\n\n\n\n\nThere is probably no difference between north and south w.r.t. the relationship between the response and race, but it might be a different story for fire\n\nggplot(data = chredlin, mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  geom_point() + # adds a layer to the empty plot above\n  stat_smooth(method=lm)+ # adds a regression line too\n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\n\n\n\n\nWhat if I want only a single regression line but ability to distinguish between points at the same time? I could either use different mappings for every layer\n\nggplot(data = chredlin) +\n  geom_point(mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  stat_smooth(method=lm, mapping=aes(x = fire, y = involact), color = \"purple\")+\n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\n\n\n\n\nor, equivalently, use one global mapping in ggplot (the most general one) and then specify it a bit more for some layers\n\nggplot(data = chredlin, mapping=aes(x = fire, y = involact)) +\n  geom_point(mapping = aes(shape = side, color = side)) +\n  stat_smooth(method=lm, color = \"purple\")\n\n\n\n\n\n\n\n\nIf we want to split into multiple plots, use the facet_wrap function\n\nggplot(data = chredlin, mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  geom_point() +\n  stat_smooth(method=lm, color = \"purple\") +\n  facet_wrap(~ side) # split by a value of a factor\n\n\n\n\n\n\n\n\nFor faceting by two factor variables, we can use facet_grid instead, such as if we wanted to create scatterplots for our flights data, split by carrier and origin (we filter only some carriers so we don’t have too many plots).\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"UA\", \"DL\", \"US\")) %&gt;%\n  ggplot(mapping = aes(x = dep_delay, y = arr_delay)) +\n  geom_point() +\n  stat_smooth(method=lm) +\n  facet_grid(origin ~ carrier)\n\n\n\n\n\n\n\n\nExercise: create histograms of dep_delay for all three origins and seven consecutive days in the flights data set.\nBack to the chredlin data set, we take a closer look at some observations in the scatterplot of involact and fire, that might be outliers or leverage points:\n\noutliers &lt;- c(3,6,35)\nleverage_pts &lt;- c(7,24)\noutliers &lt;- chredlin[outliers,]\nleverage_pts &lt;- chredlin[leverage_pts,]\n\nLet’s now make some scatterplots with the outliers and leverage points highlighted by overploting.\n\nggplot() +\n  geom_point(data = chredlin, mapping = aes(x = fire, y = involact)) +\n  geom_point(data = outliers, mapping = aes(x = fire, y = involact), col = \"blue\", pch = 17, size=3) +\n  geom_point(data = leverage_pts, mapping = aes(x = fire, y = involact), col = \"red\", pch = 18, size=3)\n\n\n\n\n\n\n\n\nAlternatively, we could incorporate the information about which points are outliers and which are leverage points into the data set as a new variable and use it for plotting. The code is quite ugly (notice how we start with booleans, then turn them into numericals, and then re-type them into factors), but it does the job.\n\noutliers &lt;- I(1:nrow(chredlin) %in% c(3,6,35))\nleverage_pts &lt;- I(1:nrow(chredlin) %in% c(7,24))\nchredlin &lt;- chredlin %&gt;%\n  mutate(out_lev =  as.factor(outliers + 2*leverage_pts))\nlevels(chredlin$out_lev) &lt;- c(\"normal obs\", \"outlier\", \"leverage point\")\n\nggplot(data = chredlin) +\n  geom_point(mapping = aes(x = fire, y = involact, shape = out_lev, color = out_lev, size = out_lev)) +\n  scale_size_manual(values = c(2,4,4))\n\n\n\n\n\n\n\n\nAnother alternative would be to use fortify() to add residuals and Cook’s distances from a linear model fit directly to the data set. Then we could use mutate() more naturally to decide which observations are outliers and leverage points.",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#boxplots",
    "href": "notes/week_02.html#boxplots",
    "title": "Exploring Data with tidyverse",
    "section": "3.3 Boxplots",
    "text": "3.3 Boxplots\nSimilarly to histograms, boxplots can give us some idea about distributions. Unlike histograms, boxplots do not really capture an underlying density shape, but only visually give several summary statistics and points in the tails.\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"B6\", \"DL\", \"EV\")) %&gt;%\n  ggplot( aes(x=carrier, y= arr_delay_log, fill=carrier)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE) +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Boxplot by factor\") +\n    xlab(\"Carrier\")\n\n\n\n\n\n\n\n\nUsing the fill argument here creates a grouped boxplot, where grouping is given by the fill variable. To reduce the size of the plot, we filter only four carriers.\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"B6\", \"DL\", \"EV\")) %&gt;%\n  ggplot( aes(x=carrier, y= arr_delay_log, fill=origin)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE) +\n    theme(\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Boxplot by two factors\") +\n    xlab(\"Carrier\")",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#multiple-plots",
    "href": "notes/week_02.html#multiple-plots",
    "title": "Exploring Data with tidyverse",
    "section": "3.4 Multiple Plots",
    "text": "3.4 Multiple Plots\nThere are numerous ways how to display multiple plots. The default is to set par(mfrow = c(n_rows, n_cols)) before plotting, an alternative is the lattice package, but these do not work with ggplot. With ggplot, one can utilize ggarrange() from the ggpubr package. However, these will require you to do a lot of copy-pasting since you still need to create plots individually, but sometimes we want to create numerous plots similar in nature. To this point, the ggplot2’s facet_wrap function allows you to split into numerous plots by a value of a factor.\nBut what if we wish to create one plot for every variable? This is doable with facet_wrap provided we have:\n\nall the values (in our data frame) given as a single variable\nanother variable linking the original variables to the values.\n\nThis is exactly what we get by using pivot_longer() below. The argument everything() specifies we want to keep all the variables, otherwise we could use the variable names like with select() (just here, multiple variables have to be wrapped in c() due to the function’s syntax) to keep only some or discard some variables. pivot_longer() will give us a long-format data frame with two columns: value and name. Then we plot value and wrap the facets based on name.\n\ndata(chredlin, package=\"faraway\")\nchredlin %&gt;% mutate(side = as.numeric(side)) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  ggplot(aes(value)) + facet_wrap(~ name, scales = \"free\") + geom_histogram()\n\n\n\n\n\n\n\n\nThe complement of pivot_longer() is pivot_wider(). We may want to use it, e.g., after group_by() and summarize(), which naturally lead to a long table. For example, assume we want to produce a heatmap of the average departure delay of flights on different days (x-axis) and months (y-axis), in order to see average departure delay in a calendar-like fashion. While this is not terribly useful, it is done below.\n\nnew_dat &lt;- flights %&gt;% \n  group_by(month, day) %&gt;%\n  summarize(count = mean(dep_delay)) %&gt;%\n  pivot_wider(names_from = day, values_from = count) %&gt;%\n  ungroup() %&gt;% select(-month)\n\nlibrary(lattice)\nlibrary(viridisLite)\ncoul &lt;- plasma(100)\nlevelplot(t(as.matrix(new_dat)), col.regions = rev(coul), xlab=\"Day\", ylab=\"Month\", main=\"Departure delay\")\n\n\n\n\n\n\n\n\nWhile we could create a heatmap using ggplot2’s geom_tile(), I personally find levelplot() of the lattice package more useful for quick plotting. One can do great heatmaps with ggplot2 and geom_tile(), but it requires some work to set up the color schemes and to magnify the colorkey.\nWhile the previous heatmap is not so useful, we will later see that heatmaps are quite handy e.g. when probing performance of a certain method based on two parameters.",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "notes/week_02.html#marginal-scatterplots-with-regression-lines-and-outliers",
    "href": "notes/week_02.html#marginal-scatterplots-with-regression-lines-and-outliers",
    "title": "Exploring Data with tidyverse",
    "section": "3.5 Marginal Scatterplots with Regression Lines and Outliers",
    "text": "3.5 Marginal Scatterplots with Regression Lines and Outliers\nIf we fit a linear model to the response variable involact with all available variables (except side) as covariates, we notice that there are some outliers and leverage points (looking at the Cook’s distance). Wouldn’t it be nice to add these into the marginal scatterplots directly to see which scatter points belong to the outliers?\nFirstly, If we wish to have scatterplots with the response variable involact on the y-axes, we naturally need one more column with involact values in the long format. This is actually done automatically by dropping involact from pivoting.\n\nchredlin %&gt;% mutate(side = as.numeric(side), income=log(income)) %&gt;% \n  pivot_longer(-involact) %&gt;%\n  ggplot(aes(y = involact, x = value)) + facet_wrap(~ name, scales = \"free\") +\n  geom_point() + stat_smooth(method=lm)\n\n\n\n\n\n\n\n\nSecondly, let’s denote the outliers. We will use the second extra-variable approach (as opposed to the over-plotting approach). We create an additional variable which tells us, which of the observations are outliers.\n\noutliers &lt;- I(1:nrow(chredlin) %in% c(3,6,35))\nleverage_pts &lt;- I(1:nrow(chredlin) %in% c(7,24))\nchredlin &lt;- chredlin %&gt;%\n  mutate(out_lev =  as.factor(outliers + 2*leverage_pts))\nlevels(chredlin$out_lev) &lt;- c(\"normal obs\", \"outlier\", \"leverage point\")\n\nchredlin %&gt;% mutate(side = as.numeric(side), income=log(income)) %&gt;% \n  pivot_longer(cols=c(-involact, -out_lev)) %&gt;%\n  ggplot() + facet_wrap(~ name, scales = \"free\") +\n  geom_point(mapping = aes(y = involact, x = value, color = out_lev)) +\n  stat_smooth(mapping = aes(y = involact, x = value), method=lm)",
    "crumbs": [
      "Supplementary notes",
      "Exploring Data with `tidyverse`"
    ]
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 3 November 2024.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-5-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (generated from the quarto file report.qmd) and the quarto file itself. The report.qmd should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 21 September 2025.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-1-YOUR_GITHUB_USERNAME to get started.",
    "crumbs": [
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 19 October 2025.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-4-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (generated from the quarto file report.qmd) and the quarto file itself. The report.qmd should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment-08.html",
    "href": "assignments/assignment-08.html",
    "title": "Assignment 8",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 10 December 2023.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-6-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (generated from the quarto file report.qmd) and the quarto file itself. The report.qmd should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 8"
    ]
  },
  {
    "objectID": "projects/project-02.html",
    "href": "projects/project-02.html",
    "title": "Main project",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 21 December 2025.\nThe goal of this project is quite broad, students are free to come up with their own ideas. While simulation studies are the designated topic, groups that found interesting data during the small project and would like to carry on analyzing it, or groups interested in studying a bit deeper one of the methodological concepts from this course are encouraged to approach the teachers during the exercises and discuss their ideas. Prospective topics for the final project will be gradually revealed during the lectures.\nPart of the grade for the final project will be awarded for value added (original data analysis, simulation study answering a previously unclear question, etc.). All of the prospective topics that will be introduced during the lecture will have this element, and by half-way through the semester (when the final project will start) it should be clear through the examples what the project should aspire to. We will also discuss this in person at some point, likely on Week 7. The remaining part of the project will be awarded for\nA project seriously lacking in any of the criteria above will be penalized.",
    "crumbs": [
      "Project",
      "Main project"
    ]
  },
  {
    "objectID": "projects/project-02.html#intermediate-submission",
    "href": "projects/project-02.html#intermediate-submission",
    "title": "Main project",
    "section": "Intermediate submission",
    "text": "Intermediate submission\n\nBy Sunday November 9, you should have chosen a team and a topic.\nOn Friday November 21 your team will submit a 1-2 page writeup. Your writeup should provide a preliminary introduction to the topic you will study and provide clear motivation for why they are interesting and/or relevant.",
    "crumbs": [
      "Project",
      "Main project"
    ]
  },
  {
    "objectID": "projects/project-02.html#final-submission",
    "href": "projects/project-02.html#final-submission",
    "title": "Main project",
    "section": "Final submission",
    "text": "Final submission\nYou are required to hand in a PDF version of your report report.pdf (max 20 pages) and the source code used. You should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on reproducing the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline. Your README.md should also include a brief description of the contributions of each team member, if you are a team of three students.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex) (max 20 pages)\nreport.qmd in GitHub repository (source code, should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report (please delete everything not related to your project in it)",
    "crumbs": [
      "Project",
      "Main project"
    ]
  },
  {
    "objectID": "projects/project-01.html",
    "href": "projects/project-01.html",
    "title": "Small project",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 13 October 2024.\nChecklist:\n\nreport.pdf in GitHub repository of no more than 10 pages\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report\n\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the project, and either join an existing team or create a new one. Once this is done, go to the course GitHub organization and locate the repo titled mini-project-TEAM-NAME to get started.\n\n\n\nThe goal of this project is data exploration. Find an interesting (in the sense it interests you!) data set and\n\nlay out some questions about the data\ndescribe the data\nexplore the data\nvisualize the data\nuse more detailed visualization techniques to hint answers\n\nThese steps correspond to the first three stages of the data science life cycle.\nNote that the purpose of this project is to play around, demonstrating your data exploration, wrangling and visualization skills. Hopefully, you will also find scientifically interesting questions or questions of personal interest, e.g., does ball possession matters in a game of football? Is there a link between the GDP of a country and its contribution to global warming? Try though to avoid Kaggle data sets that have been analyzed zillion times before.\nHere is an example of what your report could look like. See this page for some tips and resources (e.g., example datasets)."
  },
  {
    "objectID": "exercises/exercise-01.html",
    "href": "exercises/exercise-01.html",
    "title": "Exercise 1",
    "section": "",
    "text": "The severity depends on the number of peeks the researcher takes at the data and on the number of observations added between peeks.\nExercise: The “Example of Peeking” below is an example of a small simulation study, checking whether a designed test strategy respects the nominal level \\(\\alpha = 0.05\\) or not. Here, we perform a one-sample t-test with an initial sample size of 25. If we are unable to reject the null hypothesis (which is true), we add an additional 10 observations, but only once. Incorporate further levels of peeking and different amounts of added information at each peak to see how it affects the nominal significance level.\n\npeeking &lt;- function(a,b=10){\n  x &lt;- rnorm(25)\n  Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n  if(abs(Tstat) &gt; qt(0.975,length(x)-1)){\n    return(Tstat)\n  }else{\n    x &lt;- append(x, rnorm(b))\n    Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n    return(Tstat)\n  }\n}\nset.seed(517)\nTstats &lt;- sapply(1:10000,peeking)\nmean(I(abs(Tstats) &gt; qnorm(0.975)))\n\n[1] 0.0851",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/exercise-04.html",
    "href": "exercises/exercise-04.html",
    "title": "Exercise 4",
    "section": "",
    "text": "This exercise focuses on using rejection sampling to generate standard normal random variables with different instrumental densities.",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/exercise-04.html#part-a",
    "href": "exercises/exercise-04.html#part-a",
    "title": "Exercise 4",
    "section": "Part a",
    "text": "Part a\n\nProvide a rejection algorithm to generate \\(\\mathcal{N}(0,1)\\) random variables, where the instrumental density is Cauchy: \\[ g(x) = \\frac{1}{\\pi (1 + x^2)}, \\quad x \\in \\mathbb{R} \\]\n\nDetermine the efficiency of this algorithm.",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/exercise-04.html#part-b",
    "href": "exercises/exercise-04.html#part-b",
    "title": "Exercise 4",
    "section": "Part b",
    "text": "Part b\n\nDevelop an algorithm that generates realizations from a double exponential (Laplace) distribution, with density: \\[\n  g(x) = \\frac{\\alpha}{2} \\exp(-\\alpha |x|), \\quad \\alpha &gt; 0, \\; x \\in \\mathbb{R}\n\\]\n\nFormulate a rejection algorithm to generate \\(\\mathcal{N}(0,1)\\) random variables, where the instrumental density is a double exponential distribution.\nFind the optimal value of the parameter \\(\\alpha\\) for this density, and determine the corresponding efficiency.",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/exercise-04.html#part-c",
    "href": "exercises/exercise-04.html#part-c",
    "title": "Exercise 4",
    "section": "Part c",
    "text": "Part c\n\nImplement the above algorithms in R, using functions such as rcauchy, runif, rexp, or abs where appropriate.\nEmpirically verify the theoretical efficiencies of each algorithm.",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/exercise-03.html",
    "href": "exercises/exercise-03.html",
    "title": "Exercise 3",
    "section": "",
    "text": "This is one of the original illustrating examples for the use of the EM algorithm.\nSuppose that a vector of observed counts \\(y=(125,18,20,34)\\) arises from a multinomial distribution with cell-probabilities \\((\\frac{1}{2}+\\frac{\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{\\theta}{4})\\). The aim is to find the ML estimate of \\(\\theta\\).\nThe density of the observed data is \\[\nf_{Y \\mid \\Theta}(y \\mid \\theta)=\\frac{n !}{y_{1} ! y_{2} ! y_{3} ! y_{4} !}\\left(\\frac{1}{2}+\\frac{\\theta}{4}\\right)^{y_1}\\left(\\frac{1}{4}-\\frac{\\theta}{4}\\right)^{y_2}\\left(\\frac{1}{4}-\\frac{\\theta}{4}\\right)^{y_3}\\left(\\frac{\\theta}{4}\\right)^{y_4}\n\\] Although the log-likelihood can be maximized explicitly, we use the example to illustrate the EM algorithm.\nTo view the problem as an unobserved data problem, we would think of it as a multinomial experiment with five categories with observations \\(x=\\left(y_{11}, y_{12}, y_2, y_3, y_4, y_5\\right)\\), each with cell probability \\((1 / 2, \\theta / 4,(1-\\theta) / 4,(1-\\theta) / 4, \\theta / 4)\\). That is, we split the first category into two and we can only observe the sum \\(y_1=y_{11}+y_{12}\\).\n\nWrite down the complete likelihood\nPerform the EM algorithm\nFind the stationary solution which should correspond to the closed-form solution for the ML estimate",
    "crumbs": [
      "Exercises",
      "Exercise 3"
    ]
  },
  {
    "objectID": "exercises/exercise-02.html",
    "href": "exercises/exercise-02.html",
    "title": "Exercise 2",
    "section": "",
    "text": "Consider bivariate kernel density estimation. Simulate data from the bivariate normal distribution \\(\\mathcal{N}((0,0), (1, 0.3; 0.3, 1))\\).\n\nTry different bandwidths and different kernels (use the bivariate normal kernel as well as the product kernel with the univariate Epanechnikov kernel)\nFind ways to visually compare your estimates with the real density (3d plots of the density or density contour plots)\nR functions such as kde2d of package MASS, contour and persp might be helpful",
    "crumbs": [
      "Exercises",
      "Exercise 2"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MATH-517 Statistical computation and visualisation in Fall 2025 at EPFL. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here. All the additional ressources can be found either on this page or by using the search bar on the top left corner of the website. These ressources include supplementary material, tutorials, tips and tricks, etc…",
    "crumbs": [
      "Course information",
      "Overview/Organisation"
    ]
  },
  {
    "objectID": "course-overview.html#assignments",
    "href": "course-overview.html#assignments",
    "title": "Course overview",
    "section": "3.1 Assignments",
    "text": "3.1 Assignments\nThere will be 8 assignments during the semester, graded as described above. Assignments can be found in the Assignments tab on this website (or equivalently in the schedule). Exercises are not mandatory, only recommended. Note that the first assignment will only have a computational part (setting up the GitHub machinery) while the remaining assignments will have a theoretical part and a computational part.\n\nTo accept the assignment, you need to first access the google document with all the invite links, then click on the link corresponding to the assignment you want to accept. This will create a repository for you on GitHub. You can then clone this repository on your computer and start working on the assignment.",
    "crumbs": [
      "Course information",
      "Overview/Organisation"
    ]
  },
  {
    "objectID": "course-overview.html#project",
    "href": "course-overview.html#project",
    "title": "Course overview",
    "section": "3.2 Project",
    "text": "3.2 Project\nThe main project can start following the \\(7\\)-th lecture, deadline on December 21, at 23:59. This is a soft deadline. I would suggest you finish the project before Christmas, however, if all members of the team agree to this, the project can be submitted by the end of the calendar year. This is recommended in order to prevent the holiday season ruined by a lazy member(s) of the team. Note that if a single member of your team wishes to submit on December 21, you are required to do so. See here for details.\n Groups can be of size of either 2 or 3 people. The size will not matter w.r.t. to grading. However, a group of size 3 will have one additional task to do: as part of their submission, every team member will individually include a short paragraph describing contributions of every individual member of the team. This is not to be discussed among the team members, as it serves as a safeguard. Regardless of their individual contributions, each member of the team will receive the same grade, apart from where this would be extremely unfair. Such cases will be discussed personally. In case of any team-work problems, the students are encouraged to seek advice (mostly as a group) from the teachers (mostly during the exercise classes).",
    "crumbs": [
      "Course information",
      "Overview/Organisation"
    ]
  },
  {
    "objectID": "course-overview.html#license",
    "href": "course-overview.html#license",
    "title": "Course overview",
    "section": "5.1 License",
    "text": "5.1 License\n\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nThis website and part of the course materials where adapted from different sources:\n\nDr. Mine Çetinkaya-Rundel STA 101 website\nDr. Tomas Masák MATH-517 Fall 2022 course\nfredhutch.io and R for data analysis and visualization of Ecological Data",
    "crumbs": [
      "Course information",
      "Overview/Organisation"
    ]
  },
  {
    "objectID": "course-overview.html#footnotes",
    "href": "course-overview.html#footnotes",
    "title": "Course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDetails on how group submissions will be handled will be provided later in the semester.↩︎",
    "crumbs": [
      "Course information",
      "Overview/Organisation"
    ]
  },
  {
    "objectID": "projects/project-tips-resources.html",
    "href": "projects/project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "Consistency is the most important thing for a statistician.\n\njumping between different citation styles is bad\nhaving some captions centered above the figure produced by R and others flushed right below using Markdown is annoying\n\nThe report should be self-contained.\n\nit should not be a tutorial for the code\nthe reader should not have to jump between the report and the code to understand your work\n\nSave space for better readability.\n\nplots that convey little information don’t have to be large, several (related) plots can be put next to each other on the same line, etc.\neven in an html file, unnecessary scrolling back and forth when reading a report is annoying\na plot frame with one boxplot is a waste of space (e.g., histogram would be better)\nbarplots can often be replaced by tables to both save space and improve readability\nin general, if a plot only shows like 3 numbers and is not important for any argument made, it should not be a plot\n\nStory-telling matters.\n\nit is important to grasp attention with an introduction (and describe at the same time what to expect from a report)\nre-iterate the most important ideas/results in several places\ncomment on plots even if they are self-explanatory\n\nMore on plots.\n\ntext in figures (labels, etc.) should be of similar size as the main text\nlabels have to be readable, e.g., no overlaying etc.\ncaptions are necessary and should make the plot self-contained (without looking at the paragraphs around it)\n\nList of references should be itemized or enumerated (in order to be readable).\nAvoid using local paths.\n\nreproducibility of the report itself!\n\nTransforming variables.\n\nif your plots look bad because of a clear skewing in one of the variables, transform the varible (typically plot it on a log-scale)\nif plotting on a log-scale, you might consider log10 or log2 to have better interpretability",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "projects/project-tips-resources.html#tips-and-tricks-for-visualisation",
    "href": "projects/project-tips-resources.html#tips-and-tricks-for-visualisation",
    "title": "Project tips + resources",
    "section": "",
    "text": "Consistency is the most important thing for a statistician.\n\njumping between different citation styles is bad\nhaving some captions centered above the figure produced by R and others flushed right below using Markdown is annoying\n\nThe report should be self-contained.\n\nit should not be a tutorial for the code\nthe reader should not have to jump between the report and the code to understand your work\n\nSave space for better readability.\n\nplots that convey little information don’t have to be large, several (related) plots can be put next to each other on the same line, etc.\neven in an html file, unnecessary scrolling back and forth when reading a report is annoying\na plot frame with one boxplot is a waste of space (e.g., histogram would be better)\nbarplots can often be replaced by tables to both save space and improve readability\nin general, if a plot only shows like 3 numbers and is not important for any argument made, it should not be a plot\n\nStory-telling matters.\n\nit is important to grasp attention with an introduction (and describe at the same time what to expect from a report)\nre-iterate the most important ideas/results in several places\ncomment on plots even if they are self-explanatory\n\nMore on plots.\n\ntext in figures (labels, etc.) should be of similar size as the main text\nlabels have to be readable, e.g., no overlaying etc.\ncaptions are necessary and should make the plot self-contained (without looking at the paragraphs around it)\n\nList of references should be itemized or enumerated (in order to be readable).\nAvoid using local paths.\n\nreproducibility of the report itself!\n\nTransforming variables.\n\nif your plots look bad because of a clear skewing in one of the variables, transform the varible (typically plot it on a log-scale)\nif plotting on a log-scale, you might consider log10 or log2 to have better interpretability",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "projects/project-tips-resources.html#some-links-to-open-data",
    "href": "projects/project-tips-resources.html#some-links-to-open-data",
    "title": "Project tips + resources",
    "section": "Some Links to Open Data",
    "text": "Some Links to Open Data\n\nfivethirtyeight: article data of Nate Silver’s data journalism platform freely available (see also R package - fivethirtyeight)\ndata-is-plural: weekly newsletter of datasets by Jeremy Singer-Vine\nre3data: Registry of research data repositories\nopenml datasets: many uniformly formatted datasets for training machine learning models – however, not always good descriptions available\nWorldbank Datacatalog: the World Bank data catalogue\nUK Data Service: UK’s largest collection of social, economic and population data resources (filter for open data) or also data.gov.uk\nICPSR: unit within the Institute for Social Research at the University of Michigan, social and behavioral research. In particular including replication datasets for published studies.\ngovdata: Open Government - German administrative data freely accessible\ngapminder: “an independent educational non-proﬁt ﬁghting global misconceptions”; collection and vizualisation of datasets concerning gobal developement\nnature.com: peer-reviewed, open-access journal for descriptions of datasets (broad range of natural science disciplines)\nNIH (National Institute of Health) Data Sharing Repositories: overview on different thematically sorted medical databases\nUCI Machine Learning Repository or the new beta version: containing various datasets – however, sometimes with a little few description\ndata.bris Research Data Repository: Data repository of the University of Bristol\n\n… no systematic selection. Much more out there",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "projects/mini_project_example.html",
    "href": "projects/mini_project_example.html",
    "title": "Example of data visualization and exploration report",
    "section": "",
    "text": "NOTA BENE\n\n\n\nThis report was submitted by a group of students as part of the course MATH-517 at EPFL, and is presented here as an example of a data visualization and exploration."
  },
  {
    "objectID": "projects/mini_project_example.html#gender-difference-in-preferences-for-partner",
    "href": "projects/mini_project_example.html#gender-difference-in-preferences-for-partner",
    "title": "Example of data visualization and exploration report",
    "section": "Gender difference in preferences for partner",
    "text": "Gender difference in preferences for partner\n\nFirst overview\nA first question one might ask is Are male and female participants interested in the same attributes for partner? To anwer it one can try and compare the reported importance to male and female candidates for partner characteristics (attractiveness, sincerity, intelligence, fun, ambition, shared interests). This is done by grouping the data associated to the columns : attr1_1, sinc1_1, intel1_1, fun1_1, amb1_1, shar1_1 by gender and then obtaining a violin plot. For context, this data is expressed as a set of weights for the six attributes that sum up to 100.\nWe observe in Figure 1 that in our dataset, on average males attribute a greater importance to physical attractiveness, while sincerity is more important to females. Moreover, some male participants attribute a great importance to attractiveness, sometimes close to 100%. On the other hand, both genders put a similar and low importance on ambition and having shared interests.\n\n\n\n\n\n\nFigure 1: Violin plot of the weight of each attribute in participant’s choice of partner. A weight of 100% for an attribute means that only this attribute matters in the choice of partner.\n\n\n\n\n\nSuccessful vs. Unsuccessful candidates\nIn the previous paragaraph, some differences between male and female participants were outlined. Following this, it seems natural to wonder about the existance of differences within the same gender. One way to do this is to answer the question What are the differences between successful and unsuccessful female (respectively male)?\nIn order to give an answer, we use the fact that by the end of the experiment, each participant has recieved a certain number of “yes”s (grade: 1) and “no”s (grade: 0) by the partners they have met. This is contained in the column dec_o (decision of the other). We define the proportion of “yes”s each individual has recieved as a measure of their success. Notice that this measure will only take values between 0 and 1. On Figure 2 are represented the cumulative distributions of the success rate for male and female participants. These will allow us to categroize successful and unsuccessful individuals by looking at the intersection with the horizontal lines \\(y=25\\%\\) and \\(y=75\\%\\)\nThe curve representing male participants is shifted to the left compared to the one representing female ones. It can be deduced that males are on average less successful than females. The 25% least successful males are labeled in the following as “Unseccessful males” while the 25% most successful males are labeled as “Successful males”. Same definition holds for females.\n\n\n\n\n\n\nFigure 2: Success rate (measured as the proportion of “yes” recieved) cumulative distribution for male and female participants. Horizontal lines correspond to 25% and 75% proportions.\n\n\n\nIn the sake of analyzing characteristics that differentiate successful and unsuccessful individuals, it is interesting to compare the interest each category (successful and unsuccessful) have, on average, for different activites. The interest for an activity has been expressed on a 1-10 scale. Data about the interest for 17 activties have been reported. Figure 3 and Figure 4 only report the results (for males and females respectively) for the five activities (reported in columns sports,tvsports,exercise,dining and clubbing)where the differences are statistically significant.\n\n\n\n\n\n\nFigure 3: Average interest (1-10 scale) for each activity among the 25% most successful, and the 25% least successful male participants.\n\n\n\n\n\n\n\n\n\nFigure 4: Average interest (1-10 scale) for each activity among the 25% most successful, and the 25% least successful female participants.\n\n\n\nFrom the figure it can be observed that successful males have more interest for sports and exercice than unsuccessful males. Tvsports find more interest among unsuccessful females than successful ones. While successful and unsucessful female participants have the same interest for dining, successful males show a significant difference of interest compared to the unsuccessful ones. Both for male and female participants, clubbing recieves more interest from successful individuals (the difference is more marked for males).\nHaving analyzed the difference of interest for each activity there is between successful and successful male and female individuals, data regarding the rating (by the partners) of the attributes of each individual is analyzed in order to fully answer the question. Indeed, each participant recieves from their partners a grade (1-10 scale) on their percieved attributes (attractiveness, sincerity, intellect, fun and ambition). More precisely, for example, each male participant will recieve from each female participant a grade on how they look attractive. To each male will therefore be assigned an average grade on their attractiveness. An average over all successful males is then made. The same procedure is made for unsuccessful males, successful females and unsucessful females. The standard deviations of the data among each examined samples being similar and small enough (no standard deviation is greater than 0.14), it is more useful to deliver the data in tables. Table 1 shows the average grade recieved for each attribute for the 25% most successful and the 25% most unsuccessful males and female participants respectively.\n\n\n\nTable 1: Attributes of successful and unsuccessful participants as perceived by the other participants\n\n\n\n\n\n\n\n\n\n\n\n\nQuality\nSuccessful Male\nUnsuccessful Male\nSuccessful Female\nUnsuccessful Female\n\n\n\n\nAttractiveness\n7.08\n4.52\n7.54\n5.30\n\n\nSincerity\n7.41\n6.83\n7.41\n7.13\n\n\nIntellect\n7.72\n7.18\n7.53\n7.02\n\n\nFun\n7.28\n5.06\n7.19\n5.77\n\n\nAmbition\n7.30\n6.71\n6.99\n6.13\n\n\n\n\n\n\nOn average, successful individuals are more attractive, more sincere, more intelligent, more fun and more ambitous, be they males or females. It is worth noticing that unsuccessful females are on average given a grade of 5.30/10 for their attractiveness which is signficantly higer than the average grade given to unsuccessful males."
  },
  {
    "objectID": "projects/mini_project_example.html#how-much-do-shared-interests-really-matter",
    "href": "projects/mini_project_example.html#how-much-do-shared-interests-really-matter",
    "title": "Example of data visualization and exploration report",
    "section": "How much do shared interests really matter ?",
    "text": "How much do shared interests really matter ?\nIn the previous sections (Figure 1), we saw that participants overall declared putting a low importance on shared interests in the choice of partner. Surprised by this result, we wanted to shed light on How important are shared interests in matching? Exploring the data, one can find a int_corr variable, corresponding in the words of the authors to “correlation between participant’s and partner’s ratings of interests”. Thought this variable sounds promising to study impact of shared interest on compatibility, we were : 1. not able to make sense of it (i.e. re-derive the way in which it was obtained) 2. not able to obtain any sensible visualisation (the data looked like noise). Because of this we turned to the 1 to 10 grades that each participants associates to each of the 17 interests present in the dataset for help. By considering the grade vectors \\(v_i\\) and \\(v_j\\) for two candidates, we then defined the similarity between these through a scalar product as \\[\nS(v_i,v_j)=\\frac{\\langle\\tilde{v}_i,\\tilde{v}_j\\rangle}{\\|\\tilde{v}_i\\|\\ \\|\\tilde{v}_j\\|},\n\\]\nwhere \\(\\tilde{v}_i={v}_i-5\\) to recenter the vectors. Notice that this measure of similarity gives results in the \\([-1,1]\\) interval, with positive values associated to generally shared interests and negative ones with generally different interests.\nGiven this definition of similarity, one can then explore the effect of shared interests on the average match rate. This is done in Figure 5. The linear fit corresponds to an ordinary least squares linear regression. The shaded area is a 95% confidence interval that is estimated using a bootstrapping statistics, see NormandErwan (2018). By looking at it, one can easily see that there is a positive correlation between interest similarity and match rate.\n\n\n\n\n\n\nFigure 5: Average match rate as a function of the defined interest similarity measure. The linear fit corresponds to an ordinary least squares linear regression. The shaded area is a 95% confidence interval."
  },
  {
    "objectID": "projects/mini_project_example.html#other-notable-factors",
    "href": "projects/mini_project_example.html#other-notable-factors",
    "title": "Example of data visualization and exploration report",
    "section": "Other notable factors",
    "text": "Other notable factors\nAs mentionned previously, in addition to the data about prefered characteristics and interests, the used dataset contained a lot of personal information about the participants. Exapmples include age, field of study/work, SAT grade, income, religion and etnicity. In the following we explore the impact of some of this data.\n\nImpact of SAT score on success rate\nWe would like to know if academic success is correlated with success in dating. To achieve this we use the SAT score included in the sat column of the dataset. While the SAT score is only used for university admissions, we’ll use it here a proxy metric for academic sucess. On the other hand, to quantify the success of participants, we will use the column dec_o \\(\\in {0, 1}\\) indicating for each meeting if the participant was liked by the person in front of them. We define the success rate of participant \\(i\\) as the average of dec_o over all the speed dates in which \\(i\\) took part.\nThe first observation is that the average SAT score for men and women over the dataset are of 1320 and 1280 respectively, significantly higher than the national US average which is around 1050. Figure 6 contains a scatter plot of the success rate of male and female participants as a function of their SAT score. An ordinary least squares linear regression is associated to each gender. We observe a higher slope for males than females (\\(3.3 \\times 10^{-3}\\) vs \\(7.8 \\times 10^{-5}\\)), hinting that academic sucess is more important for men than women in speed dating success.\n\n\n\n\n\n\nFigure 6: Success rate of male and female participants as a function of their SAT score. At first glance, it would appear that a higher SAT score impacts success more for males than females.\n\n\n\n\n\nImpact of ethnicity on match\nWether it is because of habituation, people wanting to stay in their comfort zone or cultural influence, it is clear some people might prefer partners of their same ehnicity. Predicting the relation between stated importance and observed preference remains however untrivial. This is what we focus on in this section.\nThe relevant data to this section are the grades (on a 1-10 scale) that the participant attributed to the importance for their partner to be of the same ethnicity as them. The quantity we consider is the difference between the average acceptance of people of same ethnicity (which we call \\(G_{ethn}\\)) and the general one (which we call \\(G_{all}\\)). For the scale to be independent of the personal standards of the participants this difference is normalised by the acceptance rate of every individual. The average of \\((G_{ethn}-G_{all})/G_{all}\\) grouped by stated importance is plotted in Figure 7.\n\n\n\n\n\n\nFigure 7: Average over all individuals of the relative difference between the average over grades given to people of the same race \\(G_{ethn}\\) and the average over grades given to every person they’ve met.\n\n\n\nLooking at the figure it can be noticed that pople who stated a small importance (of 1,2 or 3 for example) for their partner to be of same ethnicity, indeed are (on average) associated to equal grading of the participants they met (the relative difference is close to zero). For people who state a bigger importance however, the relative difference seems to become significantly bigger than 0. People who reported a maximal importance to having a partner of the same ethnicity (10/10), are found to be 70% more likely to match with people of their ethnicity compared to others."
  },
  {
    "objectID": "projects/mini_project_example.html#do-people-percieve-us-as-we-do",
    "href": "projects/mini_project_example.html#do-people-percieve-us-as-we-do",
    "title": "Example of data visualization and exploration report",
    "section": "Do people percieve us as we do ?",
    "text": "Do people percieve us as we do ?\nAs seen in the section about successful and unsuccesfull candidates, some factors, such as how fun and attractive people are, affect their successrate in speed dating. Given this observation, some might try to improve their chances of success by working on their shortcomings. This should be rather straight forward, assuming that person are able to gauge where they currenly stand. But is this a reasonable assumption? In this section we focus on investigating the relevance of this assumption.\nTo do this we need to compare data about people’s own perception of themselves, and that of other people. Fortunately we have access to such data. Indeed, in addition to the attr, sinc, intel, fun, amb, shar ratings that other people give to the candidtes they meet, the dataset includes ratings that each person gave themselves on the same attributes. Figure 8 represents the self-prescribed ratings in terms of the externally prescribed ones. The values for the response variables were obtained by grouping the candidates by their self-prescribed ratings and taking the average. The confidence intervals correspond to \\(\\pm\\hat{\\sigma}\\) where \\(\\hat \\sigma\\) is the corrected sample standard deviation. Notice that only the values associated to attractiveness and intelligence are included in order not to overcrowed the plot (the other attributes behaved similarly to intelligence anyway). A square-aspect ratio plot was chosen since the goal is to compare the observed results to the \\(y=x\\) baseline.\nLooking at the figure, we immediately see that people’s own intelligence rating (on average) does not correlate with other people’s ratings. This might sound surprising at first. It can however be understood by noticing that intelligence is a vague term, which makes it is hard to evaluate both your own rating and evaluate that of someone else. In the second case, this is aggraveted by the fact that the estimate is only based on a 4 minute interraction. The observed behaviour for attractiveness is more nuanced. On one hand, it seems that people are (on average) good at evaluating when they do not appear attractive to others. This is reflected by the fact that the points associated to the 4,5,6 ratings nicely overlap with the \\(y=x\\) line. On the other hand, there is no significant difference in externally-reported attractiveness between groups that self-report their attractiveness as 7,8,9 or 10 out of 10. The only difference between these groups seems therefore to be in their ego.\n\n\n\n\n\n\nFigure 8: Square-aspect-ratio plot of externally-evaluated attributes as a function of self-evaluated ones. The response variable is obtained by grouping the control variable and taking the average. The confidence intervals correspond to \\(\\pm\\hat \\sigma\\) where \\(\\hat \\sigma\\) is the corrected sample standard deviation."
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 5 October 2025.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-2-YOUR_GITHUB_USERNAME to get started.",
    "crumbs": [
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 17 November 2024.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-6-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (generated from the quarto file report.qmd) and the quarto file itself. The report.qmd should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment-07.html",
    "href": "assignments/assignment-07.html",
    "title": "Assignment 7",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 8 December 2024.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-7-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (generated from the quarto file report.qmd) and the quarto file itself. The report.qmd should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd or report.tex)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 7"
    ]
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Deadline\n\n\n\nDue date: 11:59pm on Sunday, 12 October 2025.\n\n\n\n\n\n\n\n\nInvite link for GitHub Classroom\n\n\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to thecourse GitHub organization and locate the repo titled ae-3-YOUR_GITHUB_USERNAME to get started.\n\n\n\n\n\n\n\n\n\nSubmission requirements\n\n\n\n\n\nYou are required to hand in a PDF version of your report report.pdf (e.g., generated from the quarto file report.qmd) and the quarto file itself if applicable, or the accompanying code. The report.qmd or the accompanying code should contain all the code necessary to reproduce your results: you should not show the actual code in the PDF report, unless you want to point out something specific.\nYour README.md should contain instructions on how to reproduce the PDF report from the quarto file. This can be useful if you have issues with the automatic generation of the PDF report right before the deadline.\nChecklist:\n\nreport.pdf in GitHub repository (e.g., generated from report.qmd)\nsource code in GitHub repository (should be able to run from top to bottom)\nREADME.md with instructions on how to run the code and reproduce the PDF report",
    "crumbs": [
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notes/week_06.html",
    "href": "notes/week_06.html",
    "title": "EM Algorithm",
    "section": "",
    "text": "Maximum likelihood is the dominant form of estimation in statistics. Recall that it is a parameter estimation procedure, so we always have to put a parametric model to our data. The EM algorithm is an iterative algorithm for calculating maximum likelihood estimators (MLEs) in situations where\n\nthere is missing data (e.g., censored observations, Example 1 below) complicating the calculations, or\nit is beneficial to think of our data as if there were some components missing, because it would simplify the calculation (e.g., estimating mixture distributions, Example 2 below).\n\nLet us denote\n\n\\(X_{obs}\\) are the observed random variables\n\\(X_{miss}\\) are the missing random variables\n\\(\\ell_{comp}(\\theta)\\) is the complete log-likelihood of \\(X = (X_{obs},X_{miss})\\)\n\nmaximizing this to obtain MLE is supposed to be simple\n\\(\\theta\\) denotes all the parameters, e.g. contains \\(\\mu\\) and \\(\\Sigma\\)\n\n\nOur task is to maximize \\(\\ell_{obs}(\\theta)\\), the observed log-likelihood of \\(X_{obs}\\).\nEM Algorithm: Start from an initial estimate \\(\\theta^{(0)}\\) and for \\(l=1,2,\\ldots\\) iterate the following two steps until convergence:\n\nE-step: calculate \\[\\mathbb{E}_{\\theta^{(l-1)}}\\big[\\ell_{comp}(\\theta) \\big| X_{obs} = \\mathbf{x}_{obs}\\big] =: Q\\big(\\theta,\\theta^{(l-1)}\\big)\\]\nM-step: optimize \\[\\mathrm{arg\\,max}_{\\theta}\\; Q\\big(\\theta,\\theta^{(l-1)}\\big) =: \\theta^{(l)}\\]\n\nThe E-step, i.e. calculating the expected likelihood, sometimes coincides with calculating expected values of the unobserved data (with the current parameters) and plugging them into the complete likelihood, but this is not always the case (see Example 3 below)! Actually, as will become clear, it is the case if and only if the complete log-likelihood is linear (w.r.t. the full data).\n\n\nSee the slides of Week 6.\n\n\n\nLet \\(X_1,\\ldots,X_N\\) be i.i.d. from \\[f_{\\theta}(x) = (1-\\tau) \\varphi_{\\mu_1,\\sigma_1}(x) + \\tau \\varphi_{\\mu_2,\\sigma_2}(x) = (1-\\tau) \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp\\left\\lbrace - \\frac{1}{2} \\left( \\frac{x-\\mu_1}{\\sigma_1} \\right)^2 \\right\\rbrace + \\tau \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}} \\exp\\left\\lbrace - \\frac{1}{2} \\left( \\frac{x-\\mu_2}{\\sigma_2} \\right)^2 \\right\\rbrace\\]\nThe task is to estimate \\(\\theta = (\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\tau)^\\top\\) via MLE, i.e., solve \\[\n\\underset{\\theta}{\\mathrm{arg\\,max}} \\underbrace{\\sum_{n=1}^N \\log f(X_n)}_{\\ell_{obs}(\\theta)}.\n\\]\nStraightforward solution via derivatives is not possible because of the superposition structure of \\(f(x)\\), which breaks the product structure and thus the log-likelihood does not have a nice form. For example \\[\n\\frac{\\partial}{\\partial \\tau} \\ell_{obs}(\\theta) = \\sum_{n=1}^N \\left[ \\frac{-\\varphi_{\\mu_1,\\sigma_1}(x_n) + \\varphi_{\\mu_2,\\sigma_2}(x_n)}{(1-\\tau) \\varphi_{\\mu_1,\\sigma_1}(x_n) + \\tau \\varphi_{\\mu_2,\\sigma_2}(x_n)} \\right].\n\\] Similarly, all the other derivatives depend on the entire vector \\(\\theta\\) in a non-linear fashion, and hence analytic solution is hard to obtain. Of course, we could solve the first-order conditions numerically, which would lead to some difficult cyclic optimization. Instead, the solution via the EM algorithm presented below is quite elegant.\nWe already know how to generate \\(X_1,\\ldots,X_N\\):\n\nfirst we toss a coin to decide whether to draw from \\(\\varphi_{\\mu_1,\\sigma_1}\\) or from \\(\\varphi_{\\mu_2,\\sigma_2}\\), and\nthen we draw from the decided Gaussian.\n\nWe can use this knowledge to introduce additional variables (unobserved, related to the coin toss) such that the complete likelihood will retain a product structure and thus will be easier to work with.\nLet \\(Z = \\mathbb{I}_{\\left[X_n \\text{ drawn from } \\varphi_{\\mu_2,\\sigma_2}\\right]} \\sim \\mathrm{Bern}(\\tau)\\) be i.i.d. and independent of \\(X\\)’s. Then the joint density of \\((X,Z)^\\top\\) can be written as \\[\nf_{X,Z}(x,z) = \\underbrace{\\left[\\varphi_{\\mu_1,\\sigma_1}(x)\\right]^{1-z} \\left[\\varphi_{\\mu_2,\\sigma_2}(x)\\right]^z}_{f_{X|Z}(x|z)} \\underbrace{\\tau^z (1-\\tau)^{1-z}}_{f_Z(z)}.\n\\] Now that we have a nice product structure, things will fall into place. The log-likelihood is \\[\n\\ell_{comp}(\\theta) = \\sum_{n=1}^N \\left\\{ (1-Z_n)\\left[ \\log \\varphi_{\\mu_1,\\sigma_1}(X_n) + \\log(1-\\tau) \\right] + Z_n\\left[ \\log \\varphi_{\\mu_2,\\sigma_2}(X_n) + \\log(\\tau) \\right] \\right\\}\n\\]\nE-step: Notice that utilizing linearity, calculating \\(\\mathbb{E}_{\\theta^{(l-1)}}\\big[\\ell_{comp}(\\theta) \\big| X_1,\\ldots,X_n\\big]\\) amounts only to evaluating \\(\\mathbb{E}_{\\theta^{(l-1)}}\\big[Z_n \\big| X_1,\\ldots,X_n\\big]\\). This can be done using the Bayes theorem: \\[\n\\mathbb{E}_{\\theta^{(l-1)}}\\big[Z_n \\big| X_1,\\ldots,X_n\\big] = P(Z_n=1|X_n, \\theta^{(l-1)}) = \\frac{f_{X\\mid Z}(X_n|Z_n=1, \\theta^{(l-1)}) P(Z_n=1 \\mid \\theta^{(l-1)})}{f_{\\theta^{(l-1)}}(X_n)} = \\frac{\\varphi_{\\mu_2^{(l-1)},\\sigma_2^{(l-1)}}(X_n) \\tau^{(l-1)}}{f_{\\theta^{(l-1)}}(X_n)} =: \\gamma_n^{(l-1)}\n\\] and hence the E-step amounts to plugging-in the contemporary estimated proportion \\(\\gamma_n^{(l-1)}\\) instead of the unobserved \\(Z_n\\)’s into the complete likelihood. This gives us \\[\n\\begin{align*}\nQ\\big(\\theta,\\theta^{(l-1)}\\big) &=  \\log(1-\\tau) \\left(N - \\sum_{n=1}^{N} \\gamma^{(l-1)}_n\\right) +\n    \\log(\\tau) \\sum_{n=1}^{N} \\gamma^{(l-1)}_n +\\\\\n    &\\qquad + \\sum_{n=1}^{N} \\big\\lbrace 1-\\gamma^{(l-1)}_n\\big\\rbrace \\log \\varphi_{\\mu_1,\\sigma_1}(X_n) + \\sum_{n=1}^{N} \\gamma^{(l-1)}_n \\log \\varphi_{\\mu_2,\\sigma_2}(X_n).\n\\end{align*}\n\\]\nM-step: Now, we can solve the first-order conditions relatively easily, because the first part of \\(Q\\big(\\theta,\\theta^{(l-1)}\\big)\\) corresponding to \\(\\tau\\) resembles binomial log-likelihood, while the last two summands resemble Gaussian log-likelihoods, only weighted. Taking derivatives by individual variables and setting them to zero gives us \\[\n\\begin{split}\n\\tau^{(l)} &= N^{-1} \\gamma, \\quad \\text{where} \\quad \\gamma = \\sum_{n=1}^N \\gamma^{(l-1)}_n \\\\\n\\mu_2^{(l)} &= \\gamma^{-1} \\sum_{n=1}^N \\gamma^{(l-1)}_n X_n \\\\\n(\\sigma_2^2)^{(l)} &= \\gamma^{-1} \\sum_{n=1}^N \\gamma^{(l-1)}_n \\big(X_n - \\mu_2^{(l)} \\big)^2 \\\\\n\\mu_1^{(l)} &= (N-\\gamma)^{-1} \\sum_{n=1}^N \\big(1-\\gamma^{(l-1)}_n\\big) X_n \\\\\n(\\sigma_1^2)^{(l)} &= (N-\\gamma)^{-1} \\sum_{n=1}^N \\big(1-\\gamma^{(l-1)}_n\\big) \\big(X_n - \\mu_1^{(l)} \\big)^2 \\\\\n\\end{split}\n\\]\n\n\n\nAssume \\(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\) is a random sample from a \\(p\\)-variate Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\), but not all entries of \\(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\) are observed. The goal is to estimate \\(\\mu\\) and \\(\\Sigma\\) from the incomplete measurements. We will denote \\(\\mathbf{x}^{(n)}_{obs}\\) the observed part of \\(\\mathbf{x}^{(n)}\\) and we will denote \\(\\mu^{(n)}_{obs}\\) and \\(\\Sigma^{(n)}_{obs}\\) the mean and the covariance of \\(\\mathbf{x}^{(n)}_{obs}\\), i.e. \\(\\mu^{(n)}_{obs}\\) is just a sub-vector of \\(\\mu\\) and \\(\\Sigma^{(n)}_{obs}\\) is a sub-matrix of \\(\\Sigma\\).\nThis is one of the instances where a programming syntax can be simpler than math. In R, having our data as a matrix X with NA for the missing entries, we could do for every \\(n=1,\\ldots,N\\)\n\n\nShow the code\n#     X - a data matrix of size N x p\n#    mu - a mean vector of size p\n# Sigma - a covariance matrix of size p x p\nind_n &lt;- !is.na(X[n,])\nx_n_obs &lt;- X[n,ind_n]             # observed part of the n-th sample\nmu_n_obs &lt;- mu[ind_n]             # mean of x_n_obs\nSigma_n_obs &lt;- Sigma[ind_n,ind_n] # covariance of x_n_obs\n\n\nSample levelplots’s of mu_n_obs and Sigma_n_obs are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall the density \\(f(\\mathbf{x})\\) of a p-variate Gaussian (e.g. here on wiki). Hence we have \\[\n\\ln f(\\mathbf{x}^{(n)}) = \\mathrm{const\\,} - \\frac{1}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\frac{1}{2} \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\big( \\mathbf{x}^{(n)} - \\mu \\big),\n\\] and since \\(\\mathbf{x}^{(n)}_{obs}\\) is just a sub-vector of \\(\\mathbf{x}^{(n)}\\), we have \\[\n\\ln f(\\mathbf{x}^{(n)}_{obs}) = \\mathrm{const\\,} - \\frac{1}{2} \\mathrm{\\ln\\,det}(\\Sigma_{obs}^{(n)}) -\n\\frac{1}{2} \\big( \\mathbf{x}^{(n)}_{obs} - \\mu_{obs}^{(n)} \\big)^\\top \\Sigma^{-1}_{obs} \\big( \\mathbf{x}^{(n)}_{obs} - \\mu_{obs}^{(n)} \\big).\n\\]\nIt follows that the complete and observed likelihood are \\[\n\\begin{split}\n\\ell_{comp}(\\mu,\\Sigma) &= \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2} \\underbrace{\\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\big( \\mathbf{x}^{(n)} - \\mu \\big)}_{\\mathrm{tr}\\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\Big\\rbrace}, \\\\\n\\ell_{obs}(\\mu,\\Sigma) &= \\mathrm{const\\,} - \\frac{1}{2} \\sum_{n=1}^N \\mathrm{\\ln\\,det}(\\Sigma_{obs}^{(n)}) -\n\\sum_{n=1}^N \\frac{1}{2} \\big( \\mathbf{x}_{obs}^{(n)} - \\mu_{obs}^{(n)} \\big)^\\top \\big(\\Sigma_{obs}^{(n)}\\big)^{-1} \\big( \\mathbf{x}_{obs}^{(n)} - \\mu_{obs}^{(n)} \\big).\n\\end{split}\n\\] While optimizing \\(\\ell_{comp}\\) is easy (not that it is easy, but it is just a multivariate Gaussian MLE), optimizing \\(\\ell_{obs}\\) is hard and we will do it via the EM algorithm.\nThe \\(l\\)-th iteration of the E-step requires constructing \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathbb{E}_{\\theta^{(l-1)}}\\big[ \\ell_{comp}(\\theta) \\big| \\mathbf{x}_{obs}^{(n)}, n=1,\\ldots,N \\big] = \\mathbb{E}_{\\theta^{(l-1)}}\\big[ \\ell_{comp}(\\theta) \\big| data],\n\\] where we denote \\(\\theta=(\\mu,\\Sigma)^\\top\\). Given the linearity of \\(\\ell_{comp}\\), we can take the conditional expectation inside: \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Big| data \\Big\\rbrace \\Sigma^{-1} \\Big]\n\\]\nWe will calculate the conditional expectation above (which is a matrix) entry by entry and distinguish 3 cases depending on whether both, one, or none of the factors in the product are observed: \\[\n\\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( x_{n,i} - \\mu_i \\big) \\big( x_{n,j} - \\mu_j \\big) \\Big| data \\Big\\rbrace = \\begin{cases}\n\\big( x_{n,i} - \\mu_i \\big) \\big( x_{n,j} - \\mu_j \\big),\\qquad\\qquad\\qquad \\text{when both } x_{n,i} \\text{ and } x_{n,j} \\text{ are observed}, \\\\\n\\big( x_{n,i} - \\mu_i \\big)\\big\\lbrace\\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}|data)-\\mu_j\\big\\rbrace,\\quad \\text{when } x_{n,i} \\text{ is observed (similarly if } x_{n,j} \\text{ is observed)}, \\\\\n\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace,\\quad \\text{when neither } x_{n,i} \\text{ nor } x_{n,j} \\text{ are observed}.\n\\end{cases}\n\\] Notice that \\(\\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}|data)\\) is just the linear predictor introduced last week, denoted by \\(\\widehat{x}_{n,j}\\) last week, but now let us denote them by \\(\\widehat{x}_{n,j}^{(l-1)}\\) to emphasize the fact that they are the conditional expectations from the previous iteration.\nThe calculation of \\(\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace\\) is a bit painful, but adding and subtracting \\(\\widehat{x}_{n,i}^{(l-1)}\\), resp. \\(\\widehat{x}_{n,j}^{(l-1)}\\) in the inner-most parentheses gives \\[\n\\begin{split}\n\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace &=\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + \\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\widehat{x}_{n,i}^{(l-1)})(x_{n,j}-\\widehat{x}_{n,j}^{(l-1)})|data\\rbrace \\\\\n&\\quad+\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_i) \\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}-\\widehat{x}_{n,j}^{(l-1)}|data)+\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_j) \\mathbb{E}_{\\theta^{(l-1)}}(x_{n,i}-\\widehat{x}_{n,i}^{(l-1)}|data) \\\\\n&= (\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + \\mathrm{cov}_{\\theta^{(l-1)}}(x_{n,i},x_{n,j}|data) + 0 + 0\\\\\n&=: (\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + c_{n,i,j}.\n\\end{split}\n\\]\nAltogether, we can write \\[\n\\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Big| data \\Big\\rbrace = (\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)(\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)^\\top + \\mathbf{C}^{(n)},\n\\] where \\(\\mathbf{C}^{(n)} = (c_{n,i,j})_{i,j=1}^{p}\\). Hence we have completed the E-step: \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\Big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\boldsymbol{\\mu})(\\widehat{\\mathbf x}^{(n)(l-1)}-\\boldsymbol{\\mu})^\\top + \\mathbf{C}^{(n)}\\Big\\rbrace \\boldsymbol{\\Sigma}^{-1} \\Big].\n\\]\nThe \\(M\\)-step is now straightforward. Updating \\(\\mu\\) is exactly the same as if a Gaussian MLE was calculated, i.e., \\(\\mu^{(l)} = N^{-1} \\sum_{n} \\widehat{\\mathbf x}^{(n)(l-1)}\\), that is just the sample mean of the completed matrix. For \\(\\Sigma^{(l)}\\), rearrange \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)(\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)^\\top + \\mathbf{C}^{(n)} \\big\\rbrace \\Sigma^{-1} \\Big].\n\\] This can be solved like Gaussian MLE for \\(\\Sigma\\), i.e., we take a derivative w.r.t. \\(\\Sigma\\), set it to zero, plug in the current estimate for \\(\\mu\\), and solve to obtain \\[\n\\Sigma^{(l)} = \\frac{1}{N} \\sum_{n=1}^N \\big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\hat{\\mu}^{(l)})(\\widehat{\\mathbf x}^{(n)(l-1)}-\\hat{\\mu}^{(l)})^\\top + \\mathbf{C}^{(n)} \\big\\rbrace.\n\\]\nNote: The calculation above is a perfect example of a shortcut in calculations. Instead of solving the M-step, we recognize the connection to Gaussian MLE and utilize it.\n\n\nExample 3 shows how to perform Step I needed to cross-validate for the number of components \\(r\\). Actually, the predictors \\(\\widehat{x}_{n,j}\\) are naturally taken as the limit of \\(\\widehat{x}_{n,j}^{(l)}\\) for \\(l \\to \\infty\\).\nOne should remember that this approach to selecting the rank \\(r\\) for PCA requires distributional assumption (Gaussianity) on the observations.\nNotice that, even though it might feel quite natural, calculating the expected complete log-likelihood does NOT correspond just to simple imputation of the respective conditional means into the likelihood. What might feel quite natural would not have the desired monotone convergence property below.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#example-1-censored-observations",
    "href": "notes/week_06.html#example-1-censored-observations",
    "title": "EM Algorithm",
    "section": "",
    "text": "See the slides of Week 6.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#example-2-mixing-proportions",
    "href": "notes/week_06.html#example-2-mixing-proportions",
    "title": "EM Algorithm",
    "section": "",
    "text": "Let \\(X_1,\\ldots,X_N\\) be i.i.d. from \\[f_{\\theta}(x) = (1-\\tau) \\varphi_{\\mu_1,\\sigma_1}(x) + \\tau \\varphi_{\\mu_2,\\sigma_2}(x) = (1-\\tau) \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp\\left\\lbrace - \\frac{1}{2} \\left( \\frac{x-\\mu_1}{\\sigma_1} \\right)^2 \\right\\rbrace + \\tau \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}} \\exp\\left\\lbrace - \\frac{1}{2} \\left( \\frac{x-\\mu_2}{\\sigma_2} \\right)^2 \\right\\rbrace\\]\nThe task is to estimate \\(\\theta = (\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\tau)^\\top\\) via MLE, i.e., solve \\[\n\\underset{\\theta}{\\mathrm{arg\\,max}} \\underbrace{\\sum_{n=1}^N \\log f(X_n)}_{\\ell_{obs}(\\theta)}.\n\\]\nStraightforward solution via derivatives is not possible because of the superposition structure of \\(f(x)\\), which breaks the product structure and thus the log-likelihood does not have a nice form. For example \\[\n\\frac{\\partial}{\\partial \\tau} \\ell_{obs}(\\theta) = \\sum_{n=1}^N \\left[ \\frac{-\\varphi_{\\mu_1,\\sigma_1}(x_n) + \\varphi_{\\mu_2,\\sigma_2}(x_n)}{(1-\\tau) \\varphi_{\\mu_1,\\sigma_1}(x_n) + \\tau \\varphi_{\\mu_2,\\sigma_2}(x_n)} \\right].\n\\] Similarly, all the other derivatives depend on the entire vector \\(\\theta\\) in a non-linear fashion, and hence analytic solution is hard to obtain. Of course, we could solve the first-order conditions numerically, which would lead to some difficult cyclic optimization. Instead, the solution via the EM algorithm presented below is quite elegant.\nWe already know how to generate \\(X_1,\\ldots,X_N\\):\n\nfirst we toss a coin to decide whether to draw from \\(\\varphi_{\\mu_1,\\sigma_1}\\) or from \\(\\varphi_{\\mu_2,\\sigma_2}\\), and\nthen we draw from the decided Gaussian.\n\nWe can use this knowledge to introduce additional variables (unobserved, related to the coin toss) such that the complete likelihood will retain a product structure and thus will be easier to work with.\nLet \\(Z = \\mathbb{I}_{\\left[X_n \\text{ drawn from } \\varphi_{\\mu_2,\\sigma_2}\\right]} \\sim \\mathrm{Bern}(\\tau)\\) be i.i.d. and independent of \\(X\\)’s. Then the joint density of \\((X,Z)^\\top\\) can be written as \\[\nf_{X,Z}(x,z) = \\underbrace{\\left[\\varphi_{\\mu_1,\\sigma_1}(x)\\right]^{1-z} \\left[\\varphi_{\\mu_2,\\sigma_2}(x)\\right]^z}_{f_{X|Z}(x|z)} \\underbrace{\\tau^z (1-\\tau)^{1-z}}_{f_Z(z)}.\n\\] Now that we have a nice product structure, things will fall into place. The log-likelihood is \\[\n\\ell_{comp}(\\theta) = \\sum_{n=1}^N \\left\\{ (1-Z_n)\\left[ \\log \\varphi_{\\mu_1,\\sigma_1}(X_n) + \\log(1-\\tau) \\right] + Z_n\\left[ \\log \\varphi_{\\mu_2,\\sigma_2}(X_n) + \\log(\\tau) \\right] \\right\\}\n\\]\nE-step: Notice that utilizing linearity, calculating \\(\\mathbb{E}_{\\theta^{(l-1)}}\\big[\\ell_{comp}(\\theta) \\big| X_1,\\ldots,X_n\\big]\\) amounts only to evaluating \\(\\mathbb{E}_{\\theta^{(l-1)}}\\big[Z_n \\big| X_1,\\ldots,X_n\\big]\\). This can be done using the Bayes theorem: \\[\n\\mathbb{E}_{\\theta^{(l-1)}}\\big[Z_n \\big| X_1,\\ldots,X_n\\big] = P(Z_n=1|X_n, \\theta^{(l-1)}) = \\frac{f_{X\\mid Z}(X_n|Z_n=1, \\theta^{(l-1)}) P(Z_n=1 \\mid \\theta^{(l-1)})}{f_{\\theta^{(l-1)}}(X_n)} = \\frac{\\varphi_{\\mu_2^{(l-1)},\\sigma_2^{(l-1)}}(X_n) \\tau^{(l-1)}}{f_{\\theta^{(l-1)}}(X_n)} =: \\gamma_n^{(l-1)}\n\\] and hence the E-step amounts to plugging-in the contemporary estimated proportion \\(\\gamma_n^{(l-1)}\\) instead of the unobserved \\(Z_n\\)’s into the complete likelihood. This gives us \\[\n\\begin{align*}\nQ\\big(\\theta,\\theta^{(l-1)}\\big) &=  \\log(1-\\tau) \\left(N - \\sum_{n=1}^{N} \\gamma^{(l-1)}_n\\right) +\n    \\log(\\tau) \\sum_{n=1}^{N} \\gamma^{(l-1)}_n +\\\\\n    &\\qquad + \\sum_{n=1}^{N} \\big\\lbrace 1-\\gamma^{(l-1)}_n\\big\\rbrace \\log \\varphi_{\\mu_1,\\sigma_1}(X_n) + \\sum_{n=1}^{N} \\gamma^{(l-1)}_n \\log \\varphi_{\\mu_2,\\sigma_2}(X_n).\n\\end{align*}\n\\]\nM-step: Now, we can solve the first-order conditions relatively easily, because the first part of \\(Q\\big(\\theta,\\theta^{(l-1)}\\big)\\) corresponding to \\(\\tau\\) resembles binomial log-likelihood, while the last two summands resemble Gaussian log-likelihoods, only weighted. Taking derivatives by individual variables and setting them to zero gives us \\[\n\\begin{split}\n\\tau^{(l)} &= N^{-1} \\gamma, \\quad \\text{where} \\quad \\gamma = \\sum_{n=1}^N \\gamma^{(l-1)}_n \\\\\n\\mu_2^{(l)} &= \\gamma^{-1} \\sum_{n=1}^N \\gamma^{(l-1)}_n X_n \\\\\n(\\sigma_2^2)^{(l)} &= \\gamma^{-1} \\sum_{n=1}^N \\gamma^{(l-1)}_n \\big(X_n - \\mu_2^{(l)} \\big)^2 \\\\\n\\mu_1^{(l)} &= (N-\\gamma)^{-1} \\sum_{n=1}^N \\big(1-\\gamma^{(l-1)}_n\\big) X_n \\\\\n(\\sigma_1^2)^{(l)} &= (N-\\gamma)^{-1} \\sum_{n=1}^N \\big(1-\\gamma^{(l-1)}_n\\big) \\big(X_n - \\mu_1^{(l)} \\big)^2 \\\\\n\\end{split}\n\\]",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#example-3-multivariate-gaussian-with-missing-entries",
    "href": "notes/week_06.html#example-3-multivariate-gaussian-with-missing-entries",
    "title": "EM Algorithm",
    "section": "",
    "text": "Assume \\(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\) is a random sample from a \\(p\\)-variate Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\), but not all entries of \\(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\) are observed. The goal is to estimate \\(\\mu\\) and \\(\\Sigma\\) from the incomplete measurements. We will denote \\(\\mathbf{x}^{(n)}_{obs}\\) the observed part of \\(\\mathbf{x}^{(n)}\\) and we will denote \\(\\mu^{(n)}_{obs}\\) and \\(\\Sigma^{(n)}_{obs}\\) the mean and the covariance of \\(\\mathbf{x}^{(n)}_{obs}\\), i.e. \\(\\mu^{(n)}_{obs}\\) is just a sub-vector of \\(\\mu\\) and \\(\\Sigma^{(n)}_{obs}\\) is a sub-matrix of \\(\\Sigma\\).\nThis is one of the instances where a programming syntax can be simpler than math. In R, having our data as a matrix X with NA for the missing entries, we could do for every \\(n=1,\\ldots,N\\)\n\n\nShow the code\n#     X - a data matrix of size N x p\n#    mu - a mean vector of size p\n# Sigma - a covariance matrix of size p x p\nind_n &lt;- !is.na(X[n,])\nx_n_obs &lt;- X[n,ind_n]             # observed part of the n-th sample\nmu_n_obs &lt;- mu[ind_n]             # mean of x_n_obs\nSigma_n_obs &lt;- Sigma[ind_n,ind_n] # covariance of x_n_obs\n\n\nSample levelplots’s of mu_n_obs and Sigma_n_obs are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall the density \\(f(\\mathbf{x})\\) of a p-variate Gaussian (e.g. here on wiki). Hence we have \\[\n\\ln f(\\mathbf{x}^{(n)}) = \\mathrm{const\\,} - \\frac{1}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\frac{1}{2} \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\big( \\mathbf{x}^{(n)} - \\mu \\big),\n\\] and since \\(\\mathbf{x}^{(n)}_{obs}\\) is just a sub-vector of \\(\\mathbf{x}^{(n)}\\), we have \\[\n\\ln f(\\mathbf{x}^{(n)}_{obs}) = \\mathrm{const\\,} - \\frac{1}{2} \\mathrm{\\ln\\,det}(\\Sigma_{obs}^{(n)}) -\n\\frac{1}{2} \\big( \\mathbf{x}^{(n)}_{obs} - \\mu_{obs}^{(n)} \\big)^\\top \\Sigma^{-1}_{obs} \\big( \\mathbf{x}^{(n)}_{obs} - \\mu_{obs}^{(n)} \\big).\n\\]\nIt follows that the complete and observed likelihood are \\[\n\\begin{split}\n\\ell_{comp}(\\mu,\\Sigma) &= \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2} \\underbrace{\\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\big( \\mathbf{x}^{(n)} - \\mu \\big)}_{\\mathrm{tr}\\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Sigma^{-1} \\Big\\rbrace}, \\\\\n\\ell_{obs}(\\mu,\\Sigma) &= \\mathrm{const\\,} - \\frac{1}{2} \\sum_{n=1}^N \\mathrm{\\ln\\,det}(\\Sigma_{obs}^{(n)}) -\n\\sum_{n=1}^N \\frac{1}{2} \\big( \\mathbf{x}_{obs}^{(n)} - \\mu_{obs}^{(n)} \\big)^\\top \\big(\\Sigma_{obs}^{(n)}\\big)^{-1} \\big( \\mathbf{x}_{obs}^{(n)} - \\mu_{obs}^{(n)} \\big).\n\\end{split}\n\\] While optimizing \\(\\ell_{comp}\\) is easy (not that it is easy, but it is just a multivariate Gaussian MLE), optimizing \\(\\ell_{obs}\\) is hard and we will do it via the EM algorithm.\nThe \\(l\\)-th iteration of the E-step requires constructing \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathbb{E}_{\\theta^{(l-1)}}\\big[ \\ell_{comp}(\\theta) \\big| \\mathbf{x}_{obs}^{(n)}, n=1,\\ldots,N \\big] = \\mathbb{E}_{\\theta^{(l-1)}}\\big[ \\ell_{comp}(\\theta) \\big| data],\n\\] where we denote \\(\\theta=(\\mu,\\Sigma)^\\top\\). Given the linearity of \\(\\ell_{comp}\\), we can take the conditional expectation inside: \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Big| data \\Big\\rbrace \\Sigma^{-1} \\Big]\n\\]\nWe will calculate the conditional expectation above (which is a matrix) entry by entry and distinguish 3 cases depending on whether both, one, or none of the factors in the product are observed: \\[\n\\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( x_{n,i} - \\mu_i \\big) \\big( x_{n,j} - \\mu_j \\big) \\Big| data \\Big\\rbrace = \\begin{cases}\n\\big( x_{n,i} - \\mu_i \\big) \\big( x_{n,j} - \\mu_j \\big),\\qquad\\qquad\\qquad \\text{when both } x_{n,i} \\text{ and } x_{n,j} \\text{ are observed}, \\\\\n\\big( x_{n,i} - \\mu_i \\big)\\big\\lbrace\\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}|data)-\\mu_j\\big\\rbrace,\\quad \\text{when } x_{n,i} \\text{ is observed (similarly if } x_{n,j} \\text{ is observed)}, \\\\\n\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace,\\quad \\text{when neither } x_{n,i} \\text{ nor } x_{n,j} \\text{ are observed}.\n\\end{cases}\n\\] Notice that \\(\\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}|data)\\) is just the linear predictor introduced last week, denoted by \\(\\widehat{x}_{n,j}\\) last week, but now let us denote them by \\(\\widehat{x}_{n,j}^{(l-1)}\\) to emphasize the fact that they are the conditional expectations from the previous iteration.\nThe calculation of \\(\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace\\) is a bit painful, but adding and subtracting \\(\\widehat{x}_{n,i}^{(l-1)}\\), resp. \\(\\widehat{x}_{n,j}^{(l-1)}\\) in the inner-most parentheses gives \\[\n\\begin{split}\n\\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\mu_i)(x_{n,j}-\\mu_j)|data\\rbrace &=\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + \\mathbb{E}_{\\theta^{(l-1)}}\\lbrace(x_{n,i}-\\widehat{x}_{n,i}^{(l-1)})(x_{n,j}-\\widehat{x}_{n,j}^{(l-1)})|data\\rbrace \\\\\n&\\quad+\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_i) \\mathbb{E}_{\\theta^{(l-1)}}(x_{n,j}-\\widehat{x}_{n,j}^{(l-1)}|data)+\n(\\widehat{x}_{n,i}^{(l-1)}-\\mu_j) \\mathbb{E}_{\\theta^{(l-1)}}(x_{n,i}-\\widehat{x}_{n,i}^{(l-1)}|data) \\\\\n&= (\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + \\mathrm{cov}_{\\theta^{(l-1)}}(x_{n,i},x_{n,j}|data) + 0 + 0\\\\\n&=: (\\widehat{x}_{n,i}^{(l-1)}-\\mu_i)(\\widehat{x}_{n,j}^{(l-1)}-\\mu_j) + c_{n,i,j}.\n\\end{split}\n\\]\nAltogether, we can write \\[\n\\mathbb{E}_{\\theta^{(l-1)}} \\Big\\lbrace \\big( \\mathbf{x}^{(n)} - \\mu \\big) \\big( \\mathbf{x}^{(n)} - \\mu \\big)^\\top \\Big| data \\Big\\rbrace = (\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)(\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)^\\top + \\mathbf{C}^{(n)},\n\\] where \\(\\mathbf{C}^{(n)} = (c_{n,i,j})_{i,j=1}^{p}\\). Hence we have completed the E-step: \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\Big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\boldsymbol{\\mu})(\\widehat{\\mathbf x}^{(n)(l-1)}-\\boldsymbol{\\mu})^\\top + \\mathbf{C}^{(n)}\\Big\\rbrace \\boldsymbol{\\Sigma}^{-1} \\Big].\n\\]\nThe \\(M\\)-step is now straightforward. Updating \\(\\mu\\) is exactly the same as if a Gaussian MLE was calculated, i.e., \\(\\mu^{(l)} = N^{-1} \\sum_{n} \\widehat{\\mathbf x}^{(n)(l-1)}\\), that is just the sample mean of the completed matrix. For \\(\\Sigma^{(l)}\\), rearrange \\[\nQ(\\theta|\\theta^{(l-1)}) = \\mathrm{const\\,} - \\frac{N}{2} \\mathrm{\\ln\\,det}(\\Sigma) -\n\\sum_{n=1}^N \\frac{1}{2}\\mathrm{tr}\\Big[ \\big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)(\\widehat{\\mathbf x}^{(n)(l-1)}-\\mu)^\\top + \\mathbf{C}^{(n)} \\big\\rbrace \\Sigma^{-1} \\Big].\n\\] This can be solved like Gaussian MLE for \\(\\Sigma\\), i.e., we take a derivative w.r.t. \\(\\Sigma\\), set it to zero, plug in the current estimate for \\(\\mu\\), and solve to obtain \\[\n\\Sigma^{(l)} = \\frac{1}{N} \\sum_{n=1}^N \\big\\lbrace (\\widehat{\\mathbf x}^{(n)(l-1)}-\\hat{\\mu}^{(l)})(\\widehat{\\mathbf x}^{(n)(l-1)}-\\hat{\\mu}^{(l)})^\\top + \\mathbf{C}^{(n)} \\big\\rbrace.\n\\]\nNote: The calculation above is a perfect example of a shortcut in calculations. Instead of solving the M-step, we recognize the connection to Gaussian MLE and utilize it.\n\n\nExample 3 shows how to perform Step I needed to cross-validate for the number of components \\(r\\). Actually, the predictors \\(\\widehat{x}_{n,j}\\) are naturally taken as the limit of \\(\\widehat{x}_{n,j}^{(l)}\\) for \\(l \\to \\infty\\).\nOne should remember that this approach to selecting the rank \\(r\\) for PCA requires distributional assumption (Gaussianity) on the observations.\nNotice that, even though it might feel quite natural, calculating the expected complete log-likelihood does NOT correspond just to simple imputation of the respective conditional means into the likelihood. What might feel quite natural would not have the desired monotone convergence property below.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#speed-of-convergence",
    "href": "notes/week_06.html#speed-of-convergence",
    "title": "EM Algorithm",
    "section": "Speed of Convergence",
    "text": "Speed of Convergence\nLet \\(M:\\Theta \\to \\Theta\\) be the mapping that is implicitly defined by the EM algorithm, i.e., let \\(\\theta^{(l)} = M\\big(\\theta^{(l-1)}\\big)\\) for \\(l = 1,2\\ldots\\). Let us call \\(M\\) the iteration map and assume that it actually exists and that \\(\\{ \\theta^{(l)} \\}\\) converges to some \\(\\theta^\\star\\). Then \\(\\theta^\\star\\) must be a fixed point of the iteration map: \\[\\theta^\\star = M(\\theta^\\star).\\] In the neighborhood of \\(\\theta^\\star\\), we have by a first order Taylor expansion: \\[\\theta^{(l)} - \\theta^\\star \\approx \\mathbf{J}(\\theta^\\star) \\; (\\theta^{(l-1)} - \\theta^\\star),\\] where \\(\\mathbf{J}(\\theta)\\) is the Jacobi (the matrix of partial derivatives of all the components of \\(M\\)) and \\(\\mathbf{J}(\\theta^\\star)\\) is approximately the rate of convergence.\nIf \\(\\| \\mathbf{J}(\\theta^\\star) \\| &lt; 1\\), then \\(M\\) is a contraction (it maps points closer together) and we may hope for convergence, by the contraction mapping theorem (if a function is a contraction on a complete metric space, then it has a unique fixed point).\nSmaller \\(\\| \\mathbf{J}(\\theta^\\star) \\|\\) correspond to a faster convergence speed, though the convergence rate is always linear (a.k.a. exponential, because \\(\\| \\theta^{(l)} - \\theta^\\star \\| \\approx \\| \\mathbf{J}(\\theta^\\star)\\|^l \\; \\| \\theta^{(0)} - \\theta^\\star \\|\\)).\nInterestingly, it can be shown that \\[ \\mathbf{J}(\\theta^\\star) = \\mathbf{J}_{comp}^{-1}(\\theta^\\star)\\; \\mathbf{J}_{miss}(\\theta^\\star),\\] where \\(\\mathbf{J}_{comp}(\\theta^\\star)\\) is the Fisher information matrix of the complete data set at \\(\\theta^\\star\\), and \\(\\mathbf{J}_{miss}(\\theta^\\star)\\) similarly but of the missing data only. Thus the EM convergence rate is given by the information ratio, which measures the proportion of information about \\(\\theta\\) that is missing, by only observing \\(X_{obs}\\) compared to the full \\(X\\). The greater the proportion of missing information, the slower the rate of convergence.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#e-step-minorizes",
    "href": "notes/week_06.html#e-step-minorizes",
    "title": "EM Algorithm",
    "section": "E-step Minorizes",
    "text": "E-step Minorizes\nHere, we will cast the EM algorithm in the MM framework. While we have developed MM in the “majorization-minimization” setup, the EM naturally lies in the “minorization-maximization” setup, since we try to maximize the likelihood. To connect the two worlds that only differ by a sign, let’s minimize the negative log-likelihood here instead. So consider the following equivalent formulation of the EM algorithm aimed at minimizing \\(- \\ell_{obs}(\\theta)\\): \\[\n\\begin{split}\n\\textbf{E-step:} \\quad Q(\\theta|\\theta^{(l-1)}) &:= \\mathbb{E}_{\\theta^{(l-1)}}\\big[ - \\ell_{comp}(\\theta) \\big| X_{obs} \\big] \\\\\n\\textbf{M-step:} \\quad\\quad\\qquad \\theta^{(l)} &:= \\underset{\\theta}{\\mathrm{arg\\,min}\\ } Q(\\theta|\\theta^{(l-1)})\n\\end{split}\n\\]\nFrom the proof of Proposition 1 above, we have (incorporating the extra sign) \\[\n- \\ell_{obs}(\\theta) = - Q(\\theta|\\theta^{(l-1)}) + H(\\theta, \\theta^{(l-1)})\n\\] and since, as shown in the proof of Proposition 1 above, \\(H(\\theta, \\theta^{(l-1)}) \\leq H(\\theta^{(l-1)}, \\theta^{(l-1)})\\), we obtain \\[\n- \\ell_{obs}(\\theta) \\leq - Q(\\theta|\\theta^{(l-1)}) + H(\\theta^{(l-1)}, \\theta^{(l-1)}) =: \\widetilde{Q}(\\theta|\\theta^{(l-1)})\n\\] with equality at \\(\\theta = \\theta^{(l-1)}\\).\nHence \\(\\widetilde{Q}(\\theta|\\theta^{(l-1)})\\) is majorizing \\(- \\ell_{obs}(\\theta)\\) at \\(\\theta = \\theta^{(l-1)}\\). Finally, since \\(H(\\theta^{(l-1)}, \\theta^{(l-1)})\\) is a constant (w.r.t. \\(\\theta\\)), minimizing \\(Q\\) is equivalent to minimizing \\(\\widetilde{Q}\\).\nWe have shown above that EM is a special case of MM. If we remove the extra sign, it is clear that the E-step (of the standard EM formulation targeted to maximize the log-likelihood) minorizes the observed log-likelihood up to a constant.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_06.html#convergence-of-mm-algorithms",
    "href": "notes/week_06.html#convergence-of-mm-algorithms",
    "title": "EM Algorithm",
    "section": "Convergence of MM algorithms",
    "text": "Convergence of MM algorithms\nProposition. (Lange, 2013, Proposition 12.4.4) Suppose that all stationary points of \\(f(\\mathbf{x})\\) are isolated and that the below-stated differentiability, coerciveness, and convexity assumptions are true.\nThen, any sequence that iterates \\(\\mathbf{x}^{(l)} = M(\\mathbf{x}^{(l-1)})\\), generated by the iteration map \\(M(\\mathbf {x})\\) of the MM algorithm, possesses a limit, and that limit is a stationary point of \\(f(\\mathbf {x})\\). If \\(f(\\mathbf {x})\\) is strictly convex, then \\(\\underset{l \\to \\infty}{\\lim} \\mathbf{x}^{(l)}\\) is the minimum point.\nThe previous proposition does not properly state its assumptions, but just briefly:\n\ndifferentiability – conditions on the majorizing functions guaranteeing differentiability of the iteration map \\(M\\).\ncoerciveness – a function \\(f: \\mathbb{R}^p \\to \\mathbb{R}\\) is coercive if, on any line in \\(\\mathbb{R}^p\\), it escapes to infinity at \\(\\pm \\infty\\).\nconvexity – this is clear, \\(f\\) has to be convex. On one hand, this assumption is just a technical assumption, algorithms with the monotone convergence property will in practice always converge to a stationary point. On the other hand, we are mostly interested in the strictly convex cases anyway.\n\nThere are two points to be made here:\n\nIn numerical optimization, there are many different approaches to do the same thing, and in the case of nice (simple) problems, they coincide. Not all IRLS algorithms can be seen as MM algorithms, but the EM algorithm is just a special case of MM. That doesn’t mean, however, that taking expectation to “complete” data has to be the most natural way to find minorizations, but in statistics this is what we often encounter.\nOptimization problems in statistics are often nice from the optimization perspective: the most important distributions lead to log-concave likelihoods (e.g., exponential families), hence convexity; likelihood functions are typically coercive; and taking expectations of log-likelihood (which is typically itself differentiable) amounts to integration, hence differentiability.",
    "crumbs": [
      "Supplementary notes",
      "EM Algorithm"
    ]
  },
  {
    "objectID": "notes/week_05.html",
    "href": "notes/week_05.html",
    "title": "Cross-Validation",
    "section": "",
    "text": "Cross-validation (CV) is a way to asses the prediction capacity of a model by splitting the data set into several folds. It can be naturally used to choose between several candidate models (choosing the one with the best prediction capacity). As such, it is often used to select tuning parameters of a model (e.g. the bandwidth in local regression or the parameter \\(\\lambda\\) in ridge regression or lasso).\nMore generally, CV is often hailed as the overarching approach to tuning parameter selection by splitting the data set in a broad range of models/algorithms. However, when the problem is unsupervised (i.e. there is no response) and it is not clear how to use the model for prediction, one has to think carefully about how to devise a CV strategy.",
    "crumbs": [
      "Supplementary notes",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "notes/week_05.html#general-prediction-based-cv",
    "href": "notes/week_05.html#general-prediction-based-cv",
    "title": "Cross-Validation",
    "section": "General Prediction-based CV",
    "text": "General Prediction-based CV\nAssume now that we have a sample \\((x_1,y_1)^\\top,\\ldots, (x_N,y_N)^\\top \\in \\mathbb{R}^{p+1}\\), where \\(x\\)’s are either scalars or vectors, and we have a model \\(\\widehat{m}\\) (fitted using the data) predicting the value of \\(y\\) by \\(\\widehat{m}(x)\\). Of course, our model’s predictions are not perfect, and there is a loss associated with making a wrong prediction, which we sometimes directly aim to minimize when fitting the model. The most typical one is the mean squared error \\[\\begin{equation}\n\\mathbb{E}(Y - \\widehat{m}(X))^2, \\label{eq:loss}\\tag{1}\n\\end{equation}\\] which naturally corresponds to the least squares criterion, \\(\\frac{1}{N}\\sum_{n=1}^N \\big( y_n - m(x_n) \\big)^2\\), but other losses can be constructed, e.g. when overshooting \\(Y\\) is more problematic than undershooting \\(Y\\), the 0-1 loss (most notably in logistic regression), etc.\nTo know how good the model \\(\\widehat{m}\\) is, we would like to estimate the mean prediction error \\(\\eqref{eq:loss}\\). This would be quite simple if we had an additional data set \\((x_1^\\star,y_1^\\star)^\\top,\\ldots, (x_M^\\star,y_M^\\star)^\\top\\) distributed the same an independent of the previous one. Then we could naturally estimate \\(\\eqref{eq:loss}\\) as \\[\\frac{1}{M}\\sum_{m=1}^M \\lbrace y_k^\\star - \\widehat{m}(x_k^\\star)\\rbrace^2.\\] The reason why this estimator would work is that the model \\(\\widehat{m}\\) was trained on a training data set \\((x_1,y_1)^\\top,\\ldots, (x_N,y_N)^\\top\\) independent of the test data set \\((x_1^\\star,y_1^\\star)^\\top,\\ldots, (x_K^\\star,y_K^\\star)^\\top\\) used to evaluate its prediction performance.\nIn reality, a new test data set is seldom available, but we can artificially construct it. By splitting the data set into the training part and test part, we can estimate both the model and its prediction performance. However, such an approach is statistically wasteful, and this is where CV comes in. We only need for the model to be trained on a different data set, and this can be achieved by estimating the expected loss \\(\\eqref{eq:loss}\\) of a model \\(m\\) with \\[\nCV(\\widehat{m}) := \\frac{1}{N}\\sum_{n=1}^N \\big( y_n - \\widehat{m}^{(-n)}(x_n) \\big)^2,\n\\] where \\(\\widehat{m}^{(-n)}\\) is the model fitted without the \\(n\\)-th observation. This generally means fitting the model \\(N\\) times, which can be wasteful computationally.\nIf there is a finite number of models, \\(CV(m)\\) is evaluated for all of them and the model \\(m\\) with the lowest value of \\(CV(m)\\) is chosen.\nThe main reason why we are usually interested in estimating the prediction performance is to select the best model out of several candidates, say \\(m_1,\\ldots, m_j\\). \\(CV(m)\\) is evaluated for \\(m=m_1,\\ldots,m_n\\) and the model with the lowest value of \\(CV(m)\\) is chosen.\nThe candidate models can be completely different, but often they differ only in a single tuning parameter (as we saw in the previous section). In that case, CV can be used to pick the value of the tuning parameter.\nIf the tuning parameter is continuous, one usually chooses a finite grid of values for the parameter, even though one opt to minimize \\(CV(m)\\) as a function of the tuning parameter by some optimization approach.\nExample (Ridge Regression) The ridge regression estimator, for a fixed tuning parameter \\(\\lambda \\geq 0\\), is given as a solution to the following optimization problem: \\[\n\\mathrm{arg \\, min_\\beta} \\sum_{n=1}^N \\big( y_n - x_n^\\top \\beta \\big)^2 + \\lambda \\| \\beta \\|_2^2.\n\\] Note: In practice, when the model contains the intercept as the first regressor, it is advisable not to penalize the first regression coefficient like above.\nThe reason for adding the penalty term in the formula above (as compared to ordinary least squares, OLS) is to cope with multicollinearity. When the number of regressors (the dimension of \\(x\\)’s) is large, so is \\(\\beta\\), and estimating a lot of parameters (with our finite sample) will lead to large variance in estimation. We know that the OLS estimator (i.e. when \\(\\lambda = 0\\)) is the best (lowest-variance) linear unbiased estimator. Taking \\(\\lambda &gt; 0\\) introduced some estimation bias, but reduces variance, see the plot below. It can be rigorously shown that there is a whole range of \\(\\lambda\\)’s (specifically \\((0,\\lambda_0)\\), where \\(\\lambda_0\\) depends on the magnitude of the true \\(\\beta\\) and the noise level) for which the mean squared error of the ridge estimator will be better than that for OLS. This may or may not mean also better prediction, but in practice ridge regression performs better than OLS with respect to prediction as well, and a good value of \\(\\lambda\\) w.r.t. prediction can be selected by CV.\nNote: One should keep in mind that CV as described above is aimed at prediction. It can actually be shown that it is inconsistent for model selection (see here).\n\nThe plot above shows the bias-variance trade-off which is often controlled by the tuning parameter. Think about where OLS is on that plot, and where increasing \\(\\lambda\\) in ridge regression takes us. Also think about what is happening with local regression as the bandwidth increases.\n\nComputational Shortcut for Linear Smoothers\nCross-validation, as described above, requires fitting the model for every single observation being left and for every candidate value of the tuning parameter (or more generally for every candidate model). This can be computationally demanding. Luckily, one can sometimes avoid fitting the model for every single left-out observation, and get away with fitting every candidate model only once! This is the case for some linear smoothers and the least squares criterion.\nThe model \\(m\\) above is called a linear smoother if its fitted values \\(\\widehat{\\mathbf{y}} = (\\widehat{y}_1,\\ldots, \\widehat{y}_N)^\\top = \\big(\\widehat{m}(x_1),\\ldots, \\widehat{m}(x_N)\\big)^\\top\\) are calculated as a linear transformation of the observed independent variable values \\(\\mathbf{y} = (y_1,\\ldots, y_N)^\\top\\), i.e., when \\[\n\\widehat{\\mathbf{y}} = \\mathcal{S} \\mathbf{y},\n\\] where \\(\\mathcal{S} \\in \\mathbb{R}^{N \\times N}\\) depends on \\(x\\)’s and perhaps a bandwidh or smoothing parameter.\nThe simplest example of a linear smoother is a linear regression model, where the smoothing matrix \\(\\mathcal{S}\\) is usually denoted \\(H\\) and called the hat matrix. Other examples include local polynomial regression or ridge regression, but not the lasso.\nBelow we will demonstrate on the case of ridge regression how for a single candidate value of the tuning parameter \\(\\lambda\\), only a single full fit needs to be calculated to perform cross-validation.\nExample (Ridge Regression): Consider the least squares CV criterion for the ridge regression: \\[\\begin{equation}\nCV(\\lambda) = \\frac{1}{N}\\sum_{n=1}^N \\big( y_n - \\mathbf{x}_n^\\top \\widehat{\\beta}^{(-n)} \\big)^2, \\label{eq:cv_slow}\\tag{2}\n\\end{equation}\\] where \\(\\widehat{\\beta}^{(-n)}\\) is the ridge regression estimator (depending on the tuning parameter \\(\\lambda\\)) constructed without the \\(n\\)-th observation. The goal is to show that \\(\\eqref{eq:cv_slow}\\) can be calculated by fitting only the ridge full ridge regression model.\nFirstly, notice that \\(\\widehat{\\beta}^{(-n)} = (\\mathbf{X}^\\top\\mathbf{X} + \\lambda I - \\mathbf{x}_n \\mathbf{x}_n^\\top)^{-1}(\\mathbf{X}^\\top \\mathbf{y} - \\mathbf{x}_n y_n)\\). Using the Sherman-Morrison formula, denoting \\(\\mathbf{A} := \\mathbf{X}^\\top\\mathbf{X} + \\lambda I\\) for the formula (and also denoting \\(\\alpha_n := 1 - \\mathbf{x}_n^\\top \\mathbf{A}^{-1} \\mathbf{x}_n\\) in the formula’s denominator for short), we have \\[\n\\begin{split}\n\\widehat{\\beta}^{(-n)} &= \\left( \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1} \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{A}^{-1}}{1 - \\mathbf{x}_n^\\top \\mathbf{A}^{-1} \\mathbf{x}_n} \\right) (X^\\top \\mathbf{y} - \\mathbf{x}_n y_n) \\\\\n&= \\widehat{\\beta} - \\frac{1}{\\alpha_n} \\mathbf{A}^{-1} \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{A}^{-1} \\mathbf{X}^\\top y + \\underbrace{\\frac{1}{\\alpha_n}\\mathbf{A}^{-1} \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{A}^{-1} \\mathbf{x}_n y_n - \\mathbf{A}^{-1} \\mathbf{x}_n y_n}_{= \\frac{1}{\\alpha_n} \\mathbf{A}^{-1} \\mathbf{x}_n y_n} \\\\\n&= \\widehat{\\beta} - \\frac{1}{\\alpha_n} (\\mathbf{A}^{-1} \\mathbf{x}_n \\mathbf{x}_n^\\top \\widehat{\\beta} - \\mathbf{A}^{-1} \\mathbf{x}_n y_n).\n\\end{split}\n\\]\nNow, plugging the above expression back to \\(\\eqref{eq:cv_slow}\\) we obtain \\[\nCV(\\lambda) = \\frac{1}{N}\\sum_{n=1}^N \\bigg\\lbrace (y_n - \\mathbf{x}_n^\\top \\widehat{\\beta}) + \\frac{\\mathbf{x}_n^\\top \\mathbf{A}^{-1} \\mathbf{x}_n (\\mathbf{x}_n^\\top \\widehat{\\beta}-y_n)}{\\alpha_n} \\bigg\\rbrace^2 =  \\frac{1}{N}\\sum_{n=1}^N \\left( \\frac{y_n - \\mathbf{x}_n^\\top \\widehat{\\beta}}{\\alpha_n} \\right)^2.\n\\]\nNote that \\(\\alpha_n = 1 - \\mathcal{S}_{nn}\\). In fact, the following computationally advantageous formula holds not only for the case of ridge regression, but also for other linear smoothers, as long as the least square loss is used: \\[\nCV = \\frac{1}{N}\\sum_{n=1}^N \\bigg\\lbrace \\frac{y_n - \\widehat{m}(x_n)}{1 - \\mathcal{S}_{nn}} \\bigg\\rbrace^2. \\label{eq:cv_fast}\\tag{3}\n\\]\nNote: I am actually not sure whether there are linear smoothers for which leave-one-out CV needs to be calculated naively, i.e., for which formula \\(\\eqref{eq:cv_fast}\\) does not hold. The computational savings shown above for ridge regression can also be shown for standard regression or local constant regression, but they are shown using a specific form of the fitted model. As such, they cannot be easily generalized to all linear smoothers, but maybe similar tricks can be done for all linear smoothers? It is however easy to show that the shortcut works for a linear smoother that satisfies\n\\[\\widehat{m}^{-i}\\left(X_i\\right)=\\frac{1}{1-s_{i i}}\\left(\\widehat{m}\\left(X_i\\right)-s_{i i} Y_i\\right) \\]\n\nNote (generalized CV): One can sometimes encounter the generalized CV (GCV), which replaces the leverages \\(\\mathcal{S}_{nn}\\) in the denominator in \\(\\eqref{eq:cv_fast}\\) by their average \\(N^{-1} \\mathrm{tr}(\\mathcal{S})\\). There had been some computational reasons for this in the past. Now, GCV is still sometimes used because it can sometimes work better (it has different properties than CV).\n\n\nK-fold CV\nWhen the computational shortcut for the leave-one-out CV cannot be used, which is the case when e.g.\n\ndifferent loss than least squares is of interest, such as the 0-1 loss for logistic regression (or other classification methods), or\nthere is no closed-form solution for the model as in the case of lasso or generalized linear models,\n\napproximate criteria are typically used to achieve computational efficiency. An universal approach is the K-fold CV.\nThe data set is first randomly split into \\(K \\in \\mathbb{N}\\) subsets (folds) of approximately equal size. That is \\(J_k \\subset \\{ 1,\\ldots,N \\}\\) for \\(k = 1,\\ldots,K\\) such that \\(J_k \\cap J_{k'} = \\emptyset\\) for \\(k \\neq k'\\) and \\(\\bigcup_{k=1}^K J_k = \\{ 1,\\ldots,n \\}\\) are created by dividing a random permutation of the data indices \\(\\{ 1,\\ldots,n \\}\\).\nThe leave-one-out CV criterion for a model \\(m\\) and the least squares loss is then replaced by \\[\nCV_K(m) = K^{-1} \\sum_{k=1}^K |J_k|^{-1} \\sum_{n \\in J_k} \\big\\lbrace Y_n - m^{(-J_k)}(X_n) \\big\\rbrace^2,\n\\] where \\(m^{(-J_k)}\\) is the model fitted without the data in the \\(k\\)-th fold \\(J_k\\). The model with smallest \\(CV_K(m)\\) is then chosen.\nComputationally, \\(K\\)-fold CV requires every candidate model to be fit \\(K\\)-times. Since \\(K\\) is usually taken small, such as \\(K=10\\), this mostly poses no issues. But if it is possible to fit a model only once, such as in the case of the linear smoothers above or using other approximate criteria, one should opt to save calculations. From the statistical perspective, it is difficult to compare \\(K\\)-fold CV against leave-one-out CV. Bias is usually higher for \\(K\\)-fold CV, while the case of variance is not clear.\nNote: The results here are not deterministic due to the random splitting, so don’t forget to set the seed when using \\(K\\)-fold CV.\nExample (Lasso): For \\((x_1,y_1)^\\top,\\ldots, (x_N,y_N)^\\top \\in \\mathbb{R}^{p+1}\\) and a fixed tuning parameter \\(\\lambda \\geq 0\\), the lasso (i.e., the \\(\\ell_1\\)-penalized least-squares regression estimator) is given as a solution to \\[\n\\widehat{\\beta}_\\lambda := \\mathrm{arg \\, min_\\beta} \\sum_{n=1}^N \\big( y_n - x_n^\\top \\beta \\big)^2 + \\lambda \\| \\beta \\|_1.\n\\] Unlike ridge regression, the lasso estimator does not have a closed-form solution (unless the data matrix is orthogonal). However, the estimator is unique (unless there is perfect linear dependence among predictors and there is some bad luck) and the optimization problem is convex, so there are plenty of optimization algorithms (of different computational costs!) that provide us with the lasso estimator for a fixed \\(\\lambda\\).\nTo choose a good value of \\(\\lambda\\), we first choose candidate values for it, say \\(\\lambda_1,\\ldots,\\lambda_j\\) and then use \\(K\\)-fold CV to choose the best of the candidate values.\nThe candidate values are typically dictated by some practical considerations (e.g. in the case of lasso one tries for which values of \\(\\lambda\\) there is a reasonable sparsity level), but very generally one can take an equispaced grid \\(t_1,\\ldots,t_j\\) on \\((0,1)\\) and transform it to the range of the parameter, in the case of \\(\\lambda \\in (0, \\infty)\\), e.g., by taking \\(\\lambda_i = 1/(1-t_i)\\) for \\(i=1,\\ldots,j\\).\n\n\n\n\n\n\n\n\n\nThen \\(CV(\\lambda)\\) is calculated for every \\(\\lambda = \\lambda_1,\\ldots,\\lambda_j\\) and the \\(\\lambda\\) with the smallest value of \\(CV(\\lambda)\\) is chosen.",
    "crumbs": [
      "Supplementary notes",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "notes/week_03.html",
    "href": "notes/week_03.html",
    "title": "Kernel Density Estimation",
    "section": "",
    "text": "Let \\(X_1,\\ldots,X_n\\) be a random sample from a density \\(f(x)\\). Our goal is to estimate the density \\(f\\) non-parametrically (as flexibly as possible). We have already used a histogram to give us some intuition on an underlying distribution of a variable in a data set.\nWe will be using the faithful data which provides the time between eruptions (variable waiting) and the duration time of the eruptions (variable eruptions) for the Old Faithful geyser in Yellowstone. By specifying probability=T, we scaled the y-axis so each histogram integrates to one (it is easy to see why this is not the default – the numbers do not tell us much, but here we want to look at histograms as density estimators).\nShow the code\ndata(faithful)\npar(mfrow=c(1,2))\nhist(faithful$eruptions, probability=T, main = \"Eruption duration\", xlab=\"time [min]\")\nhist(faithful$waiting, probability=T, main = \"Waiting time\", xlab=\"time [min]\")\nWe assume that there exist underlying densities from which the eruption duration and waiting times are drawn. While histograms are useful in providing some quick information (supports, bimodality, etc.) they are quite useless in capturing finer properties, such as the shape, since they heavily depend on the binwidth as well as the origin – the two quantities that determine equally-spaced breaks for the bins.\nShow the code\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2)) # reduce the white space around individual plots\nhist(faithful$eruptions, probability=T, main=\"Origin at 1.5\");\nhist(faithful$eruptions, breaks=seq(1.4,5.4,by=0.5), probability=T, main=\"Origin at 1.4\", xlab=\"time [min]\")  \nhist(faithful$eruptions, breaks=seq(1.3,5.3,by=0.5), probability=T, main=\"Origin at 1.3\", xlab=\"time [min]\") \nhist(faithful$eruptions, breaks=seq(1.2,5.2,by=0.5), probability=T, main=\"Origin at 1.2\", xlab=\"time [min]\")\nShow the code\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))\nhist(faithful$eruptions, probability=T, main=\"Binwidth=0.5\")\nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.25), probability=T, main=\"Binwidth=0.25\") \nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.1), probability=T, main=\"Binwidth=0.1\") \nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.01), probability=T, main=\"Binwidth=0.01\")\nWhile binwidth is a reasonable tuning parameter (it’s role is quite intuitive and the histogram for different binwidths provides the same qualitative information), the choice of origin is completely arbitrary, and can significantly distort the histogram.",
    "crumbs": [
      "Supplementary notes",
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "notes/week_03.html#circulant-matrices-and-discrete-fourier-transform",
    "href": "notes/week_03.html#circulant-matrices-and-discrete-fourier-transform",
    "title": "Kernel Density Estimation",
    "section": "5.1 Circulant Matrices and Discrete Fourier Transform",
    "text": "5.1 Circulant Matrices and Discrete Fourier Transform\nIn this sub-section only, we will index \\(p\\)-dimensional vectors from \\(0\\) to \\(p-1\\) and similarly for matrices. I will write it down explicitly as a reminder from time to time.\nDefinition: A matrix \\(\\mathbf{C} = (c_{jk})_{j,k=0}^{p-1} \\in \\mathbb{R}^{p \\times p}\\) is called circulant if \\(c_{jk} = c_{|j-k|}\\), where \\(\\mathbf{c} = (c_j) \\in \\mathbb{R}^{p}\\) is the symbol of \\(\\mathbf{C}\\) (the first column of \\(\\mathbf{C}\\)).\n(https://en.wikipedia.org/wiki/Circulant_matrix)\nDefinition: The discrete Fourier basis in \\(\\mathbf{R}^p\\) is (in the columns of) the matrix \\(\\mathbf{E} = (e_{jk})_{j,k=0}^{p-1} \\in \\mathbb{R}^{p \\times p}\\) with entries given by \\[ e_{jk} = \\frac{1}{\\sqrt{p}} e^{2\\pi i j k / p} , \\qquad j,k=0,\\ldots,p-1\\] It is straightforward to check that \\(\\mathbf{E}\\) is unitary and hence the discrete Fourier basis is really a basis.\nDefinition: The discrete Fourier transform (DFT) of \\(\\mathbf{x} \\in \\mathbb{R}^p\\) is \\(\\mathbf{E}^* \\mathbf{x}\\) with \\(\\mathbf{E}\\) being the discrete Fourier basis from the previous definition.\nIn the previous definition, the superscript \\(*\\) denotes the conjugate transpose. Since \\(\\mathbf{E}\\) is symmetric, it only corresponds to adding minuses to all the complex exponents. The inverse DFT (IDFT) is just the application \\(\\mathbf{E} \\mathbf{x}\\) (without the complex conjugate).\nA fast Fourier transform (FFT) is an algorithm for evaluating the DFT \\(\\mathbf{E}^* \\mathbf{x}\\) efficiently. While the naive matrix-vector multiplication would require \\(\\mathcal{O}(p^2)\\) operations, this can be reduced to \\(\\mathcal{O}(p \\log p)\\) by utilizing the special (cyclic) structure of \\(\\mathbf{E}\\). Any specific algorithm achieving this reduction is referred to as a FTT.\nThe FFT is indispensable (mainly in the engineering areas), and is widely considered to be one of the most important algorithms out there. While the FFT idea goes back to Gauss and has little to do with statistics, the FFT is attributed to the great statistician John W. Tukey.\nThe important connection between circulant matrices is in the following claim.\nClaim: Circulant matrices are diagonalizable by the DFT. Specifically, the eigendecomposition of a circulant matrix \\(\\mathbf{C}\\) (with a symbol \\(\\mathbf{c}\\)) is given by \\(\\mathbf{C} = \\mathbf{E} \\mathrm{diag}(\\mathbf{q}) \\mathbf{E}^*\\), where \\(\\mathbf{q} = \\mathbf{E}^* \\mathbf{c}\\).\nProof: As usual, let \\(\\mathbf{e}_j\\) denote the \\(j\\)-th column of \\(\\mathbf{E}\\). Then we have \\[\n\\mathbf{C} \\mathbf{e}_j = \\begin{pmatrix}\n\\sum_{k=0}^{p-1} c_{0k} e_{jk} \\\\\n\\sum_{k=0}^{p-1} c_{1k} e_{jk} \\\\\n\\vdots \\\\\n\\sum_{k=0}^{p-1} c_{(p-1),k} e_{jk}\n\\end{pmatrix} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\n\\sum_{k} c_{|0-k|} e_{jk} \\\\\n\\sum_{k} c_{|1-k|} e_{jk} \\\\\n\\vdots \\\\\n\\sum_{k} c_{|(p-1)-k|} e_{jk}\n\\end{pmatrix} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\ne^{2 \\pi i j/p} \\sum_k c_{|0-k|} e^{2 \\pi i j (k-0)/p} \\\\\ne^{2 \\pi i j 2/p} \\sum_k c_{|1-k|} e^{2 \\pi i j (k-1)/p} \\\\\n\\vdots \\\\\ne^{2 \\pi i j p/p} \\sum_k c_{|(p-1)-k|} e^{2 \\pi i j (k-p+1)/p}\n\\end{pmatrix}\n\\] Since both the symbol \\(\\mathbf{c}\\) and the Fourier basis are cyclic, all the sums on the RHS of the previous formula are the same, let us denote them by \\(\\widetilde{q}_j\\). Then we have \\(\\mathbf{C} \\mathbf{e}_j = \\frac{1}{\\sqrt{p}} \\widetilde{q}_j \\mathbf{e}_j\\) for all \\(j\\). Equivalently, in the full matrix format: \\[ \\mathbf{C} \\mathbf{E} = \\mathbf{E} \\frac{1}{\\sqrt{p}} \\mathrm{diag}\\big(\\widetilde{q}\\big) \\] and hence we have the eigendecomposition \\(\\mathbf{C} = \\mathbf{E} \\frac{1}{\\sqrt{p}} \\mathrm{diag}\\big(\\widetilde{q}\\big) \\mathbf{E}^*\\). Defining \\(\\mathbf{q} := \\frac{1}{\\sqrt{p}} \\widetilde{q}\\), this is exactly what we wanted. It remains to show that \\(\\mathbf{q} = \\mathbf{E}^* \\mathbf{c}\\), but this is clear since: \\[\n\\mathbf{q} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\n\\sum_k c_{|0-k|} e^{2 \\pi i j (k-0)/p} \\\\\n\\sum_k c_{|1-k|} e^{2 \\pi i j (k-1)/p} \\\\\n\\vdots \\\\\n\\sum_k c_{|(p-1)-k|} e^{2 \\pi i j (k-p+1)/p}\n\\end{pmatrix} = \\begin{pmatrix}\n\\sum_k c_{|0-k|} e^{- 2 \\pi i j (0-k)/p} \\\\\n\\sum_k c_{|1-k|} e^{- 2 \\pi i j (1-k)/p} \\\\\n\\vdots \\\\\n\\sum_k c_{|(p-1)-k|} e^{- 2 \\pi i j (p-1+k)/p}\n\\end{pmatrix} = \\begin{pmatrix}\n\\sum_k c_{k} e^{- 2 \\pi i j k/p} \\\\\n\\sum_k c_{k} e^{- 2 \\pi i j k/p} \\\\\n\\vdots \\\\\n\\sum_k c_{k} e^{- 2 \\pi i j k/p}\n\\end{pmatrix} = \\mathbf{E}^* \\mathbf{c}\n\\]\n\nQ.E.D.\n\nNow we know that every circulant matrix can be applied efficiently thanks to the FFT: \\[\\mathbf{C} \\mathbf{x} = \\underbrace{\\mathbf{E} \\underbrace{\\mathrm{diag}(\\mathbf{\\underbrace{\\mathbf q}_{= \\mathrm{FFT}(\\mathbf{c})}}) \\underbrace{\\mathbf{E}^* \\mathbf x}_{ = \\mathrm{FFT}(\\mathbf x)}}_{\\text{entry-wise prod of those 2 vectors}}}_{\\text{inverse FFT of that product}}\\] Hence instead of the \\(\\mathcal{O}(p^2)\\) operations needed to calculate \\(\\mathbf{C} \\mathbf{x}\\) naively, we can calculate it using FFT (twice FFT and once inverse FFT) in just \\(\\mathcal{O}(p \\log(p))\\) operations.",
    "crumbs": [
      "Supplementary notes",
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "notes/week_03.html#kde-viewed-as-a-linear-smoother",
    "href": "notes/week_03.html#kde-viewed-as-a-linear-smoother",
    "title": "Kernel Density Estimation",
    "section": "5.2 KDE viewed as a Linear Smoother",
    "text": "5.2 KDE viewed as a Linear Smoother\nWhy are circulant matrices and FFT interesting to us here? Instead of calculating \\(\\widehat{f}(x)\\) by the formula above, i.e., by applying a smoother directly to data \\(X_1,\\ldots,X_n\\), let’s create a histogram first (with a small binwidth) and then smooth the histogram instead of the original values. This is equivalent to rounding the observations \\(X_1,\\ldots,X_n\\) to a common grid \\(t_1,\\ldots, t_p \\in \\mathbb{R}\\), obtaining approximate values \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\), and it will allow us to use FFT to reduce the KDE complexity from \\(\\mathcal{O}(n^2)\\) to \\(\\mathcal{O}(n \\log n)\\). It could actually be even less if we used a larger binwidth for the initial histogram (i.e., rounded to a coarser grid), but that is not what we want.\nLet us denote the vector of counts in the initial histogram by \\(\\mathbf{y} \\in \\mathbb{R}^p\\), where \\(p\\) is the grid size (the number of bins of the initial histogram). For simplicity, we will assume \\(p=n\\). It is easy to see that applying KDE to the rounded data \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\) is equivalent to calculating a linear smoother of the initial histogram: \\[\n\\widehat{f}(x) = \\frac{1}{n h_n} \\sum_{i=1}^n K\\left(\\frac{\\widetilde{X}_i - x}{h_n} \\right) =\n\\frac{1}{n h_n} \\sum_{j=1}^p K\\left(\\frac{t_j - x}{h_n} \\right) y_j\n\\]\n\n\nShow the code\npar(mfrow=c(1,2), mar = c(3.2, 3, 1.6, 0.2))\ninit_hist &lt;- hist(faithful$eruptions, breaks = 256, plot=F)\nx_tilde &lt;- rep(init_hist$mids, times=init_hist$counts) # read \\tilde{X} from the initial histogram\nhist(faithful$eruptions, breaks = 256, freq=F, plot=T,xlab=\"\",main=\"\") # plot init hist on the density scale\npoints(density(x_tilde,bw=0.25),col=4,type=\"l\",lwd=3)\n# compare with the default KDE\nplot(density(x_tilde,bw=0.25),col=4,lwd=3,xlab=\"\",main=\"\")\npoints(density(faithful$eruptions,bw=0.25),type=\"l\",main=\"KDE\",lwd=3,lty=2)\nlegend(\"topleft\",legend=c(\"original\",\"rounded\"),lty=c(2,1),col=c(1,4))\n\n\n\n\n\n\n\n\n\nAnd as we can see from the right-hand plot above, calculating KDE using the rounded data \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\) is very similar to calculating it using the original data \\(X_1,\\ldots,X_n\\).\nHowever, the rounding essentially enables us to utilize the algebra above. Say we only want to evaluate \\(\\widehat{f}(x)\\) on a grid – for simplicity let us say the same grid, given by \\(t_1, \\ldots, t_p\\) – we can write the whole resulting vector as \\[\n\\big(\\widehat{f}(t_1),\\ldots,\\widehat{f}(t_p)\\big)^\\top = \\mathbf{S} y\n\\] where the entries of \\(\\mathbf{S}\\) are given by \\(s_{ij} = \\frac{1}{n h_n} K\\left(\\frac{t_j - t_i}{h_n} \\right)\\).\nMethods that calculate the output as a matrix transformation of the input (like in the previous formula or in linear regression) are called linear smoothers, and their simplicity leads to nice properties. This will be seen in the next section as well as later in the course. For example, questions such as “how would the fit look like if we dropped a single observations” (which is needed e.g. in linear regression for calculating the Cook’s distances) can be answered without refitting for linear smoothers. Adopting linear models terminology, \\(\\mathbf{S}\\) is called the hat matrix.\nYou can notice that KDE is a linear smoother even without rounding the observations on a grid (simply take \\(\\mathbf{y} \\equiv \\mathbf{1}\\)), but only on an equidistant grid is the hat matrix \\(\\mathbf{S}\\) Toeplitz (in this case, it is symmetric, see below).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe kernel on the left-hand plot above just shifts (from left to right, visualized in the middle) and its values form the hat matrix on the right-hand side (from bottom to top).\nAny Toeplitz matrix \\(\\mathbf{S}\\) of dimensions \\(n \\times n\\) can be embedded into a circulant matrix \\(\\mathbf{C}\\) of dimensions at most \\((2n -1) \\times (2n-1)\\). The easiest way is to wrap the first row of \\(\\mathbf{S}\\), denoted \\(\\mathbf{s}\\), to form the first row of \\(\\mathbf{C}\\) as \\[\n\\mathbf{c} = (s_1, s_2, \\ldots, s_{n-1}, s_n, s_{n-1},\\ldots,s_2)^\\top\n\\]\n\n\n\n\n\n\n\n\n\nThen, when we want to apply the Toeplitz matrix \\(\\mathbf{S}\\), i.e., calculate the KDE from the initial histogram as \\(\\mathbf{S} y\\), we can instead apply the wrapped circulant matrix \\(\\mathbf{C}\\) on a vector suitably extended with zeros: \\[\n\\mathbf{C} \\begin{pmatrix}\n\\mathbf{y} \\\\\n\\mathbf 0\n\\end{pmatrix}\n= \\left( \\begin{array}{c|c}\n\\mathbf{S} & \\cdot \\\\\n\\hline\n\\cdot & \\cdot\n\\end{array}\\right)\n\\left( \\begin{array}{c}\n\\mathbf{y} \\\\\n\\hline\n\\mathbf 0\n\\end{array}\\right) = \\left( \\begin{array}{c}\n\\mathbf S \\mathbf{y} \\\\\n\\hline\n\\cdot\n\\end{array}\\right)\n\\] Since this section is a bit longer and we might have lost the thread, let’s summarize what we have done to make KDE calculations efficient:\n\nRound up the original data to a common equidistant grid – equivalently: calculate the initial histogram\nFollowing point 1, KDE reduces to a linear smoother with a Toeplitz hat matrix. Embed this into a circulant matrix, and add appropriate number of zeros to the observation vector.\nUse FFT to calculate the KDE.",
    "crumbs": [
      "Supplementary notes",
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "notes/week_03.html#kde-in-r",
    "href": "notes/week_03.html#kde-in-r",
    "title": "Kernel Density Estimation",
    "section": "5.3 KDE in R",
    "text": "5.3 KDE in R\nWhile I have been using solely the function density() from the base R distributions, there are in fact dozens of packages in R performing univariate kernel density estimation. An overview is given in Wickham (2011), including a simulation study concerning the speed and accuracy of some of the available packages. The takeaway message is clear: one should be aware of what software they use. To this point, the density() function uses an FFT algorithm as described above and, while still not being the fastest, it is a well documented and reliable option.\nSecondly, now that we understand that KDE (which is really the standard for density estimation) is basically just a histogram smoother, it is clear why “densigrams” (histograms overlaid with KDEs) are so popular for visualization purposes (like the first plot in Section 5.2 above).",
    "crumbs": [
      "Supplementary notes",
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 517: Statistical computation and visualisation",
    "section": "",
    "text": "This page contains an outline of the topics, exercises (ex.), and assignments (ae.) for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester. See the overview page for more information about the course.\n\n\n\n\n\n\nTip\n\n\n\nIt can be useful to check the available tutorials to get ready for the assignments quickly. You can find them under the resources section on your left. Please check the FAQ page for common questions and answers.\n\n\n\n\n\n\n\n\nThis is important\n\n\n\nWeeks 9 and 10 of the semester will exceptionally have a different structure in terms of lecture and exercises.\n\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nRESOURCES\nAE\nEX\nDUE\n\n\n\n\n1\nFri, Sep 12\n\nSoftware and Data Considerations\n\n\n\n📖 ae-01\n\n\n⌨️ ex-01\n\n\n\n\n2\nFri, Sep 19\n\nGraphics and Visualization\n\n\n📖 exploring data\n\n\n\n\n\n\n\nSun, Sep 21\n\n\n\n\n\n📖 ae-01\n\n\n\n3\nFri, Sep 26\n\nKernel Density Estimation\n\n\n🖥️ KDE demo\n📖 KDE notes\n\n\n📖 ae-02\n\n\n⌨️ ex-02\n\n\n\n\n\nSun, Sep 28\n\n\n\n\n\n\n\n4\nFri, Oct 3\n\nNonparametric Regression (Smoothing)\n\n\n📖 Smoothing notes\n\n\n📖 ae-03\n\n\n\n\n\n\nSun, Oct 5\n\n\n\n\n\n📖 ae-02\n\n\n\n5\nFri, Oct 10\n\nCross-validation\n\n\n📖 CV notes\n\n\n📖 ae-04\n\n\n\n\n\n\nSun, Oct 12\n\n\n\n\n\n📖 ae-03\n\n\n\n6\nFri, Oct 17\n\nEM algorithm\n\n\n📖 EM notes\n\n\n📖 ae-05\n\n\n⌨️ ex-03\n\n\n\n\n\nSun, Oct 19\n\n\n\n\n\n📖 ae-04\n\n\n\n\nFri, Oct 24\n\nBreak\n\n\n\n\n\n\n\n\nSun, Oct 26\n\n\n\n\n\n\n\n7\nFri, Oct 31\n\nEM algorithm\n\n\n📖 EM notes\n\n\n💻 project\n\n\n\n\n\n\nSun, Nov 2\n\n\n\n\n\n📖 ae-05\n\n\n\n8\nFri, Nov 7\n\nMonte Carlo\n\n\n📖 Monte Carlo notes\n\n\n📖 ae-06\n\n\n\n\n\n\nSun, Nov 9\n\n\n\n\n\n\n\n9\nFri, Nov 14\n\nBootstrap I + Bootstrap II\n\n\n📖 Bootstrap notes\n\n\n📖 ae-07\n\n\n\n\n\n\nSun, Nov 16\n\n\n\n\n\n\n\n10\nFri, Nov 21\n\n📖 Support for [ae-06] and [ae-07]\n\n\n\n\n\n\n\n\nSun, Nov 23\n\n\n\n\n\n📖 ae-06\n\n\n\n11\nFri, Nov 28\n\nBayesian Computations\n\n\n\n📖 ae-08\n\n\n\n\n\n\nSun, Nov 30\n\n\n\n\n\n📖 ae-07\n\n\n\n12\nFri, Dec 5\n\nBayesian Computations\n\n\n\n\n\n\n\n\nSun, Dec 7\n\n\n\n\n\n📖 ae-08\n\n\n\n13\nFri, Dec 12\n\nDecision trees for classification or Conformal prediction\n\n\n\n\n\n\n\n\nSun, Dec 14\n\n\n\n\n\n\n\n14\nFri, Dec 19\n\nSupport for final project (no lecture)\n\n\n\n\n\n\n\n\nSun, Dec 21\n\n\n\n\n\n💻 project",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "Help & FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ.",
    "crumbs": [
      "Course information",
      "Help & FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#faq",
    "href": "course-faq.html#faq",
    "title": "Help & FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nOrganizational\n\nWhich language will we be using?\nYou can use either Python, R, or Julia. We will be providing code examples in R and suppport in all three languages.\n\n\nWhat software do I need to install?\nYou will need to use git, and a text editor or IDE.\nFor git, please see here for instructions on how to install it.\nYou can use any kind of text editor or IDE you like. We recomment using VS Code with the extension corresponding to your language of choice. You can also use Rstudio for R. See here for instructions on how to install the software and IDEs.\n\n\nHow do I submit an assignment?\nWe will be using Github-classroom to distribute and collect assignments. You will need to create a Github account if you don’t already have one.\n\n\n\nTechnical issues\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\nI have bugs in my code/ something doesn’t work/ I don’t know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours.\n\n\nI cannot find the repository linked to the assignment, what should I do?\nAt the top of each assignment there should be a link you can manually click to accept the assignment. If this happens after the first assignment, please let us know.",
    "crumbs": [
      "Course information",
      "Help & FAQ"
    ]
  },
  {
    "objectID": "resources/computing/intro_to_python/index.html",
    "href": "resources/computing/intro_to_python/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "For a refresher on Python basics, you can check out one of the following resources:\n\nPython For Beginners\nIntroductory books from the official documentation\nThe Python Tutorial\nThe Hitchhiker’s Guide to Python\nDuchesnay, Lofstedt, and Younes (2020)\nand many others\n\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nGood practices in Python\n\n\nTips and tricks for writing better code\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\nReferences\n\nDuchesnay, Edouard, Tommy Lofstedt, and Feki Younes. 2020. “Statistics and Machine Learning in Python.”",
    "crumbs": [
      "Resources",
      "Coding introduction",
      "Python"
    ]
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html",
    "title": "Good practices in R",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "title": "Good practices in R",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\nExample in R:\n# Good: Use meaningful variable names and comments\npopulation_size &lt;- 1000  # Number of individuals in the population\nsample_size &lt;- 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps &lt;- 1000\nss &lt;- 100"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "title": "Good practices in R",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ncalculate_mean &lt;- function(data) {\n    # Calculate the sum of elements in the data\n    total &lt;- sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value &lt;- total / length(data)\n    \n    return(mean_value)\n}\n\n# Avoid: Lack of comments and explanation\ncalc_mean &lt;- function(d) {\n    t &lt;- sum(d)\n    m &lt;- t / length(d)\n    return(m)\n}"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in R",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ncalculate_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;- sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}\n\n# Avoid: Using global variables in functions\nmean_value &lt;- 0\ncalc_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;&lt;- sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}"
  },
  {
    "objectID": "resources/computing/intro_to_r/index.html",
    "href": "resources/computing/intro_to_r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "You might want to choose from other tutorials as they might suit your learning style better. See Section 0.1 for a list of other tutorials.\n\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nFunctions and objects\n\n\nR syntax, assigning objects, using functions\n\n\n\n\n\n\nWorking with data\n\n\nData types and structures; slicing and subsetting data\n\n\n\n\n\n\nData manipulation with tidyverse\n\n\nData manipulation with dplyr\n\n\n\n\n\n\nData visualization in R\n\n\nData visualization in ggplot2\n\n\n\n\n\n\nGood practices in R\n\n\nTips and tricks for writing better code\n\n\n\n\n\n\nNo matching items\n\n\n0.1 Additional resources\n\nR for Data Science (2e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data\nHands-On Programming with R\n\n\n\n\n\n\n\nAttributions\n\n\n\nAll these classes are taken almost verbatim from fredhutch.io, the data and computational analysis training program at Fred Hutch, which was adapted from content originally appearing in R for data analysis and visualization of Ecological Data, Copyright (c) Data Carpentry.",
    "crumbs": [
      "Resources",
      "Coding introduction",
      "R"
    ]
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "title": "Functions and objects",
    "section": "A brief orientation to RStudio",
    "text": "A brief orientation to RStudio\nR is a statistical programming language, while RStudio is an integrated development environment (IDE) that allows you to code in R more easily. RStudio possesses many features that you may find useful in your work. We’ll highlight a few of the most common and useful parts for our introductory course.\nThe first time you open RStudio, you’ll see three panels, or windows.\n\nThe panel on the left is the console, where you can run R code. The text printed in this panel is basic information about R and the version you’re running. You can test how the console can be used to run code by entering 3 + 4 and then pressing enter. This instructs your computer to read, interpret, and execute the command, then print the result (7) to the Console, and show a right facing arrow (&gt;), indicating it is ready to accept additional code.\nThe panel on the top right is the environment. It’s empty right now, but we’ll learn more about this later in this lesson.\nThe panel on the lower right shows the files present in your working directory. Currently, that’s probably your Home directory, which includes folders like Documents and Downloads.\n\nYou may notice that some of the panels possess additional tabs. We’ll explore some of these features in this class, but for more information:\nHelp -&gt; Cheetsheets -&gt; RStudio IDE cheat sheet\nThis PDF includes an overview of each of the things you see in RStudio, as well as explanations of how you can use them. It may be intimidating right now, but will come in handy as you gain experience with R.\nOne of the ways that RStudio makes working in R easier is by allowing you to create R projects. You can think of a project as a discrete unit of work, such as a chapter of a thesis/dissertation, analysis for a manuscript, or a monthly report. We recommend organizing your code, data, and other associated files as projects, which allows you to keep all parts of an analysis together for easier access.\nWe’ll be creating a project to use for the duration of this course. Create a new project in RStudio:\n\nFile -&gt; New Project\nChoose New Directory, then New Project\nname your project intro_r and save it somewhere on your computer you’ll be able to find easily later (we recommend your Desktop or Documents)\nClick Create project\n\nAfter your RStudio screen reloads, note two things:\n\nThe file browser in the lower right panel will now show the contents of a new folder, intro_r, that was created as a part of your RStudio project.\nThe console window will show the path, or location in your computer, for your project directory. This is important later in class, when this path will be required to locate data for analysis.\n\nNow we’re ready to create a new R script:\n\nFile -&gt; New File -&gt; R Script\nSave the new file as class1.R. By default, RStudio will save this in your project directory.\n\nThis R script is a text file that we’ll use to save code we learn in this class. We’ll refer to this window as the script or source window. Remember to save this file periodically to retain the record of the work you’re doing, so you can re-execute the code later if necessary.\nBy convention, a script should include a title at the top, so type the following on the first line:\n# Introduction to R: Class 1"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "title": "Functions and objects",
    "section": "Using functions",
    "text": "Using functions\nNow that we have a project and new script set up, we’re ready to begin adding code. Skipping a line after the title, type the following on the next two lines:\n\n# basic math\n4 + 5 \n\n[1] 9\n\n\nThe first of the two boxes above represents the code you execute. The second box (prefaced with ##) shows the output you should expect. The [1] in the second box means there is one item (in this case, 9) present in the output.\nThe first line in that example is a code comment. It is not interpreted by R, but is a human-readable explanation of the code that follows. This is also how we included a title in our script. In R, anything to the right of one or more # symbols represents a comment.\nThe code above is the same mathematical operation we executed earlier. If we wanted to re-run this command, we have two options:\n\nCopy and paste the code into the Console\nUse the Run button at the top of the script window\nUse the keyboard shortcut: Ctrl + Enter\n\nThe third option is the most efficient, especially as your coding skills progress. With your cursor on the line with 4 + 5, hold down the Control key and press Enter. You’ll see the code and answer both appear in the Console. A few things to note about this keyboard shortcut:\n\nIt doesn’t matter where your cursor is on the line of code; the entire line will be executed with the keyboard shortcut.\nIf there isn’t code on the line where your cursor is located, RStudio will attempt to execute following lines.\n\nIn practice, a script should represent code you are developing in R, and you should only save the code that you know functions. For this class, we’ll be including notes about things we learn as comments.\n\nCtrl + Enter is the only keyboard shortcut we emphasize in this course, but there are many others available. You can view them on the second page of the cheat sheet linked above, or by going to Help -&gt; Keyboard Shortcuts Help.\n\nIf you were looking carefully, you may have noticed that the + in the previous code example had spaces on either side, separating it from the numbers. You may wonder whether spaces matter in how the code is interpreted. As with many questions in coding, the easiest way to assess whether removing the spaces matters is to simply try it:\n\n# same code as above, without spaces\n4+5\n\n[1] 9\n\n\nGiven the output, we can conclude that spaces do not matter in how the code functions. In this case, however, spaces represent a common convention in formatting R code, as it makes it easier for human eyes to read. In general, you should attempt to replicate the code presented here as closely as possible, and we’ll do our best to note when something is required as opposed to convention.\n\nCode convention and style doesn’t make or break the ability of your code to run, but it does affect whether other people can easily understand your code. A brief overview of common code style is available here, and more information is available in the tidyverse style guide.\n\nSo far, we’ve used R with mathematical symbols representing operations. R possesses the ability to perform much more complex tasks using functions, which is a pre-defined set of code that allows you to repeat particular actions.\nR includes functions for other types of math:\n\n# using a function: rounding numbers\nround(3.14)\n\n[1] 3\n\n\nIn this case, round is the function, and 3.14 is the number (data) being manipulated by the funcion. A word followed by parentheses is a common format for functions in R.\n\nSyntax refers to the rules that dictate how combinations of words and symbols are interpreted in a language (either programming or human).\n\nAdditional options for modifying functions are called arguments, and are included with the data between parentheses. For the round function, a common modification would be the number of decimal points output. You can change this detail by adding a comma and then additional argument:\n\n# using a function with more arguments\nround(3.14, digits = 1)\n\n[1] 3.1\n\n\nIf you would like to learn more about how this function works, you can go to the bottom righthand panel and click on the Help tab. Enter the name of a function into the search box and hit Enter. Alternatively, execute the following in your console:\n?round\nThis is a shortcut for performing the same task in the panel described above.\nR help documentation tends to be formatted very consistently. At the very top, you’ll see the name of the function. Below that, a short title indicates the purpose of the function, along with a more verbose “Description”. “Usage” tells you how to use the function in code, and “Arguments” details each of the optiond in “Usage”. The rest of the subheadings should be self-explanatory.\nIn the example above, there is no label associated with 3.14. In reality, 3.14 represents x, so the command can actually be written as round(x = 3.14, digits = 1). Even if not explicitly stated, the computer assumes that 3.14 represents x if the number is the first thing that appears after the opening parenthesis.\nIf you define both arguments explicitly, you can switch the order in which they appear:\n\n# can switch order of arguments\nround(digits = 1, x = 3.14)\n\n[1] 3.1\n\n\nIf you remove the labels (round(1, 3.14)), the answer is different, because R is assuming you mean round(x = 1, digits = 3.14).\n\nYou may notice that boxes pop up as you type. These represent RStudio’s attempts to guess what you’re typing and share additional options.\n\n\nChallenge-hist\nWhat does the function hist do? What are its main arguments? How did you determine this?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "title": "Functions and objects",
    "section": "Assigning objects",
    "text": "Assigning objects\nSo far, we’ve been performing tasks with R that require us to input the data manually. One of the strengths of using a programming language is the ability to assign data to objects, or variables.\n\nObjects in R are referred to as variables in other programming languages. We’ll use these terms synonymously for this course, though in other contexts there may be differences between them. Please see the R documentation on objects for more information.\n\nLike in math, a variable is a word used to represent a value (in this case, a number):\n\n# assigning value to an object\nweight_kg &lt;- 55\n\nIn the code above, &lt;- is the assignment operator: it instructs R to recognize weight_kg as representing the value 55. You can think of this code as referencing “55 goes into weight_kg.”\nAfter executing the code above, you’ll see the object appear in the Environment panel on the upper right hand side of the RStudio screen. The name of the object will appear on the left, with the value assigned to it on the right.\nThe name you assign to objects can be arbitrary, but we recommend using names that are relatively short and meaningful in the context of the values they represent. It’s useful to also know other general limitations on object names:\n\ncase sensitive\ncannot start with numbers\navoid other common words in R (e.g., function names, like mean)\navoid dots (underscores are a good alternative, such as the example above)\n\nExtra information on object names is available in the tidyverse style guide.\nNow that the object has been assigned, we can reference that object by executing its name:\n\n# recall object\nweight_kg\n\n[1] 55\n\n\nThus, the value weight_kg represents is printed to the Console.\nWe can also perform operations on an object:\n\n# multiple an object (convert kg to lb)\n2.2 * weight_kg\n\n[1] 121\n\n\nIn that case, the answer is printed to the Console. You can also assign the output to a new object:\n\n# assign weight conversion to object\nweight_lb &lt;- 2.2 * weight_kg\n\nAfter executing that line of code, you’ll see weight_lb appear in the Environment panel, too.\nNow let’s explore what happens if we assign a value to an existing object name:\n\n# reassign new value to an object\nweight_kg &lt;- 100\n\nNote that the value assigned to weight_kg as it appears in the Environment panel changes after executing the code above.\nHas the value assigned to weight_lb also changed? You might expect this would be the case, since this value is derived from weight_kg. However, weight_kg remains the same as previously assigned. If you want the value for weight_kg to reflect the new value for weight_kg, you will need to again execute weight_lb &lt;- 2.2 * weight_kg. This should help you understand an important concept in writing code: the order in which you execute lines of code matters! In the context of the material we cover in this class, we’ll continue saving code in scripts so we have a record of both the relevant commands and the appropriate order for execution.\n\nYou can think of the names of objects like sticky notes. You have the option to place the sticky note (name) on any value you choose. You can pick up the sticky note and place it on another value, but you need to explicitly tell R when you want values assigned to certain objects.\n\nAt this point in the lesson, it’s common to have accidentally created an object with a typo in the name. If this has happened to you, it’s useful to know how to remove the object to keep your environment up to date. Here, we’ll practice removing an object with something everyone has available:\n\n# remove object\nremove(weight_lb) \n\nThis removes the specified object from the environment, which you can confirm by its absence in the Environment panel. You can also abbreviate this command to rm(weight_lb).\n\nYou can clear the entire environment using the button at the top of the Environment panel with a picture of a broom. This may seem extreme, but don’t worry! We can re-create all the work we’ve already done by executing each line of code again.\n\n\nChallenge-values\nFor the code chunk below, what is the value of each item at each step?\n\n\nmass &lt;- 47.5            # mass?\nwidth  &lt;- 122             # width?\nmass &lt;- mass * 2.0      # mass?\nwidth  &lt;- width - 20        # width?\nmass_index &lt;- mass/width  # mass_index?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "title": "Functions and objects",
    "section": "Vectors",
    "text": "Vectors\nSo far, we’ve worked with objects containing a single value. For most research purposes, however, it’s more realistic to work with a collection of values. We can do that in R by creating a vector with multiple values:\n\n# assign vector\nages &lt;- c(50, 55, 60, 65) \n# recall vector\nages\n\n[1] 50 55 60 65\n\n\nThe c function used above stands for “combine,” meaning all of the values in parentheses after it are included in the object. This is reflected in the Console, where recalling the value shows all four values, and the Environment window, where multiple values are shown on the right side.\nWe can use functions to ask basic questions about our vector, including:\n\n# how many things are in object?\nlength(ages)\n\n[1] 4\n\n# what type of object?\nclass(ages)\n\n[1] \"numeric\"\n\n# get overview of object\nstr(ages)\n\n num [1:4] 50 55 60 65\n\n\nIn the code above, we learn that there are four items (values) in our vector, and that the vector is composed of numeric data. str stands for “structure”, and shows us a general overview of the data, including a preview of the first few values (or all the values, as is the case in our small vector).\nEven more useful is the ability to use functions to perform more complex tasks for us, such as statistical summaries:\n\n# performing functions with vectors\nmean(ages)\n\n[1] 57.5\n\nrange(ages)\n\n[1] 50 65\n\n\nAlthough we’ve focused on numbers as data so far, it’s also possible for data to be words instead:\n\n# vector of body parts\norgans &lt;- c(\"lung\", \"prostate\", \"breast\")\n\nIn this case, each word is encased in quotation marks, indicating these are character data, rather than object names.\n\nChallenge-organs\nPlease answer the following questions about organs: - How many values are in organs? - What type of data is organs? - How can you see an overview of organs?\n\nWe’ve seen data as numbers and letters so far. In fact, R has all of the following basic data types:\n\ncharacter: sometimes referred to as string data, tend to be surrounded by quotes\nnumeric: real or decimal numbers, sometimes referred to as “double”\ninteger: a subset of numeric in which numbers are stored as integers\nlogical: Boolean data (TRUE and FALSE)\ncomplex: complex numbers with real and imaginary parts (e.g., 1 + 4i)\nraw: bytes of data (machine readable, but not human readable)\n\nThe three data types listed in bold above are the focus of this class. R automatically interprets the type as you enter data. Most data analysis activities will not require you to understand specific details of the other data types.\n\nChallenge-dtypes\nR tends to handle interpreting data types in the background of most operations. The following code is designed to cause some unexpected results in R. What is unusual about each of the following objects?\n\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ntricky &lt;- c(1, 2, 3, \"4\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "title": "Functions and objects",
    "section": "Manipulating vectors",
    "text": "Manipulating vectors\nIn the section above, we learned to create and assess vectors, and use functions to calculate statistics across the values. We can also modify a vector after it’s been created:\n\n# add a value to end of vector\nages &lt;- c(ages, 90) \n\nThe example above uses the same combine (c) function as when we initially created the vector. We can also use it to add values to the beginning of the vector:\n\n# add value at the beginning\nages &lt;- c(30, ages)\n\nIf we wanted to extract, or subset, a portion of a vector:\n\n# extracting second value\norgans[2] \n\n[1] \"prostate\"\n\n\nIn general, square brackets ([ ]) in R refer to a part of an object. The number 2 indicates the second value in the vector.\n\nThe index position of a value is the number associated with its location in a collection. In the example above, note that R indexes (or counts) starting with 1. This is different from many other programming languages, like Python, which use 0-based indexing.\n\nIn R, a minus sign (-) can be used to negate a value’s position, which excludes that value from the output:\n\n# excluding second value\norgans[-2] \n\n[1] \"lung\"   \"breast\"\n\n\nYou may be tempted to try extracting multiple values at a time by separating the numbers with commas (e.g., organs[2,3]). This will result in a rather cryptic error, which we’ll talk more about next time. For now, remember that you can use the combine function to indicate multiple values for subsetting:\n\n# extracting first and third values\norgans[c(1, 3)] \n\n[1] \"lung\"   \"breast\"\n\n\nWe’ll switch back to our numerical ages object to explore another common need when subsetting: extracting values based on a condition (or criteria). For numerical data, we’re often interested in extracting data that are in a certain range of values. It is tempting to try something like:\n\nages &gt; 60 \n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nThe result, however, is less than satisfying: you receive either TRUE or FALSE for each data point, depending on whether it meets the condition or not.\nWhile that information isn’t quite what we expected, we can combine it with the subsetting syntax we learned earlier:\n\n# extracts values which meet condition\nages[ages &gt; 60] \n\n[1] 65 90\n\n\nIf we read the code above from the inside out (a common strategy for R), the code above identifies which values meet the criteria, and the square brackets are used to extract this from the original vector.\nIf you want to extract items exactly equal to a specific value, you need to use two equal signs:\n\n# extracts values numerically equivalent values\nages[ages == 60]\n\n[1] 60\n\n\nYou can think of this as a way to differentiate mathematical equivalency from specification of parameters for arguments (such as digits = 1 for round(), as we learned earlier). R also allows you to use &lt;= and &gt;=.\nFinally, it’s common to need to combine conditions while subsetting. For example, you may be interested in only values between 50 and 60:\n\n# ages less than 50 OR greater than 60\nages[ages &lt; 50 | ages &gt; 60]\n\n[1] 30 65 90\n\n\nIn the code above, the vertical pipe | is interpreted to mean “or,” so each data point can belong to either the category on the left of the pipe, the category on the right, or both. In other words, the vertical pipe means any single value being evaluated must meet one or both conditions.\nYou can also combine conditions with &, but this means any single value must meet both conditions:\n\n# ages greater than 50 OR less than 60\nages[ages &gt; 50 & ages &lt; 60]\n\n[1] 55\n\n\n\nBe careful when thinking about human language as opposed to programming languges. When speaking, we is reasonable to say “extract all values below 50 and above 60.” While this makes sense in context, it is mathematically impossible for a value to be both less than 50 AND greater than 60.\n\n\nChallenge-compare\nWhy does the following code return the answer it\n\n\n\"four\" &gt; \"five\"\n\n[1] TRUE"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "title": "Functions and objects",
    "section": "Missing data",
    "text": "Missing data\nMost of the data we encounter has missing data. Programming languages interpret and handle missing data in different ways, so it’s worth taking time to dig into how R approaches this issue.\nFirst, we’ll create a new vector some values indicated as missing data:\n\n# create a vector with missing data\nheights &lt;- c(2, 4, 4, NA, 6)\n\nIn the vector above, NA represents a value where data are missing. You may notice NA is not encased in quotation marks. This is because R interprets that set of characters specifically as missing data.\nNext, let’s investigate how this vector responds to use in functions:\n\n# calculate mean and max on vector with missing data\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nThe answer isn’t very satisfying; we’re told the answer is missing data because of the presence of a single missing value in the vector. This is a slightly frustrating default behavior for some common statistical functions in R, but we can add an argument to ignore missing data and calculate across the remaining values:\n\n# add argument to remove NA\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIn the code above, the na.rm parameter controls whether missing data are removed. The default (which you can also reference in the help documentation) is for missing values to be included (na.rm = FALSE). By switching to na.rm = TRUE, we’re instructing R to remove missing data.\nThe example above retains missing values in the dataset while performing calculations. There are certainly cases in which you may want to specifically filter out the missing data from your dataset.\nThe function is.na allows you to ask whether elements in a dataset are missing:\n\n# identify elements which are missing data\nis.na(heights)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nIf a resulting value is TRUE, the value is missing. If FALSE, the data point is present. We can invert the resulting logical data using an exclamation point:\n\n# reverse the TRUE/FALSE\n!is.na(heights)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n\nThis means missing data are now listed as FALSE, with data present as TRUE.\nAs with the conditional statements we learned earlier, we can combine these results with our square bracket subsetting syntax to extract only values that are present in the dataset:\n\n# extract elements which are not missing values\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n\nAlternatively, you can use a function specifically designed for excluding (omitting) missing data:\n\n# remove incomplete cases\nna.omit(heights) \n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nYou may notice that this output looks slightly different than the previous example. This is because na.omit includes output about attributes, or information about the data. The output vectors are the same for the last two code examples, even though the way they appear in the Console seems different.\n\nIf you aren’t sure how to interpret the output in your console, sometimes it helps to assign the output to an object. You can then inspect the data type, structure, etc to ensure you’re getting the answer you expected.\n\n\nChallenge-analyze\nComplete the following tasks after executing the code chunk below. (Note: there are multiple solutions): - Remove NAs - Calculate the median - Identify how many elements in the vector are greater than 67 inches - Visualize the data as a histogram (hint: function hist)\n\n\n# create vector\nmore_heights &lt;- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "title": "Functions and objects",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we spent some time getting to know the RStudio interface for writing and running R code, explored the basic principles of R syntax for functions and object assignment, and worked with vectors to understand how R handles missing data.\nIn the next session, we’ll learn to import spreadsheet-style data that are more similar to what you’d like handle for a research project, and practice accessing different portions of the data.\nWhen you are done working in RStudio, you should save any changes to your R script. When you close RStudio, you will see a pop-up box asking if you want to save your workspace image. We do not recommend saving your project in this way, as it creates extra (hidden) files on your computer that can be unwieldy in size and inadvertently retain sensitive data (if you’re working with PHI or other private data). If you’ve saved your R script, you can recreate all the work you’ve accomplished. For more information on this topic, please review this explanation. If you would like to prevent this box from popping up in the future, we recommend:\n\nGo to Tools -&gt; Global Options (Global means for all projects; you can also change this for each project using Project Options)\nIn the drop-down menu next to Save workspace to ~/.Rdata on exit select Never.\n\nIf you need to reopen your project after closing RStudio, you should use the File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file."
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "title": "Functions and objects",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-objects\n\nCreate an object called agge that contains your age in years\nReassign the object to a new object called age (e.g., correct the typo)\nRemove the previous object from your environment\nCalculate your age in days\n\n\n\nChallenge-char\nThe following vector represents the number of vacation days possessed by various employees:\n\nvacation_days &lt;- c(5, 7, 20, 1, 0, 0, 12, 4, 2, 2, 2, 4, 5, 6, 7, 10, 4)\n\n\nHow many employees are represented in the vector?\nHow many employees have at least one work week’s worth of vacation available to them?"
  },
  {
    "objectID": "resources/computing/intro_to_julia/index.html",
    "href": "resources/computing/intro_to_julia/index.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Julia for Optimization and Learning should contain everything (and more) that you need to know about Julia for this course.\nFor a refresher on Julia basics, you can check out one of the following resources:\n\nTutorials from the official documentation\nJulia Workshop for Data Science\nJulia academy\nIntroduction to Scientific Programming and Machine Learning with Julia\nMTH 229 - Calculus Computer Laboratory\nand many others\n\n\n\n\n\n\n\nSubject\n\n\n\nDescription\n\n\n\n\n\n\n\n\nGood practices in Julia\n\n\nTips and tricks for writing better code\n\n\n\n\n\n\nNo matching items\n\n\nA very useful Julia package is RDatasets.jl, which provides access to many of the datasets available in R packages.",
    "crumbs": [
      "Resources",
      "Coding introduction",
      "Julia"
    ]
  },
  {
    "objectID": "resources/computing/computing-cheatsheets.html",
    "href": "resources/computing/computing-cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Python\nYou can find cheatsheets for Python here and here.\n\n\nR\nThe following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Resources",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "resources/tutorials/github.html",
    "href": "resources/tutorials/github.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Important\n\n\n\nPlease read the full guide if something does not work: it is very likely that the answer to your problem lies just a few lines after wherever you stopped reading.\nFirst follow this set up Git tutorial1 and then follow this tutorial to get a gentle introduction to Git and GitHub.\nSummary of the steps\nIf all of the above worked, you are ready to go.\nNext steps\nYou should follow the instructions you received by email to join the GitHub organization of your course. This will allow you to access the course repository and to submit your assignments.",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub"
    ]
  },
  {
    "objectID": "resources/tutorials/github.html#footnotes",
    "href": "resources/tutorials/github.html#footnotes",
    "title": "Git and GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you need more help installing and using git, see this tutorial.↩︎",
    "crumbs": [
      "Resources",
      "Tutorials",
      "GitHub"
    ]
  },
  {
    "objectID": "resources/tips/virtual_environments.html",
    "href": "resources/tips/virtual_environments.html",
    "title": "Virtual environments",
    "section": "",
    "text": "Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. When package versions collide, this can lead to problems ranging from error messages and frustration to silent bugs and unexpected code behavior !"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "href": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "title": "Virtual environments",
    "section": "Why can’t I just install the packages I need ?",
    "text": "Why can’t I just install the packages I need ?\nInstead of installing everything globally and risking conflicts, virtual environments give you separate spaces for each project. This means no more worrying about messing up your setup! Plus, package managers make installing, updating, and removing packages a breeze, saving you time and hassle. You can easily share your projects with classmates and reproduce your work on any machine.\n\n\n\n\n\n\nNote\n\n\n\nThis is a very brief introduction to virtual environments and package managers. For more details, please see the documentation of the package manager you use.\n\nPython virtual environment tutorial\nJulia package manager documentation\nR renv documentation"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#benefits",
    "href": "resources/tips/virtual_environments.html#benefits",
    "title": "Virtual environments",
    "section": "Benefits",
    "text": "Benefits\nHere are some examples of why using virtual environments and package managers can be incredibly useful for scientific computing:\n\nProject Isolation: Let’s say you’re working on two different projects—one in R and another in Python. Each project requires different versions of certain packages. By using virtual environments, you can create separate environments for each project, ensuring that the specific package versions needed for each don’t interfere with one another.\nReproducibility: With virtual environments, you can easily share your projects with classmates or professors, ensuring that they can replicate your exact setup without any compatibility issues. This enhances the reproducibility of your work and allows others to verify your results.\nDependency Management: Sometimes, a package may rely on a specific version of another package to work correctly. Package managers handle these dependencies automatically, saving you the headache of figuring out and managing dependencies manually.\nExperimentation: Working on a new statistical model and want to test different libraries or versions? With virtual environments, you can create a sandbox to experiment freely without worrying about affecting your main setup.\nCollaboration: When collaborating with classmates or researchers, having consistent environments through virtual environments ensures that everyone is on the same page. It prevents conflicts arising from different package versions and improves overall productivity.\nSystem Cleanliness: Installing packages globally can clutter your system, making it difficult to manage and potentially leading to conflicts between different software. Virtual environments keep your system clean and organized.\nVersion Control: Using virtual environments makes it easier to integrate your projects with version control systems like Git. You can include the configuration files for your virtual environment in the repository, making it simpler for others to work on the project.\nEfficient Updates: Package managers allow you to update packages quickly and efficiently. You can easily check for updates, install the latest versions, and keep your project up-to-date with the latest features and bug fixes.\n\nBy embracing virtual environments and package managers, you’ll have a smoother, more organized, and productive workflow, making your research and analysis process much more enjoyable and effective."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#downsides",
    "href": "resources/tips/virtual_environments.html#downsides",
    "title": "Virtual environments",
    "section": "Downsides",
    "text": "Downsides\nYou will have to run a few commands everytime you start a new project. This is a small price to pay for the benefits you get. (You may also need to activate the virtual environment everytime you start a new shell session, but this can be automated)."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#how",
    "href": "resources/tips/virtual_environments.html#how",
    "title": "Virtual environments",
    "section": "How ?",
    "text": "How ?\nrenv for R package management, venv, conda or others for Python package management. Julia has this feature built in using Pkg.\n\nCreating a virtual environment\n\n\n\nIn terminal\npython3.6 -m venv my_env \nsource my_env/bin/activate\n\n\n\nIn Julia repl\nusing Pkg\nPkg.activate(\"my_env\")\n\n\n\nIn R console\nrenv::init()\n\n\n\n\n\nAdding packages to the virtual environment (already activated)\n\n\n\nIn terminal\npip install numpy\n\n\n\nIn Julia repl\nPkg.add(Plots)\n\n\n\nIn R console\nrenv::install(\"tidyverse\")\n\n\n\n\n\nRecreating the virtual environment from a file (after creating the environment)\n\n\n\nIn terminal\npip install -r requirements.txt\n\n\n\nIn Julia repl\nPkg.instantiate()\n\n\n\nIn R console\nrenv::restore()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPython is the only one that has a specific command to create a file with the list of packages.\npip freeze &gt; requirements.txt\nFor R and Julia, the file is created automatically when you add a package to the environment and updates automatically when you add or remove packages. (For Renv you may need to run renv::snapshot() to update the file sometimes).\n\n\nFor more details on the commands or the OS specificity please see the documentation of the package manager you are using:\n\nrenv renv wignet\nJulia Julia pkg\nPython Python venv and pip, conda\n\n\nStep by step tutorial\nPython | Julia | R\n\nPython Virtual Environments (venv):\nVirtual environments in Python enable you to create isolated environments for each project. Here’s how to use venv:\n\nOpen your terminal or command prompt.\nNavigate to your project’s directory.\nCreate a new virtual environment:\npython -m venv my_project_env\nActivate the virtual environment:\n\nOn Windows:\n\nmy_project_env\\Scripts\\activate\n\nOn macOS/Linux:\n\nsource my_project_env/bin/activate\nInstall packages within the virtual environment:\npip install package_name\nDeactivate the virtual environment when you’re done:\ndeactivate\n\n\n\nJulia Package Manager (Pkg):\nJulia’s Pkg allows you to manage and install packages effortlessly. Here’s how to use Pkg:\n\nOpen the Julia REPL (Read-Eval-Print Loop).\nTo enter package management mode, type ].\nCreate a new environment and activate it:\nactivate my_project_env\nInstall packages within the environment:\nadd package_name\nUpdate packages:\nupdate\nTo exit package management mode, press Ctrl + C or type exit().\n\n\n\nR Package Manager (renv):\nIn R, renv provides a similar functionality to Python’s venv and Julia’s Pkg. Here’s how to use renv:\n\nOpen your R console or RStudio.\nInstall the renv package (if not already installed):\ninstall.packages(\"renv\")\nInitialize renv for your project:\nrenv::init()\nInstall packages within the renv environment:\ninstall.packages(\"package_name\")\nRestore the project’s environment to remove any packages that aren’t listed in the lockfile:\nrenv::restore()\nor update the lockfile to include any new packages:\nrenv::snapshot()\nDeactivate the renv environment (optional):\nrenv::deactivate()\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\ninitialization\nactivate\ndeactivate\nadd package\n\n\n\n\nrenv\nrenv::init()\nrenv:activate()\nrenv::deactivate()\nrenv::install()\n\n\nvenv\npython -m venv {name}\nsource {name}/bin/activate\ndeactivate\npip install ...\n\n\nJulia\n] activate {name}\n] activate {name}\n] activate\n] add ...\n\n\n\n\n\n\n\n\n\n\n\n\nfiles to share\nto recreate\n\n\n\n\nR and renv\nrenv.lock\nrenv::restore()\n\n\nPython and venv\nrequirements.txt\npip install -r requirements.txt\n\n\nJulia\nProject.toml\n] instantiate"
  },
  {
    "objectID": "resources/tips/tips_coding.html",
    "href": "resources/tips/tips_coding.html",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#why-should-you-care",
    "href": "resources/tips/tips_coding.html#why-should-you-care",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "href": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "title": "Good practices for coding",
    "section": "Clear naming of variables and functions",
    "text": "Clear naming of variables and functions\n\nBe explicit in your naming. If you name a variable my_variable, don’t name another one my_var or my_var_. If you name a function my_function, don’t name another one my_func or my_func_.\nNames should be self-explanatory. If you need to add a comment to explain what a variable or a function does, it means that you should change its name. For example, my_variable is a bad name, but number_of_samples is a good name. df, df2, … are bad names, but raw_data, ìmputed_data, … are good names."
  },
  {
    "objectID": "resources/tips/tips_coding.html#do-not-repeat-yourself",
    "href": "resources/tips/tips_coding.html#do-not-repeat-yourself",
    "title": "Good practices for coding",
    "section": "Do not repeat yourself",
    "text": "Do not repeat yourself\n\nWrite functions to avoid copying and pasting slight variations of the same piece of code in many places\nUse map functions for applying a piece of code iteratively to all the elements of an object"
  },
  {
    "objectID": "resources/tips/tips_coding.html#consistency",
    "href": "resources/tips/tips_coding.html#consistency",
    "title": "Good practices for coding",
    "section": "Consistency",
    "text": "Consistency\n\nBe consistent in your style. If you start a project, try to follow the style of the project. If you join a project, try to follow the style of the project.\nBe consistent in your naming. If you name a variable my_variable, don’t name another one myVariable or myVariable_. If you name a function my_function, don’t name another one myFunction or myFunction_.\nBe consistent in your formatting. If you use 2 spaces for indentation, don’t use 4 spaces for indentation. If you use 2 spaces for indentation, don’t use tabs for indentation.\n\nAll of this will make your code easier to read and understand."
  },
  {
    "objectID": "resources/tips/tips_coding.html#code-style-guidelines",
    "href": "resources/tips/tips_coding.html#code-style-guidelines",
    "title": "Good practices for coding",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  }
]